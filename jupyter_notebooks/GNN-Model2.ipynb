{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f0d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import dgl.nn as dglnn\n",
    "from dgl import from_networkx\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3eceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6b645",
   "metadata": {},
   "source": [
    "# Data and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050fb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constante\n",
    "size_embedding = 20\n",
    "# to print\n",
    "pr = False\n",
    "# True if you want to print the embedding vectors\n",
    "# the name of the file where the vectors are printed\n",
    "filename = 'M2_weights.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d840eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>h</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 20, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[2, 2, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[3, 2, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 4]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Source IP   Destination IP           h  label\n",
       "0           1                2   [1, 2, 3]      0\n",
       "1           2                3  [1, 20, 3]      1\n",
       "2           1                3   [2, 2, 3]      0\n",
       "3           3                4   [3, 2, 3]      0\n",
       "4           1                2   [1, 2, 4]      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## small example dataset\n",
    "# size of h vectors\n",
    "sizeh = 3\n",
    "# size of the embedding vectors\n",
    "# nmber of classes\n",
    "nbclasses =  2\n",
    "\n",
    "\n",
    "# dataframe\n",
    "columns=[\" Source IP\", \" Destination IP\", 'h','label']\n",
    "data = [[1,2,[1,2,3],0], [2,3,[1,20,3],1],[1,3,[2,2,3],0],[3,4,[3,2,3],0],[1,2,[1,2,4],0]]\n",
    "X1_train = pd.DataFrame(data,columns=columns)\n",
    "data = [[1,2,[1,22,3],1], [2,4,[1,1,3],0],[1,3,[2,2,3],0],[2,4,[3,2,3],0],[1,4,[3,2,4],0]]\n",
    "X1_test = pd.DataFrame(data,columns=columns)\n",
    "\n",
    "X1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d304de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BENIGN          288566\n",
      "Infiltration        36\n",
      "Name:  Label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celine/miniconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:92: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/home/celine/miniconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:97: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    }
   ],
   "source": [
    "## dataset\n",
    "# nmber of classes\n",
    "nbclasses =  2\n",
    "# True if you want to print the embedding vectors\n",
    "\n",
    "#data1 = pd.read_csv(p + 'Wednesday-workingHours.pcap_ISCX.csv')\n",
    "#data1 = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "data1 = pd.read_csv('Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "###data1 = pd.read_csv('Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "#data1 = pd.read_csv('Monday-WorkingHours.pcap_ISCX.csv') -> benin\n",
    "#data1 = pd.read_csv('Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "#data1 = pd.read_csv('Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "\n",
    "cols = list(set(list(data1.columns )) - set(list(['Flow Bytes/s',' Flow Packets/s'])) )\n",
    "data1 = data1[cols]\n",
    "\n",
    "##mise en forme des noeuds\n",
    "data1[' Source IP'] = data1[' Source IP'].apply(str)\n",
    "data1[' Source Port'] = data1[' Source Port'].apply(str)\n",
    "data1[' Destination IP'] = data1[' Destination IP'].apply(str)\n",
    "data1[' Destination Port'] = data1[' Destination Port'].apply(str)\n",
    "data1[' Source IP'] = data1[' Source IP'] + ':' + data1[' Source Port']\n",
    "data1[' Destination IP'] = data1[' Destination IP'] + ':' + data1[' Destination Port']\n",
    "data1.drop(columns=['Flow ID',' Source Port',' Destination Port',' Timestamp'],inplace=True)\n",
    "\n",
    "# labels\n",
    "print(data1[' Label'].value_counts())\n",
    "nom = []\n",
    "nom = nom + [data1[' Label'].unique()[0]]\n",
    "for i in range(1,len(data1[' Label'].unique())):\n",
    "    nom = nom + [data1[' Label'].unique()[i]]\n",
    "data1[' Label'].replace(nom[0], 0,inplace = True)\n",
    "for i in range(1,len(data1[' Label'].unique())):\n",
    "    data1[' Label'].replace(nom[i], 1,inplace = True)\n",
    "data1.rename(columns={\" Label\": \"label\"},inplace = True)\n",
    "label1 = data1.label\n",
    "data1.drop(columns=['label'],inplace = True)\n",
    "\n",
    "\n",
    "#split train and co\n",
    "data1 =  pd.concat([data1, label1], axis=1)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(\n",
    "     data1, label1, test_size=0.3, random_state=123, stratify= label1)\n",
    "\n",
    "# for non numerical attributes\n",
    "encoder1 = ce.TargetEncoder(cols=[' Protocol',  'Fwd PSH Flags', ' Fwd URG Flags', ' Bwd PSH Flags', ' Bwd URG Flags'])\n",
    "encoder1.fit(X1_train, y1_train)\n",
    "X1_train = encoder1.transform(X1_train)\n",
    "\n",
    "# scaler\n",
    "scaler1 = StandardScaler()\n",
    "cols_to_norm1 = list(set(list(X1_train.iloc[:, :].columns )) - set(list(['label', ' Source IP', ' Destination IP'])) )\n",
    "\n",
    "X1_train[cols_to_norm1] = scaler1.fit_transform(X1_train[cols_to_norm1])\n",
    "\n",
    "X1_train['h'] = X1_train[ cols_to_norm1 ].values.tolist()\n",
    "##test\n",
    "X1_test = encoder1.transform(X1_test)\n",
    "X1_test[cols_to_norm1] = scaler1.transform(X1_test[cols_to_norm1])\n",
    "X1_test['h'] = X1_test[ cols_to_norm1 ].values.tolist()\n",
    "\n",
    "sizeh = len(cols_to_norm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0df460",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0900f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        self.W_msg = nn.Linear(ndim_in , ndim_out) \n",
    "        self.W_apply = nn.Linear(ndim_in + ndim_out, ndim_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        x =  th.cat([edges.src['h']]) \n",
    "        x.type(th.cuda.FloatTensor)\n",
    "        y = self.W_msg(x)\n",
    "        y = {'m': y}\n",
    "        return y\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "        with g_dgl.local_scope():\n",
    "            g = g_dgl\n",
    "            g.ndata['h'] = nfeats\n",
    "            g.edata['g'] = efeats\n",
    "            # Eq4\n",
    "            g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "            # Eq5          \n",
    "            g.ndata['h'] = F.relu(self.W_apply(th.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "            return g.ndata['h']\n",
    "\n",
    "    \n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.c1 = self.layers.append(SAGELayer(ndim_in, edim, size_embedding, activation))\n",
    "        self.c2 = self.layers.append(SAGELayer(size_embedding, edim, size_embedding, activation)) ##\n",
    "        self.c3 = self.layers.append(SAGELayer(size_embedding, edim, ndim_out, activation))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        g.ndata['s0'] = nfeats\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                nfeats = self.dropout(nfeats)\n",
    "            nfeats = layer(g, nfeats, efeats)\n",
    "            if(i==0):\n",
    "                g.ndata['s1'] = nfeats\n",
    "            if(i==1):\n",
    "                g.ndata['s2'] = nfeats\n",
    "            if(i==2):\n",
    "                g.ndata['s3'] = nfeats\n",
    "        return nfeats.sum(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "    \n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u0 = edges.src['s0']\n",
    "        h_u1 = edges.src['s1']\n",
    "        h_u2 = edges.src['s2']\n",
    "        h_u3 = edges.src['s3']\n",
    "        \n",
    "        h_v0 = edges.dst['s0']\n",
    "        h_v1 = edges.dst['s1']\n",
    "        h_v2 = edges.dst['s2']\n",
    "        h_v3 = edges.dst['s3']\n",
    "    \n",
    "        v = th.cat([h_u0, h_u1, h_u2, h_u3, h_v0, h_v1, h_v2, h_v3], 2)\n",
    "        #v = th.cat([h_u3,h_v3],2)\n",
    "        #v = th.cat([edges.src['h'],edges.dst['h']],1)\n",
    "        if(pr == True):\n",
    "            sourceFile = open(filename, 'w')\n",
    "            if pr:\n",
    "                print(v, file = sourceFile)\n",
    "            sourceFile.close()\n",
    "            \n",
    "        score = self.W(v)\n",
    "        score = th.reshape(score, (score.shape[0], score.shape[2]))\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.gnn = SAGE(ndim_in, ndim_out, edim, activation, dropout)\n",
    "        new_dim = ndim_out * 3 + ndim_in\n",
    "        self.pred = MLPPredictor(new_dim, nbclasses)\n",
    "        \n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        h = self.gnn(g, nfeats, efeats)\n",
    "        return self.pred(g, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22870bac",
   "metadata": {},
   "source": [
    "# Construction of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6655e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celine/miniconda3/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  return th.as_tensor(data, dtype=dtype)\n",
      "/tmp/ipykernel_66246/413242809.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  G1.ndata['h'] = th.tensor(G1.ndata['h'], dtype=th.float)\n",
      "/tmp/ipykernel_66246/413242809.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  G1.edata['g'] = th.tensor(G1.edata['g'], dtype=th.float)\n"
     ]
    }
   ],
   "source": [
    "G1 = nx.from_pandas_edgelist(X1_train, \" Source IP\", \" Destination IP\", ['h','label'],\n",
    "                             create_using=nx.MultiDiGraph())\n",
    "## directed\n",
    "G1 = G1.to_directed()\n",
    "#pretreatment to compute the vectors associated to the nodes: \n",
    "#the average values of the attributes associated to the edges for which the node is a destination\n",
    "att  = {}\n",
    "for n in G1.nodes():\n",
    "    x = np.ones(sizeh)\n",
    "    att[n] = {'h':x, 'nb':1}\n",
    "                   \n",
    "nx.set_node_attributes(G1, att)\n",
    "att  = {}\n",
    "for n in G1.edges(keys=True): \n",
    "    x = np.ones(sizeh)\n",
    "    att[n] = {'g':x}\n",
    "nx.set_edge_attributes(G1, att)\n",
    "for node1, node2, data in G1.edges(data=True):\n",
    "    G1.nodes[node2]['h'] = G1.nodes[node2]['h'] + data['h']\n",
    "    G1.nodes[node2]['nb'] = G1.nodes[node2]['nb'] + 1\n",
    "    G1.nodes[node2]['g'] = np.ones(sizeh)\n",
    "\n",
    "for node2 in G1.nodes:\n",
    "    G1.nodes[node2]['h'] = G1.nodes[node2]['h'] / G1.nodes[node2]['nb'] \n",
    "\n",
    "G1 = from_networkx(G1, edge_attrs=['g','label'], node_attrs=['h'] )\n",
    "\n",
    "# create the variables to store the embedding vectors\n",
    "G1.ndata['s0'] = th.ones(G1.num_nodes(), sizeh)\n",
    "G1.ndata['s0'] = th.reshape(G1.ndata['s0'], (G1.ndata['s0'].shape[0], 1, G1.ndata['s0'].shape[1]))\n",
    "G1.ndata['s1'] = th.ones(G1.num_nodes(), size_embedding)\n",
    "G1.ndata['s1'] = th.reshape(G1.ndata['s1'], (G1.ndata['s1'].shape[0], 1, G1.ndata['s1'].shape[1]))\n",
    "G1.ndata['s2'] = th.ones(G1.num_nodes(), size_embedding)\n",
    "G1.ndata['s2'] = th.reshape(G1.ndata['s2'], (G1.ndata['s2'].shape[0], 1, G1.ndata['s2'].shape[1]))\n",
    "G1.ndata['s3'] = th.ones(G1.num_nodes(), size_embedding)\n",
    "G1.ndata['s3'] = th.reshape(G1.ndata['s3'], (G1.ndata['s3'].shape[0], 1, G1.ndata['s3'].shape[1]))\n",
    "\n",
    "G1.ndata['h'] = th.reshape(G1.ndata['h'], (G1.ndata['h'].shape[0], 1, G1.ndata['h'].shape[1]))\n",
    "G1.edata['g'] = th.reshape(G1.edata['g'], (G1.edata['g'].shape[0], 1, G1.edata['g'].shape[1]))\n",
    "G1.ndata['h'] = th.tensor(G1.ndata['h'], dtype=th.float)\n",
    "G1.edata['g'] = th.tensor(G1.edata['g'], dtype=th.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa975ce",
   "metadata": {},
   "source": [
    "# Learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ba0392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9998762607574463 tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998762607574463 tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998762607574463 tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998911023139954 tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998911023139954 tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998911023139954 tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998911023139954 tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998911023139954 tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998911023139954 tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9998911023139954 tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "[[201974     22]\n",
      " [     0     25]]\n"
     ]
    }
   ],
   "source": [
    "G1 = G1.to('cuda:0')\n",
    "G1.device\n",
    "G1.ndata['h'].device\n",
    "G1.edata['g'].device\n",
    "G1.ndata['h'].type(th.cuda.FloatTensor)\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights1 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                  classes = np.unique(G1.edata['label'].cpu().numpy()),\n",
    "                                                  y = G1.edata['label'].cpu().numpy())\n",
    "class_weights1 = th.FloatTensor(class_weights1).cuda()\n",
    "criterion1 = nn.CrossEntropyLoss(weight=class_weights1,reduction='mean')\n",
    "\n",
    "\n",
    "\n",
    "node_features1 = G1.ndata['h']\n",
    "edge_features1 = G1.edata['g']\n",
    "edge_label1 = G1.edata['label']\n",
    "\n",
    "\n",
    "model1 = Model(G1.ndata['h'].shape[2], size_embedding, G1.ndata['h'].shape[2], F.relu, 0.2).cuda()\n",
    "opt = th.optim.Adam(model1.parameters())\n",
    "\n",
    "for epoch in range(1,1000):\n",
    "    pred = model1(G1, node_features1, edge_features1).cuda()\n",
    "    loss = criterion1(pred, edge_label1)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if epoch % 100 == 0:\n",
    "      print('Training acc:', compute_accuracy(pred, edge_label1), loss)\n",
    "print('Training acc:', compute_accuracy(pred, edge_label1), loss)\n",
    "## embedding vectors\n",
    "#print('v0', G1.ndata['s0'])\n",
    "#print('v1', G1.ndata['s1'])\n",
    "#print('v2', G1.ndata['s2'])\n",
    "#print('v3', G1.ndata['s3'])\n",
    "\n",
    "pred1 = model1(G1, node_features1, edge_features1).cuda()\n",
    "pred1 = pred1.argmax(1)\n",
    "pred1 = th.Tensor.cpu(pred1).detach().numpy()\n",
    "edge_label1 = th.Tensor.cpu(edge_label1).detach().numpy()\n",
    "\n",
    "cm = confusion_matrix(edge_label1, pred1)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224d145",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53f7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66246/2097946953.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  G1_test.ndata['h'] = th.tensor(G1_test.ndata['h'], dtype=th.float)\n",
      "/tmp/ipykernel_66246/2097946953.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  G1_test.edata['g'] = th.tensor(G1_test.edata['g'], dtype=th.float)\n"
     ]
    }
   ],
   "source": [
    "G1_test = nx.from_pandas_edgelist(X1_test, \" Source IP\", \" Destination IP\", ['h','label'],\n",
    "                                  create_using=nx.MultiDiGraph())\n",
    "G1_test = G1_test.to_directed()\n",
    "\n",
    "#pretreatment to compute the vectors associated to the nodes: \n",
    "#the average values of the attributes associated to the edges for which the node is a destination\n",
    "att  = {}\n",
    "for n in G1_test.nodes():\n",
    "    x = np.ones(sizeh)\n",
    "    att[n] = {'h':x, 'nb':1}\n",
    "                   \n",
    "nx.set_node_attributes(G1_test, att)\n",
    "att  = {}\n",
    "for n in G1_test.edges(keys=True): \n",
    "    x = np.ones(sizeh)\n",
    "    att[n] = {'g':x}\n",
    "nx.set_edge_attributes(G1_test, att)\n",
    "for node1, node2, data in G1_test.edges(data=True):\n",
    "    G1_test.nodes[node2]['h'] = G1_test.nodes[node2]['h'] + data['h']\n",
    "    G1_test.nodes[node2]['nb'] = G1_test.nodes[node2]['nb'] + 1\n",
    "    G1_test.nodes[node2]['g'] = np.ones(sizeh)\n",
    "\n",
    "for node2 in G1_test.nodes:\n",
    "    G1_test.nodes[node2]['h'] = G1_test.nodes[node2]['h'] / G1_test.nodes[node2]['nb'] \n",
    "    \n",
    "G1_test = from_networkx(G1_test, edge_attrs=['g','label'], node_attrs=['h'] )\n",
    "\n",
    "# create the variables to store the embedding vectors\n",
    "G1_test.ndata['s0'] = th.ones(G1_test.num_nodes(), sizeh)\n",
    "G1_test.ndata['s0'] = th.reshape(G1_test.ndata['s0'], (G1_test.ndata['s0'].shape[0], 1, G1_test.ndata['s0'].shape[1]))\n",
    "G1_test.ndata['s1'] = th.ones(G1_test.num_nodes(), size_embedding)\n",
    "G1_test.ndata['s1'] = th.reshape(G1_test.ndata['s1'], (G1_test.ndata['s1'].shape[0], 1, G1_test.ndata['s1'].shape[1]))\n",
    "G1_test.ndata['s2'] = th.ones(G1_test.num_nodes(), size_embedding)\n",
    "G1_test.ndata['s2'] = th.reshape(G1_test.ndata['s2'], (G1_test.ndata['s2'].shape[0], 1, G1_test.ndata['s2'].shape[1]))\n",
    "G1_test.ndata['s3'] = th.ones(G1_test.num_nodes(), size_embedding)\n",
    "G1_test.ndata['s3'] = th.reshape(G1_test.ndata['s3'], (G1_test.ndata['s3'].shape[0], 1, G1_test.ndata['s3'].shape[1]))\n",
    "\n",
    "G1_test.ndata['h'] = th.reshape(G1_test.ndata['h'], (G1_test.ndata['h'].shape[0], 1, G1_test.ndata['h'].shape[1]))\n",
    "G1_test.edata['g'] = th.reshape(G1_test.edata['g'], (G1_test.edata['g'].shape[0], 1, G1_test.edata['g'].shape[1]))\n",
    "G1_test.ndata['h'] = th.tensor(G1_test.ndata['h'], dtype=th.float)\n",
    "G1_test.edata['g'] = th.tensor(G1_test.edata['g'], dtype=th.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da38eda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86557    13]\n",
      " [    0    11]]\n"
     ]
    }
   ],
   "source": [
    "G1_test = G1_test.to('cuda:0')\n",
    "G1_test.device\n",
    "G1_test.ndata['h'].device\n",
    "G1_test.edata['g'].device\n",
    "G1_test.ndata['h'].type(th.cuda.FloatTensor)\n",
    "\n",
    "node_features_test1 = G1_test.ndata['h']\n",
    "edge_features_test1 = G1_test.edata['g']\n",
    "edge_label_test1 = G1_test.edata['label']\n",
    "pred1 = model1(G1_test, node_features_test1, edge_features_test1).cuda()\n",
    "pred1 = pred1.argmax(1)\n",
    "pred1 = th.Tensor.cpu(pred1).detach().numpy()\n",
    "edge_label_test1 = th.Tensor.cpu(edge_label_test1).detach().numpy()\n",
    "\n",
    "cm = confusion_matrix(edge_label_test1, pred1)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a92e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb72a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

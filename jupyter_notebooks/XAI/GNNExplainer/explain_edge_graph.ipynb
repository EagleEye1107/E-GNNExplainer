{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "round-guide",
   "metadata": {},
   "source": [
    "# E-GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "responsible-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIC-IDS-2017-Dataset4.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460167\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset4.csv': [[0, 0.7582073464633492], [1, 0.24179265353665083]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58668 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9858900904655457 tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9873957633972168 tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9879080057144165 tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9890256524085999 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907330870628357 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911211729049683 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991260826587677 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916799664497375 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911211729049683 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9920990950436955\n",
      "Precision :  0.9690003697769013\n",
      "Recall :  0.9996185390043868\n",
      "f1_score :  0.9840713503364106\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58817 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9909349083900452 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908262491226196 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919127821922302 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918662309646606 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921301007270813 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921766519546509 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917420744895935 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895844459533691 tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907641410827637 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9921146174502895\n",
      "Precision :  0.9690919206280522\n",
      "Recall :  0.9994899585591329\n",
      "f1_score :  0.9840562425459795\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59044 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9916178584098816 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9884513020515442 tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9868680238723755 tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916178584098816 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920679926872253 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9888704344721606\n",
      "Precision :  0.9564734895191122\n",
      "Recall :  0.999291465378422\n",
      "f1_score :  0.9774137659473932\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59007 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9894136786460876 tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899724721908569 tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898017644882202 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899104237556458 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898017644882202 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897241592407227 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910901188850403 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906399846076965 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909504055976868 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9907486456700246\n",
      "Precision :  0.9639617093161484\n",
      "Recall :  0.9987035716600765\n",
      "f1_score :  0.9810251512257243\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64424\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58881 nodes and 64424 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1.edata['h'] after reshape :  64424\n",
      "Training acc: 0.9895691275596619 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903297424316406 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907022714614868 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902210831642151 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905470013618469 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906867146492004 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905315041542053 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909195303916931 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911057949066162 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9904228237923756\n",
      "Precision :  0.9651061173533083\n",
      "Recall :  0.9962626457890328\n",
      "f1_score :  0.9804369193696693\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138051\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.9915466023426125\n",
      "Precision :  0.9698101099670391\n",
      "Recall :  0.9960455362492511\n",
      "f1_score :  0.9827527600017736\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset1.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460165\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset1.csv': [[0, 0.7577868807927591], [1, 0.24221311920724087]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58851 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9913850426673889 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895068407058716 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911677241325378 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908106923103333 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916644096374512 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9901121854782104 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902673959732056 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918662309646606 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.991260885087624\n",
      "Precision :  0.9657284256108877\n",
      "Recall :  0.9994238156209987\n",
      "f1_score :  0.9822872424099418\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial nx multigraph G1 :  MultiDiGraph with 58844 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9918041229248047 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919593334197998 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9711903929710388 tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9887151718139648 tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893826246261597 tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9882029294967651 tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897551536560059 tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9894447326660156 tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903295040130615 tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9897707340546078\n",
      "Precision :  0.9601728928681692\n",
      "Recall :  0.999100488306348\n",
      "f1_score :  0.9792499763846468\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58889 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9882339835166931 tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9894757866859436 tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.987768292427063 tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.989398181438446 tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9887151718139648 tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990189790725708 tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990189790725708 tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905157685279846 tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906089305877686 tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9903916303183645\n",
      "Precision :  0.9645442732504177\n",
      "Recall :  0.9970576947678137\n",
      "f1_score :  0.980531530114798\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59221 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.989429235458374 tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907486438751221 tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906399846076965 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912298321723938 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911211729049683 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906554818153381 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910125136375427 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911366701126099 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990531325340271 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9908883473293699\n",
      "Precision :  0.965342245327392\n",
      "Recall :  0.998272\n",
      "f1_score :  0.981531007142183\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58980 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9908728003501892 tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907020330429077 tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911987781524658 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914160966873169 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913694858551025 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991633415222168 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914315938949585 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913850426673889 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9915558108129084\n",
      "Precision :  0.9670083876980429\n",
      "Recall :  0.999165436220068\n",
      "f1_score :  0.9828239454407678\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9918290474465773\n",
      "Precision :  0.9689404934687954\n",
      "Recall :  0.9982654464979963\n",
      "f1_score :  0.9833843978317228\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset0.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460165\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset0.csv': [[0, 0.7576130301087654], [1, 0.24238696989123468]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58846 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9896620512008667 tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903605580329895 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990251898765564 tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906554818153381 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990189790725708 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906089305877686 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902673959732056 tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990500271320343 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903295040130615 tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9909038697359639\n",
      "Precision :  0.9658506731946145\n",
      "Recall :  0.9982289690069576\n",
      "f1_score :  0.9817729393468118\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58820 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9912142753601074 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909814596176147 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909349083900452 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912763833999634 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916799664497375 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914005398750305 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991571307182312 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9915713332195023\n",
      "Precision :  0.9667364789947025\n",
      "Recall :  0.9998088806778366\n",
      "f1_score :  0.9829945820675833\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58690 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9905157685279846 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899259209632874 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9902829527854919 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916489124298096 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906709790229797 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910745620727539 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906865358352661 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914626479148865 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910280108451843 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9917886469118172\n",
      "Precision :  0.9682943475832971\n",
      "Recall :  0.9988479262672811\n",
      "f1_score :  0.9833338584165591\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58868 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9898017644882202 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914315938949585 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912298321723938 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913850426673889 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991633415222168 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908882975578308 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914315938949585 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914160966873169 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9914781987799388\n",
      "Precision :  0.9675121829314007\n",
      "Recall :  0.9981308411214953\n",
      "f1_score :  0.9825830398781764\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58730 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9914626479148865 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991540253162384 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915557503700256 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913539886474609 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917575716972351 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915557503700256 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916954636573792 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914781451225281 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918041229248047 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9919128261645686\n",
      "Precision :  0.9678067147787889\n",
      "Recall :  0.9994815294880104\n",
      "f1_score :  0.9833891280089272\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.991944947482796\n",
      "Precision :  0.9696028335849495\n",
      "Recall :  0.9980574980574981\n",
      "f1_score :  0.9836244219951109\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset2.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460167\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset2.csv': [[0, 0.7570208207020495], [1, 0.24297917929795051]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58951 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9906244277954102 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915247559547424 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920214414596558 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920369982719421 tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921301007270813 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920524954795837 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920990467071533 tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922077059745789 tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9921767070766652\n",
      "Precision :  0.968858776727996\n",
      "Recall :  0.999807568954458\n",
      "f1_score :  0.9840899046656986\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58845 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9875820279121399 tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9887462258338928 tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9892739653587341 tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.989398181438446 tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9892584681510925 tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895378947257996 tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897396564483643 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903916120529175 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904226660728455 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.990686556043649\n",
      "Precision :  0.9647856922602529\n",
      "Recall :  0.9981496841702291\n",
      "f1_score :  0.9811841445057703\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58954 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9896309971809387 tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900500774383545 tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909969568252563 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913539886474609 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911677241325378 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909193515777588 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919283390045166 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919903874397278 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9914316315601571\n",
      "Precision :  0.9665745007680492\n",
      "Recall :  0.9994917084948218\n",
      "f1_score :  0.982757543574686\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58946 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9907641410827637 tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914471507072449 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912142753601074 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916799664497375 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917730689048767 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918196797370911 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9911832213401794 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913074374198914 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914626479148865 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9919438709777564\n",
      "Precision :  0.9685321157524527\n",
      "Recall :  0.99942697058449\n",
      "f1_score :  0.9837370350640805\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64424\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58955 nodes and 64424 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1.edata['h'] after reshape :  64424\n",
      "Training acc: 0.9911524057388306 tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908574819564819 tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912765622138977 tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991897463798523 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916801452636719 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991835355758667 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919595122337341 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919440150260925 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918509125709534 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9920216068545884\n",
      "Precision :  0.9684289183635345\n",
      "Recall :  0.9995500996207982\n",
      "f1_score :  0.9837434372825604\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138051\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.8338077956697163\n",
      "Precision :  0.9154973739907502\n",
      "Recall :  0.348169568328166\n",
      "f1_score :  0.504481544675061\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset3.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460164\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset3.csv': [[0, 0.757532097252284], [1, 0.24246790274771604]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58972 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9887771606445312 tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.988978922367096 tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.989258348941803 tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897550940513611 tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895222187042236 tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895843267440796 tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898792505264282 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899103045463562 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906864166259766 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9905156623513707\n",
      "Precision :  0.9638569206842924\n",
      "Recall :  0.9980675083741304\n",
      "f1_score :  0.9806639450615526\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58863 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9913694262504578 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919437766075134 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919437766075134 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922076463699341 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924405217170715 tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916023015975952 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923784136772156 tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925181269645691 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920679330825806 tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.991943745925305\n",
      "Precision :  0.9685068774506753\n",
      "Recall :  0.9991652754590985\n",
      "f1_score :  0.98359723144022\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58848 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9914780855178833 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922076463699341 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922852516174316 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918661713600159 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920834898948669 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918816685676575 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915246367454529 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918351173400879 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992331862449646 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9923007668187886\n",
      "Precision :  0.9715203558411071\n",
      "Recall :  0.9977793287227967\n",
      "f1_score :  0.9844747715036934\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58787 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9920834898948669 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992021381855011 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921921491622925 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921610951423645 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.99226975440979 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922852516174316 tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.99226975440979 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922852516174316 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.99226975440979 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9922386762286175\n",
      "Precision :  0.9692519217548903\n",
      "Recall :  0.9994844364245666\n",
      "f1_score :  0.9841360492417031\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64426\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58857 nodes and 64426 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64426\n",
      "G1.edata['h'] after reshape :  64426\n",
      "Training acc: 0.9902213215827942 tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900816679000854 tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991618275642395 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991618275642395 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916958808898926 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916338324546814 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917890429496765 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917424917221069 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912458062171936 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9917735075901034\n",
      "Precision :  0.9677995320773304\n",
      "Recall :  0.9995548772733054\n",
      "f1_score :  0.9834209209209209\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.8367330677290836\n",
      "Precision :  0.904304097027067\n",
      "Recall :  0.36530935380754637\n",
      "f1_score :  0.5203957867858283\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "from dgl import from_networkx\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from dgl.data.utils import save_graphs\n",
    "\n",
    "#constante\n",
    "size_embedding = 152\n",
    "nb_batch = 5\n",
    "\n",
    "# Accuracy --------------------------------------------------------------------\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------ Model Architecture -----------------------------------------------------------------\n",
    "\n",
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        self.W_msg = nn.Linear(ndim_in + edims, ndim_out)\n",
    "        self.W_apply = nn.Linear(ndim_in + ndim_out, ndim_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        x = th.cat([edges.src['h'], edges.data['h']], 2)\n",
    "        y = self.W_msg(x)\n",
    "        return {'m': y}\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "        with g_dgl.local_scope():\n",
    "            g = g_dgl\n",
    "            g.ndata['h'] = nfeats\n",
    "            g.edata['h'] = efeats\n",
    "            # Line 4 of algorithm 1 : update all because we are using a full neighborhood sampling and not a k-hop neigh sampling\n",
    "            g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "            # Line 5 of algorithm 1\n",
    "            g.ndata['h'] = F.relu(self.W_apply(th.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "            return g.ndata['h']\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGELayer(ndim_in, edim, size_embedding, activation))\n",
    "        self.layers.append(SAGELayer(size_embedding, edim, size_embedding, activation)) ##\n",
    "        self.layers.append(SAGELayer(size_embedding, edim, ndim_out, activation))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                nfeats = self.dropout(nfeats)\n",
    "            nfeats = layer(g, nfeats, efeats)\n",
    "            # Save edge_embeddings\n",
    "            # nf = 'edge_embeddings'+str(i)+'.txt'\n",
    "            # sourceFile = open(nf, 'w')\n",
    "            # print(nfeats, file = sourceFile)\n",
    "        return nfeats.sum(1)\n",
    "        # Return a list of node features [[node1_feature1, node1_feature2, ...], [node2_feature1, node2_feature2, ...], ...]\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        v = th.cat([h_u, h_v], 1)\n",
    "        # if(pr == True):\n",
    "            # sourceFile = open(filename, 'w')\n",
    "            # if pr:\n",
    "                # print(v, file = sourceFile)\n",
    "            # sourceFile.close()\n",
    "        score = self.W(v)\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            # Update the features of the specified edges by the provided function\n",
    "            # DGLGraph.apply_edges(func, edges='__ALL__', etype=None, inplace=False)\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.gnn = SAGE(ndim_in, ndim_out, edim, activation, dropout)\n",
    "        self.pred = MLPPredictor(ndim_out, nbclasses)\n",
    "    def forward(self, g, nfeats, efeats, eweight = None):\n",
    "        if eweight != None:\n",
    "            # apply eweight on the graph\n",
    "            efe = []\n",
    "            for i, x in enumerate(eweight):\n",
    "                efe.append(list(th.Tensor.cpu(g.edata['h'][i][0]).detach().numpy() * th.Tensor.cpu(x).detach().numpy()))\n",
    "\n",
    "            efe = th.FloatTensor(efe).cuda()\n",
    "            efe = th.reshape(efe, (efe.shape[0], 1, efe.shape[1]))\n",
    "            g.edata['h'] = efe = efe\n",
    "\n",
    "        h = self.gnn(g, nfeats, efeats)\n",
    "        # h = list of node features [[node1_feature1, node1_feature2, ...], [node2_feature1, node2_feature2, ...], ...]\n",
    "        return self.pred(g, h)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# # --------------------------------------------------- MAIN -----------------------------------------------------------\n",
    "\n",
    "#Data\n",
    "nbclasses =  2\n",
    "\n",
    "\n",
    "# Model *******************************************************************************************\n",
    "# G1.ndata['h'].shape[2] = sizeh = 76 dans ANIDS\n",
    "# model1 = Model(G1.ndata['h'].shape[2], size_embedding, G1.ndata['h'].shape[2], F.relu, 0.2).cuda()\n",
    "model1 = Model(76, size_embedding, 76, F.relu, 0.2).cuda()\n",
    "opt = th.optim.Adam(model1.parameters())\n",
    "\n",
    "\n",
    "\n",
    "path, dirs, files = next(os.walk(\"/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/input/Dataset/GlobalDataset/Splitted/\"))\n",
    "file_count = len(files)\n",
    "\n",
    "\n",
    "for nb_files in range(file_count):\n",
    "    data1 = pd.read_csv(f'{path}{files[nb_files]}', encoding=\"ISO-8859–1\", dtype = str)\n",
    "\n",
    "    print(f'{files[nb_files]} ++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print(\"nb total instances in the file : \", len(data1.values))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    # Delete two columns (U and V in the excel)\n",
    "    cols = list(set(list(data1.columns )) - set(list(['Flow Bytes/s',' Flow Packets/s'])) )\n",
    "    data1 = data1[cols]\n",
    "\n",
    "    # Mise en forme des noeuds\n",
    "    data1[' Source IP'] = data1[' Source IP'].apply(str)\n",
    "    data1[' Source Port'] = data1[' Source Port'].apply(str)\n",
    "    data1[' Destination IP'] = data1[' Destination IP'].apply(str)\n",
    "    data1[' Destination Port'] = data1[' Destination Port'].apply(str)\n",
    "    data1[' Source IP'] = data1[' Source IP'] + ':' + data1[' Source Port']\n",
    "    data1[' Destination IP'] = data1[' Destination IP'] + ':' + data1[' Destination Port']\n",
    "\n",
    "    data1.drop(columns=['Flow ID',' Source Port',' Destination Port',' Timestamp'], inplace=True)\n",
    "\n",
    "    # -------------------- ????????????????????????????????????????? --------------------\n",
    "    # simply do : nom = list(data1[' Label'].unique())\n",
    "    nom = []\n",
    "    nom = nom + [data1[' Label'].unique()[0]]\n",
    "    for i in range(1, len(data1[' Label'].unique())):\n",
    "        nom = nom + [data1[' Label'].unique()[i]]\n",
    "    \n",
    "    nom.insert(0, nom.pop(nom.index('BENIGN')))\n",
    "\n",
    "    # Naming the two classes BENIGN {0} / Any Intrusion {1}\n",
    "    data1[' Label'].replace(nom[0], 0,inplace = True)\n",
    "    for i in range(1,len(data1[' Label'].unique())):\n",
    "        data1[' Label'].replace(nom[i], 1,inplace = True)\n",
    "    \n",
    "    ##################### LABELS FREQ #######################################\n",
    "    print()\n",
    "    print(\"labels freq after changing labels to binary\")\n",
    "    counts = list(data1[' Label'].value_counts().to_dict().items())\n",
    "    for j, x in enumerate(counts):\n",
    "        x = list(x)\n",
    "        x[1] = x[1] / len(data1)\n",
    "        counts[j] = x\n",
    "    print({f'{files[nb_files]}' : counts})\n",
    "    ##############################################################################\n",
    "\n",
    "    data1.rename(columns={\" Label\": \"label\"},inplace = True)\n",
    "    label1 = data1.label\n",
    "    data1.drop(columns=['label'],inplace = True)\n",
    "\n",
    "    # ******** At this step data1 contains only the data without label column\n",
    "    # ******** The label column is stored in the label variale \n",
    "\n",
    "    # split train and test\n",
    "    data1 =  pd.concat([data1, label1], axis=1) # ??????? WHY ?\n",
    "\n",
    "    # -------------------- ????????????????????????????????????????? --------------------\n",
    "    # X will contain the label column due to the concatination made earlier !!\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(data1, label1, test_size=0.3, random_state=123, stratify= label1)\n",
    "\n",
    "    # Create mini batches on the Train set\n",
    "    # 1st step : Duplicate instances of least populated classes (nb occ < 100 => x100)\n",
    "    for indx, x in enumerate(X1_train[\"label\"].value_counts()) :\n",
    "        if x < 100 :\n",
    "            inst = X1_train.loc[X1_train['label'] == X1_train[\"label\"].value_counts().index[indx]]\n",
    "            for i in range(int(100 / x)) :\n",
    "                X1_train = pd.concat([X1_train, inst], ignore_index = True)\n",
    "    \n",
    "    X1_train = shuffle(X1_train)\n",
    "    \n",
    "    # At this step we duplicated the least populated classes in the Train Set\n",
    "    # 2nd step : Create the mini batches\n",
    "    a = b = mean_macro_f1 = 0\n",
    "    for batch in range(1, nb_batch + 1):\n",
    "        print(f\"+++++++++++++++++ Batch {batch} ++++++++++++++++\")\n",
    "        a = b\n",
    "        b = int(len(X1_train) / nb_batch) * batch\n",
    "        if batch == nb_batch :\n",
    "            b = len(X1_train)\n",
    "        # The batch :\n",
    "        X1_train_batched = X1_train.iloc[a:b]\n",
    "        # y1_train_batched = y1_train.iloc[a:b]\n",
    "        y1_train_batched = X1_train_batched['label']\n",
    "\n",
    "        # Each batch will contain 64500 instance and all classes are present (The least populated one has > 10 instances)\n",
    "\n",
    "        print(\"nb Train instances : \", len(X1_train_batched.values))\n",
    "\n",
    "        # for non numerical attributes (categorical data)\n",
    "        # Since we have a binary classification, the category values willl be replaced with the posterior probability (p(target = Ti | category = Cj))\n",
    "        # TargetEncoding is also called MeanEncoding, cuz it simply replace each value with (target_i_count_on_category_j) / (total_occurences_of_category_j)\n",
    "        encoder1 = ce.TargetEncoder(cols=[' Protocol',  'Fwd PSH Flags', ' Fwd URG Flags', ' Bwd PSH Flags', ' Bwd URG Flags'])\n",
    "        encoder1.fit(X1_train_batched, y1_train_batched)\n",
    "        X1_train_batched = encoder1.transform(X1_train_batched)\n",
    "\n",
    "        # scaler (normalization)\n",
    "        scaler1 = StandardScaler()\n",
    "\n",
    "        # Manipulate flow content (all columns except : label, Source IP & Destination IP)\n",
    "        cols_to_norm1 = list(set(list(X1_train_batched.iloc[:, :].columns )) - set(list(['label', ' Source IP', ' Destination IP'])) )\n",
    "        X1_train_batched[cols_to_norm1] = scaler1.fit_transform(X1_train_batched[cols_to_norm1])\n",
    "\n",
    "        ## Create the h attribute that will contain the content of our flows\n",
    "        X1_train_batched['h'] = X1_train_batched[ cols_to_norm1 ].values.tolist()\n",
    "        # size of the list containig the content of our flows\n",
    "        sizeh = len(cols_to_norm1)\n",
    "\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Before training the data :\n",
    "        # We need to delete all the attributes (cols_to_norm1) to have the {Source IP, Destination IP, label, h} representation\n",
    "        X1_train_batched.drop(columns = cols_to_norm1, inplace = True)\n",
    "\n",
    "        # Then we need to Swap {label, h} Columns to have the {Source IP, Destination IP, h, label} representation\n",
    "        columns_titles = [' Source IP', ' Destination IP', 'h', 'label']\n",
    "        X1_train_batched = X1_train_batched.reindex(columns=columns_titles)\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # ------------------------------------------- Creating the Graph Representation -------------------------------------------------------------\n",
    "        # Create our Multigraph\n",
    "        G1 = nx.from_pandas_edgelist(X1_train_batched, \" Source IP\", \" Destination IP\", ['h','label'], create_using=nx.MultiDiGraph())\n",
    "        print(\"initial nx multigraph G1 : \", G1)\n",
    "\n",
    "        # Convert it to a directed Graph\n",
    "        # NB : IT WILL CREATE A DEFAULT BIDIRECTIONAL RELATIONSHIPS BETWEEN NODES, and not the original relationships ???????????????????????\n",
    "        # G1 = G1.to_directed()\n",
    "        # print(\"G1 after todirected : \", G1)\n",
    "        # Convert the graph from a networkx Graph to a DGL Graph\n",
    "        G1 = from_networkx(G1,edge_attrs=['h','label'] )\n",
    "        print(\"G1.edata['h'] after converting it to a dgl graph : \", len(G1.edata['h']))\n",
    "\n",
    "        # nodes data // G1.edata['h'].shape[1] : sizeh = number of attributes in a flow\n",
    "        G1.ndata['h'] = th.ones(G1.num_nodes(), G1.edata['h'].shape[1])\n",
    "        # edges data // we create a tensor bool array that will represent the train mask\n",
    "        G1.edata['train_mask'] = th.ones(len(G1.edata['h']), dtype=th.bool)\n",
    "\n",
    "        # Reshape both tensor lists to a single value in each element for both axis\n",
    "        G1.ndata['h'] = th.reshape(G1.ndata['h'], (G1.ndata['h'].shape[0], 1, G1.ndata['h'].shape[1]))\n",
    "        G1.edata['h'] = th.reshape(G1.edata['h'], (G1.edata['h'].shape[0], 1, G1.edata['h'].shape[1]))\n",
    "        print(\"G1.edata['h'] after reshape : \", len(G1.edata['h']))\n",
    "        # ------------------------------------------- --------------------------------- -------------------------------------------------------------\n",
    "\n",
    "        # ------------------------------------------- Model -----------------------------------------------------------------------------------------\n",
    "        ## use of model\n",
    "        from sklearn.utils import class_weight\n",
    "        class_weights1 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(G1.edata['label'].cpu().numpy()),\n",
    "                                                        y = G1.edata['label'].cpu().numpy())\n",
    "        ''' \n",
    "            Using class weights, you make the classifier aware of how to treat the various classes in the loss function.\n",
    "            In this process, you give higher weights to certain classes & lower weights to other classes.\n",
    "            Example : [ 0.51600999 16.11525117] \n",
    "            Basically : \n",
    "                - For classes with small number of training images, you give it more weight\n",
    "                so that the network will be punished more if it makes mistakes predicting the label of these classes. \n",
    "                - For classes with large numbers of images, you give it small weight\n",
    "        '''\n",
    "        class_weights1 = th.FloatTensor(class_weights1).cuda()\n",
    "        criterion1 = nn.CrossEntropyLoss(weight = class_weights1)\n",
    "        G1 = G1.to('cuda:0')\n",
    "\n",
    "        node_features1 = G1.ndata['h']\n",
    "        edge_features1 = G1.edata['h']\n",
    "\n",
    "        edge_label1 = G1.edata['label']\n",
    "        train_mask1 = G1.edata['train_mask']\n",
    "\n",
    "\n",
    "        # to print\n",
    "        pr = True\n",
    "        # True if you want to print the embedding vectors\n",
    "        # the name of the file where the vectors are printed\n",
    "        filename = './models/M1_weights.txt'\n",
    "\n",
    "        for epoch in range(1,1000):\n",
    "            pred = model1(G1, node_features1, edge_features1).cuda()\n",
    "            loss = criterion1(pred[train_mask1], edge_label1[train_mask1])\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training acc:', compute_accuracy(pred[train_mask1], edge_label1[train_mask1]), loss)\n",
    "\n",
    "        pred1 = model1(G1, node_features1, edge_features1).cuda()\n",
    "        pred1 = pred1.argmax(1)\n",
    "        pred1 = th.Tensor.cpu(pred1).detach().numpy()\n",
    "        edge_label1 = th.Tensor.cpu(edge_label1).detach().numpy()\n",
    "\n",
    "        print('Train metrics :')\n",
    "        print(\"Accuracy : \", sklearn.metrics.accuracy_score(edge_label1, pred1))\n",
    "        print(\"Precision : \", sklearn.metrics.precision_score(edge_label1, pred1, labels = [0,1]))\n",
    "        print(\"Recall : \", sklearn.metrics.recall_score(edge_label1, pred1, labels = [0,1]))\n",
    "        print(\"f1_score : \", sklearn.metrics.f1_score(edge_label1, pred1, labels=[0,1]))\n",
    "\n",
    "    # ------------------------------------------------ Test ---------------------------------------------------------------------\n",
    "    print(\"++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\")\n",
    "    print(\"nb Test instances : \", len(X1_test.values))\n",
    "    X1_test = encoder1.transform(X1_test)\n",
    "    X1_test[cols_to_norm1] = scaler1.transform(X1_test[cols_to_norm1])\n",
    "\n",
    "    # Save X1_test for XAI\n",
    "    X1_test.to_csv(f'/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/input/Dataset/XAI/X_test{nb_files}.csv', sep=',', index = False)\n",
    "\n",
    "    X1_test['h'] = X1_test[ cols_to_norm1 ].values.tolist()\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Before training the data :\n",
    "    # We need to delete all the attributes (cols_to_norm1) to have the {Source IP, Destination IP, label, h} representation\n",
    "    X1_test.drop(columns = cols_to_norm1, inplace = True)\n",
    "\n",
    "    # Then we need to Swap {label, h} Columns to have the {Source IP, Destination IP, h, label} representation\n",
    "    columns_titles = [' Source IP', ' Destination IP', 'h', 'label']\n",
    "    X1_test=X1_test.reindex(columns=columns_titles)\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    G1_test = nx.from_pandas_edgelist(X1_test, \" Source IP\", \" Destination IP\", ['h','label'],create_using=nx.MultiDiGraph())\n",
    "    # G1_test = G1_test.to_directed()\n",
    "    G1_test = from_networkx(G1_test,edge_attrs=['h','label'] )\n",
    "    # actual1 = G1_test.edata.pop('label')\n",
    "    actual1 = G1_test.edata['label']\n",
    "    G1_test.ndata['feature'] = th.ones(G1_test.num_nodes(), G1.ndata['h'].shape[2])\n",
    "    G1_test.ndata['feature'] = th.reshape(G1_test.ndata['feature'], (G1_test.ndata['feature'].shape[0], 1, G1_test.ndata['feature'].shape[1]))\n",
    "    G1_test.edata['h'] = th.reshape(G1_test.edata['h'], (G1_test.edata['h'].shape[0], 1, G1_test.edata['h'].shape[1]))\n",
    "    G1_test = G1_test.to('cuda:0')\n",
    "    node_features_test1 = G1_test.ndata['feature']\n",
    "    edge_features_test1 = G1_test.edata['h']\n",
    "\n",
    "    # to print\n",
    "    pr = True\n",
    "    # True if you want to print the embedding vectors\n",
    "    # the name of the file where the vectors are printed\n",
    "    filename = './models/M1_weights.txt'\n",
    "\n",
    "    print(\"nb instances : \", len(X1_test.values))\n",
    "\n",
    "    test_pred1 = model1(G1_test, node_features_test1, edge_features_test1).cuda()\n",
    "    test_pred1 = test_pred1.argmax(1)\n",
    "    test_pred1 = th.Tensor.cpu(test_pred1).detach().numpy()\n",
    "\n",
    "    print('Metrics : ')\n",
    "    print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1, test_pred1))\n",
    "    print(\"Precision : \", sklearn.metrics.precision_score(actual1, test_pred1, labels = [0,1]))\n",
    "    print(\"Recall : \", sklearn.metrics.recall_score(actual1, test_pred1, labels = [0,1]))\n",
    "    print(\"f1_score : \", sklearn.metrics.f1_score(actual1, test_pred1, labels = [0,1]))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-blues",
   "metadata": {},
   "source": [
    "# XAI : explain_edges & explain_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fresh-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "from dgl import EID, NID, khop_out_subgraph\n",
    "\n",
    "\n",
    "# init mask\n",
    "def init_masks(graph, efeat):\n",
    "    # efeat.size() = torch.Size([nb_edges, 1, 76])\n",
    "    efeat_size = efeat.size()[2]\n",
    "    num_edges = graph.num_edges()\n",
    "    num_nodes = graph.num_nodes()\n",
    "    device = efeat.device\n",
    "    std = 0.1\n",
    "    # feat_mask = [[f1, f2, .... fn]] / n = nb_features\n",
    "    efeat_mask = nn.Parameter(th.randn(1, efeat_size, device=device) * std)\n",
    "    std = nn.init.calculate_gain(\"relu\") * sqrt(2.0 / (2 * num_nodes))\n",
    "    # edge_mask = [e1, e2, .... em] / m = nb_edges\n",
    "    edge_mask = nn.Parameter(th.randn(num_edges, device=device) * std)\n",
    "    return efeat_mask, edge_mask\n",
    "\n",
    "\n",
    "# Regularization loss\n",
    "def loss_regularize(loss, feat_mask, edge_mask):\n",
    "    # epsilon for numerical stability\n",
    "    eps = 1e-15\n",
    "    # From self GNNExplainer self\n",
    "    alpha1 = 0.005,\n",
    "    alpha2 = 1.0\n",
    "    beta1 = 1.0\n",
    "    beta2 = 0.1\n",
    "\n",
    "    edge_mask = edge_mask.sigmoid()\n",
    "    # Edge mask sparsity regularization\n",
    "    loss = loss + th.from_numpy(alpha1 * th.Tensor.cpu(th.sum(edge_mask)).detach().numpy()).cuda()\n",
    "    # Edge mask entropy regularization\n",
    "    ent = -edge_mask * th.log(edge_mask + eps) - (\n",
    "        1 - edge_mask\n",
    "    ) * th.log(1 - edge_mask + eps)\n",
    "    loss = loss + alpha2 * ent.mean()\n",
    "\n",
    "    feat_mask = feat_mask.sigmoid()\n",
    "    # Feature mask sparsity regularization\n",
    "    loss = loss + beta1 * th.mean(feat_mask)\n",
    "    # Feature mask entropy regularization\n",
    "    ent = -feat_mask * th.log(feat_mask + eps) - (\n",
    "        1 - feat_mask\n",
    "    ) * th.log(1 - feat_mask + eps)\n",
    "    loss = loss + beta2 * ent.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Edge features\n",
    "def explain_edge_features(model, graph, node_feat, edge_feat, **kwargs):\n",
    "    model = model.to(graph.device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "    print(\"graph : \", graph)\n",
    "    print(\"graph_edges : \", graph.edges()) # edges ids in graph.edges()\n",
    "    print(\"graph_nodes : \", graph.nodes()) # nodes ids in graph.nodes()\n",
    "    print(\"edge_feat : \", edge_feat)\n",
    "    print(\"node_feat : \", node_feat)\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "\n",
    "    # If we add kwargs\n",
    "    # for key, item in kwargs.items():\n",
    "    #     if th.is_tensor(item) and item.size(0) == num_nodes:\n",
    "    #         item = item[sg_nodes]\n",
    "    #     elif th.is_tensor(item) and item.size(0) == num_edges:\n",
    "    #         item = item[sg_edges]\n",
    "    #     kwargs[key] = item\n",
    "\n",
    "    # Get the initial prediction.\n",
    "    print(\"Get the initial prediction :\")\n",
    "    with th.no_grad():\n",
    "        # logits = model(g = sg, nfeats = node_feat, efeats = edge_feat, **kwargs)\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat)\n",
    "        pred_label = logits.argmax(dim=-1)\n",
    "        # pred_label1 = logits.argmax(1)\n",
    "\n",
    "    print(\"pred_label : \", pred_label)\n",
    "\n",
    "    #\n",
    "    efeat_mask, edge_mask = init_masks(graph, edge_feat)\n",
    "\n",
    "    params = [efeat_mask]\n",
    "    # params = [efeat_mask]\n",
    "    # lr=0.01\n",
    "    optimizer = th.optim.Adam(params, lr = 0.01)\n",
    "\n",
    "    # num_epochs = 100\n",
    "    print(\"***********************************\")\n",
    "    print(\"initial masks : \")\n",
    "    print(\"efeat_mask : \", efeat_mask)\n",
    "    print(\"***********************************\")\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights2 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(graph.edata['label'].cpu().numpy()),\n",
    "                                                        y = graph.edata['label'].cpu().numpy())\n",
    "    class_weights2 = th.FloatTensor(class_weights2).cuda()\n",
    "    criterion2 = nn.CrossEntropyLoss(weight = class_weights2)\n",
    "    train_mask2 = th.ones(len(graph.edata['h']), dtype=th.bool)\n",
    "    import datetime\n",
    "    \n",
    "    print(f'explanation starts at {datetime.datetime.now()}')\n",
    "    print(\"nb edges : \", graph.num_edges())\n",
    "    print(\"nb nodes : \", graph.num_nodes())\n",
    "    for epoch in range(1,300):\n",
    "        optimizer.zero_grad()\n",
    "        # Matrix multiplication\n",
    "        h = edge_feat * efeat_mask.sigmoid()\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = h).cuda()\n",
    "        # pred_label = tensor([0, 0, 0,  ..., 0, 1, 0], device='cuda:0')\n",
    "        # logits = tensor([[ 0.0059,  0.0517], [-0.0075,  0.0101], ..., device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "        loss = criterion2(logits[train_mask2], pred_label[train_mask2])\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"+++++++++++++++\")\n",
    "            print(f'epoch number {epoch}, CrossEntropy_Loss = {loss}, time = {datetime.datetime.now()}')\n",
    "            print(\"efeat_mask : \", efeat_mask.detach().sigmoid())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    efeat_mask = efeat_mask.detach().sigmoid().squeeze()\n",
    "\n",
    "    return efeat_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Edge mask\n",
    "def explain_edges(model, graph, node_feat, edge_feat, **kwargs):\n",
    "    model = model.to(graph.device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "    print(\"graph : \", graph)\n",
    "    print(\"graph_edges : \", graph.edges()) # edges ids in graph.edges()\n",
    "    print(\"graph_nodes : \", graph.nodes()) # nodes ids in graph.nodes()\n",
    "    print(\"edge_feat : \", edge_feat)\n",
    "    print(\"node_feat : \", node_feat)\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "\n",
    "    # If we add kwargs\n",
    "    # for key, item in kwargs.items():\n",
    "    #     if th.is_tensor(item) and item.size(0) == num_nodes:\n",
    "    #         item = item[sg_nodes]\n",
    "    #     elif th.is_tensor(item) and item.size(0) == num_edges:\n",
    "    #         item = item[sg_edges]\n",
    "    #     kwargs[key] = item\n",
    "\n",
    "    # Get the initial prediction.\n",
    "    print(\"Get the initial prediction :\")\n",
    "    with th.no_grad():\n",
    "        # logits = model(g = sg, nfeats = node_feat, efeats = edge_feat, **kwargs)\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat)\n",
    "        pred_label = logits.argmax(dim=-1)\n",
    "        # pred_label1 = logits.argmax(1)\n",
    "\n",
    "    print(\"pred_label : \", pred_label)\n",
    "\n",
    "    #\n",
    "    efeat_mask, edge_mask = init_masks(graph, edge_feat)\n",
    "\n",
    "    params = [edge_mask]\n",
    "    # params = [efeat_mask]\n",
    "    # lr=0.01\n",
    "    optimizer = th.optim.Adam(params, lr = 0.01)\n",
    "\n",
    "    # num_epochs = 100\n",
    "    print(\"***********************************\")\n",
    "    print(\"initial masks : \")\n",
    "    print(\"edge_mask : \", edge_mask)\n",
    "    print(\"***********************************\")\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights2 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(graph.edata['label'].cpu().numpy()),\n",
    "                                                        y = graph.edata['label'].cpu().numpy())\n",
    "    class_weights2 = th.FloatTensor(class_weights2).cuda()\n",
    "    criterion2 = nn.CrossEntropyLoss(weight = class_weights2)\n",
    "    train_mask2 = th.ones(len(graph.edata['h']), dtype=th.bool)\n",
    "    import datetime\n",
    "    \n",
    "    print(f'explanation starts at {datetime.datetime.now()}')\n",
    "    print(\"nb edges : \", graph.num_edges())\n",
    "    print(\"nb nodes : \", graph.num_nodes())\n",
    "    for epoch in range(1,300):\n",
    "        optimizer.zero_grad()\n",
    "        # Matrix multiplication\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat, eweight=edge_mask.sigmoid()).cuda()\n",
    "        # pred_label = tensor([0, 0, 0,  ..., 0, 1, 0], device='cuda:0')\n",
    "        # logits = tensor([[ 0.0059,  0.0517], [-0.0075,  0.0101], ..., device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "        loss11 = criterion2(logits[train_mask2], pred_label[train_mask2])\n",
    "        loss = loss_regularize(loss11, efeat_mask, edge_mask)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"+++++++++++++++\")\n",
    "            print(f'epoch number {epoch}, CrossEntropy_Loss = {loss11}, final_loss = {loss}, time = {datetime.datetime.now()}')\n",
    "            print(\"edge_mask : \", edge_mask.detach().sigmoid())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    edge_mask = edge_mask.detach().sigmoid()\n",
    "\n",
    "    return edge_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-profession",
   "metadata": {},
   "source": [
    "# XAI : Results interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imposed-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++\n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "graph_edges :  (tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "graph_nodes :  tensor([     0,      1,      2,  ..., 104637, 104638, 104639], device='cuda:0')\n",
      "edge_feat :  tensor([[[ 0.0043,  0.7111,  1.6772,  ...,  0.0000, -0.0158,  0.1105]],\n",
      "\n",
      "        [[ 0.0043,  0.7111, -0.2094,  ...,  0.0000, -0.0158, -0.2778]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2080,  ...,  0.0000, -0.0158,  4.2246]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.0768,  ...,  0.0000, -0.0158,  0.3723]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2072,  ...,  0.0000, -0.0158,  0.2599]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2073,  ...,  0.0000, -0.0158,  0.6878]]],\n",
      "       device='cuda:0')\n",
      "node_feat :  tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "Get the initial prediction :\n",
      "pred_label :  tensor([0, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "***********************************\n",
      "initial masks : \n",
      "efeat_mask :  Parameter containing:\n",
      "tensor([[-0.0875,  0.0391, -0.0286, -0.0006, -0.1264,  0.0409, -0.0359,  0.0429,\n",
      "         -0.0367,  0.2013, -0.0849, -0.1532,  0.0062, -0.0256,  0.0632, -0.1974,\n",
      "         -0.0793,  0.1561,  0.0729,  0.0950,  0.0722, -0.0420,  0.0405,  0.0258,\n",
      "          0.0585,  0.0029, -0.0694, -0.1459,  0.0957, -0.1464,  0.0311, -0.0690,\n",
      "         -0.0003, -0.0515, -0.0763,  0.0078,  0.1383, -0.0343,  0.0481,  0.0719,\n",
      "          0.0925,  0.1034, -0.0149,  0.2045, -0.0299,  0.0476,  0.0312, -0.0284,\n",
      "         -0.1813, -0.0474, -0.0862,  0.0582,  0.0003,  0.1025, -0.0426, -0.0869,\n",
      "         -0.0627, -0.0879,  0.0908, -0.0316, -0.0265,  0.0946, -0.0320, -0.0838,\n",
      "         -0.0043, -0.0061, -0.1333,  0.0718, -0.0429, -0.2182, -0.0040, -0.0450,\n",
      "         -0.0657, -0.2312, -0.0362,  0.0485]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "***********************************\n",
      "explanation starts at 2023-05-17 11:07:18.096709\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "+++++++++++++++\n",
      "epoch number 100, CrossEntropy_Loss = 0.059856437146663666, time = 2023-05-17 11:07:26.453799\n",
      "efeat_mask :  tensor([[0.4904, 0.4070, 0.5242, 0.4609, 0.4684, 0.5384, 0.4271, 0.6207, 0.4771,\n",
      "         0.5302, 0.4788, 0.3536, 0.6779, 0.6044, 0.3882, 0.3563, 0.6005, 0.5390,\n",
      "         0.6518, 0.6059, 0.4501, 0.4257, 0.6340, 0.4618, 0.5146, 0.4313, 0.5834,\n",
      "         0.4827, 0.4443, 0.6201, 0.6034, 0.5677, 0.5799, 0.4150, 0.3427, 0.3420,\n",
      "         0.6174, 0.4775, 0.5959, 0.5180, 0.3865, 0.4244, 0.5778, 0.5637, 0.4187,\n",
      "         0.4304, 0.7152, 0.4929, 0.2901, 0.4131, 0.5926, 0.4434, 0.5727, 0.5256,\n",
      "         0.3951, 0.4123, 0.4050, 0.4106, 0.4504, 0.4186, 0.5273, 0.4381, 0.4134,\n",
      "         0.4008, 0.5832, 0.6967, 0.5572, 0.4228, 0.4228, 0.3831, 0.5792, 0.3941,\n",
      "         0.4021, 0.4424, 0.3516, 0.5942]], device='cuda:0')\n",
      "+++++++++++++++\n",
      "epoch number 200, CrossEntropy_Loss = 0.05064254254102707, time = 2023-05-17 11:07:34.816399\n",
      "efeat_mask :  tensor([[0.4339, 0.3588, 0.4489, 0.5423, 0.4684, 0.5224, 0.4710, 0.6009, 0.4339,\n",
      "         0.6300, 0.4788, 0.3006, 0.8027, 0.6281, 0.3863, 0.3338, 0.5870, 0.5390,\n",
      "         0.7624, 0.6147, 0.4750, 0.4589, 0.7752, 0.5256, 0.5146, 0.4545, 0.6244,\n",
      "         0.4651, 0.4472, 0.6109, 0.6390, 0.5846, 0.5860, 0.4306, 0.2409, 0.2961,\n",
      "         0.6272, 0.4361, 0.6058, 0.5180, 0.3621, 0.4099, 0.5844, 0.5080, 0.4308,\n",
      "         0.4323, 0.8346, 0.4929, 0.2206, 0.4354, 0.6190, 0.4612, 0.5677, 0.5256,\n",
      "         0.3603, 0.4367, 0.4118, 0.4408, 0.4670, 0.4350, 0.6574, 0.4426, 0.4202,\n",
      "         0.4090, 0.5931, 0.7902, 0.5777, 0.3191, 0.4492, 0.4164, 0.5844, 0.3755,\n",
      "         0.4014, 0.4424, 0.2592, 0.6015]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "graph_edges :  (tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "graph_nodes :  tensor([     0,      1,      2,  ..., 104637, 104638, 104639], device='cuda:0')\n",
      "edge_feat :  tensor([[[ 0.0043,  0.7111,  1.6772,  ...,  0.0000, -0.0158,  0.1105]],\n",
      "\n",
      "        [[ 0.0043,  0.7111, -0.2094,  ...,  0.0000, -0.0158, -0.2778]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2080,  ...,  0.0000, -0.0158,  4.2246]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.0768,  ...,  0.0000, -0.0158,  0.3723]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2072,  ...,  0.0000, -0.0158,  0.2599]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2073,  ...,  0.0000, -0.0158,  0.6878]]],\n",
      "       device='cuda:0')\n",
      "node_feat :  tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "Get the initial prediction :\n",
      "pred_label :  tensor([0, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "***********************************\n",
      "initial masks : \n",
      "edge_mask :  Parameter containing:\n",
      "tensor([-0.0012, -0.0024,  0.0011,  ...,  0.0038,  0.0004, -0.0024],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "***********************************\n",
      "explanation starts at 2023-05-17 11:07:43.191957\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "+++++++++++++++\n",
      "epoch number 100, CrossEntropy_Loss = 0.016169250011444092, final_loss = tensor([346.8345], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), time = 2023-05-17 11:19:42.266042\n",
      "edge_mask :  tensor([0.2194, 0.2182, 0.7805,  ..., 0.7826, 0.7786, 0.2182], device='cuda:0')\n",
      "+++++++++++++++\n",
      "epoch number 200, CrossEntropy_Loss = 0.016169250011444092, final_loss = tensor([346.8889], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), time = 2023-05-17 11:31:40.175180\n",
      "edge_mask :  tensor([0.0744, 0.0743, 0.9256,  ..., 0.9257, 0.9255, 0.0743], device='cuda:0')\n",
      "\n",
      "final results : \n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "efeat_mask :  tensor([0.3624, 0.3053, 0.4011, 0.6265, 0.4684, 0.5058, 0.5250, 0.6237, 0.4145,\n",
      "        0.7254, 0.4788, 0.2508, 0.8700, 0.6208, 0.3584, 0.3207, 0.6174, 0.5390,\n",
      "        0.8420, 0.6258, 0.5058, 0.5022, 0.8557, 0.6025, 0.5146, 0.4830, 0.6830,\n",
      "        0.4252, 0.4523, 0.6280, 0.6795, 0.6054, 0.5956, 0.4511, 0.1704, 0.2632,\n",
      "        0.6384, 0.4177, 0.6171, 0.5180, 0.3134, 0.4366, 0.5919, 0.4324, 0.4463,\n",
      "        0.4352, 0.8905, 0.4929, 0.2334, 0.4642, 0.6108, 0.4837, 0.5698, 0.5256,\n",
      "        0.3216, 0.4700, 0.4192, 0.4807, 0.4887, 0.4572, 0.7892, 0.4667, 0.4249,\n",
      "        0.4151, 0.6048, 0.8296, 0.6033, 0.2194, 0.4839, 0.4621, 0.5928, 0.3670,\n",
      "        0.4024, 0.4424, 0.1924, 0.6106], device='cuda:0')\n",
      "edge_mask :  tensor([0.0338, 0.0338, 0.9662,  ..., 0.9662, 0.9663, 0.0338], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "efeat_mask = explain_edge_features(model1, G1_test, node_features_test1, edge_features_test1)\n",
    "edge_mask = explain_edges(model1, G1_test, node_features_test1, edge_features_test1)\n",
    "\n",
    "print()\n",
    "print(\"final results : \")\n",
    "print(\"graph : \", G1_test)\n",
    "print(\"nb edges : \", G1_test.num_edges())\n",
    "print(\"nb nodes : \", G1_test.num_nodes())\n",
    "print(\"efeat_mask : \", efeat_mask)\n",
    "print(\"edge_mask : \", edge_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "laughing-humanity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138050\n",
      "tensor([0., 0., 1.,  ..., 1., 1., 0.], device='cuda:0')\n",
      "(tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 7.9992e-32,  1.3074e-29, -3.8238e-30,  ...,  0.0000e+00,\n",
      "          -2.8976e-31,  7.7667e-29]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.7460e-31,  2.8271e-29, -3.0537e-30,  ...,  0.0000e+00,\n",
      "          -6.2660e-31,  1.4801e-29]],\n",
      "\n",
      "        [[ 4.0694e-32,  6.6517e-30, -1.9378e-30,  ...,  0.0000e+00,\n",
      "          -1.4742e-31,  2.4310e-30]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(len(edge_mask))\n",
    "print(th.round(edge_mask))\n",
    "\n",
    "print(G1_test.edges())\n",
    "print(G1_test.edata['h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-practitioner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "round-guide",
   "metadata": {},
   "source": [
    "# E-GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "responsible-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIC-IDS-2017-Dataset4.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460167\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset4.csv': [[0, 0.7582073464633492], [1, 0.24179265353665083]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59085 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9857814311981201 tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9871008396148682 tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9883581399917603 tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893671274185181 tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908262491226196 tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893671274185181 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899259209632874 tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9896775484085083 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904226660728455 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9903450630985828\n",
      "Precision :  0.9636744243319788\n",
      "Recall :  0.9979597041571028\n",
      "f1_score :  0.9805174465952514\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58747 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9906089305877686 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911521673202515 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917886257171631 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918196797370911 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918196797370911 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918351769447327 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9870542883872986 tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917886257171631 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991850733757019 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9918507365381929\n",
      "Precision :  0.9680969457153457\n",
      "Recall :  0.9994255441373587\n",
      "f1_score :  0.983511824377375\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58934 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9865575432777405 tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895068407058716 tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897396564483643 tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9892429113388062 tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906089305877686 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906554818153381 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903760552406311 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910280108451843 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913539886474609 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9906710336370551\n",
      "Precision :  0.9630961979780438\n",
      "Recall :  0.9996137504828119\n",
      "f1_score :  0.9810152572890672\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58841 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9899569749832153 tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895844459533691 tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893515706062317 tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9887151718139648 tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902053475379944 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902829527854919 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9890100955963135 tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905468225479126 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908728003501892 tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9906089440106794\n",
      "Precision :  0.9646816992649807\n",
      "Recall :  0.9975523349436393\n",
      "f1_score :  0.9808416985971691\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64424\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58869 nodes and 64424 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1.edata['h'] after reshape :  64424\n",
      "Training acc: 0.9919905662536621 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918819069862366 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923475980758667 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924717545509338 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923165440559387 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922544360160828 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921768307685852 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925028085708618 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923475980758667 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9924717496585124\n",
      "Precision :  0.9701417638941162\n",
      "Recall :  0.9994184168012924\n",
      "f1_score :  0.9845624980106312\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138051\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.9919015436324257\n",
      "Precision :  0.972136041678862\n",
      "Recall :  0.995026962252846\n",
      "f1_score :  0.9834483167026915\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset1.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460165\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset1.csv': [[0, 0.7577868807927591], [1, 0.24221311920724087]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58895 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9911521673202515 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912453293800354 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914471507072449 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917265176773071 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991633415222168 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905778765678406 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991850733757019 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915557503700256 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919438362121582 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9919438709777564\n",
      "Precision :  0.969361595698118\n",
      "Recall :  0.9981328869430852\n",
      "f1_score :  0.9835368754956384\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial nx multigraph G1 :  MultiDiGraph with 58964 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9909193515777588 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9901432394981384 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991260826587677 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991291880607605 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991509199142456 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906244277954102 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899569749832153 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914781451225281 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903916120529175 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9916799900656598\n",
      "Precision :  0.9690232787289075\n",
      "Recall :  0.9979071537290716\n",
      "f1_score :  0.9832531400362432\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58996 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9672166705131531 tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9871008396148682 tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9879545569419861 tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9879235625267029 tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9888548851013184 tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991260826587677 tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893671274185181 tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895223379135132 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9889635443687439 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9897552116480139\n",
      "Precision :  0.9601827273288475\n",
      "Recall :  0.999036546984392\n",
      "f1_score :  0.9792243767313019\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58958 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9893671274185181 tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906865358352661 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990220844745636 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911211729049683 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899259209632874 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991291880607605 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914626479148865 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904226660728455 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910901188850403 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.991447153966751\n",
      "Precision :  0.9681050656660413\n",
      "Recall :  0.9973584176277301\n",
      "f1_score :  0.9825140427152423\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58819 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9903295040130615 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902829527854919 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909038543701172 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916178584098816 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913850426673889 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908572435379028 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991322934627533 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991540253162384 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911987781524658 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9917265572854416\n",
      "Precision :  0.9683563938698269\n",
      "Recall :  0.9985284708893154\n",
      "f1_score :  0.983211012064132\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.8385874683085839\n",
      "Precision :  0.9158899410931325\n",
      "Recall :  0.3673365631915784\n",
      "f1_score :  0.5243655147388417\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset0.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460165\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset0.csv': [[0, 0.7576130301087654], [1, 0.24238696989123468]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58786 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9921922087669373 tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992704451084137 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9929372668266296 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9928751587867737 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9929062128067017 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9928441643714905 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9929372668266296 tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992952823638916 tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9929838180541992 tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9929838722195489\n",
      "Precision :  0.9720603896510681\n",
      "Recall :  0.9996134767763963\n",
      "f1_score :  0.9856444133900781\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58667 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9880632162094116 tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9883891940116882 tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.988109827041626 tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9886065125465393 tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9892429113388062 tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897862076759338 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990189790725708 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9894136786460876 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990531325340271 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9897241668348261\n",
      "Precision :  0.9600293811593316\n",
      "Recall :  0.9994264958898872\n",
      "f1_score :  0.9793318763659069\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58892 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9897551536560059 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9892274141311646 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9897086024284363 tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898483157157898 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899104237556458 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903760552406311 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904226660728455 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900811314582825 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.99015873670578 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9907331232634308\n",
      "Precision :  0.9646711847389559\n",
      "Recall :  0.9977932108781723\n",
      "f1_score :  0.9809526848100055\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58761 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9905933737754822 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904847145080566 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909349083900452 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904381632804871 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906399846076965 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9896154999732971 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902829527854919 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914626479148865 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911832213401794 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9913695419337815\n",
      "Precision :  0.9667567899717341\n",
      "Recall :  0.9990474980949962\n",
      "f1_score :  0.9826369371057398\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58829 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9910125136375427 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914471507072449 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917730689048767 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916489124298096 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914471507072449 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917575716972351 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916489124298096 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.99160236120224 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916644096374512 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.990298495878801\n",
      "Precision :  0.9623497669855285\n",
      "Recall :  0.9992995861190703\n",
      "f1_score :  0.9804766813482022\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.991944947482796\n",
      "Precision :  0.9693303156917363\n",
      "Recall :  0.9983563445101906\n",
      "f1_score :  0.9836292435886111\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset2.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460167\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset2.csv': [[0, 0.7570208207020495], [1, 0.24297917929795051]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58990 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9910280108451843 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909193515777588 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911987781524658 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911056160926819 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911211729049683 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911211729049683 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914626479148865 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913694858551025 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914471507072449 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9915092435931266\n",
      "Precision :  0.9671145469047181\n",
      "Recall :  0.999302649930265\n",
      "f1_score :  0.982945156362049\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58942 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9915557503700256 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919748902320862 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917886257171631 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913539886474609 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919283390045166 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922077059745789 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916489124298096 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921611547470093 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919438362121582 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9919904381975382\n",
      "Precision :  0.9696780633890691\n",
      "Recall :  0.9980734651939378\n",
      "f1_score :  0.9836708860759493\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58858 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9848811626434326 tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897241592407227 tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907330870628357 tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911521673202515 tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914781451225281 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921611547470093 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920835494995117 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921922087669373 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9922853639228226\n",
      "Precision :  0.9696522006771314\n",
      "Recall :  0.9997461284589998\n",
      "f1_score :  0.9844692353363957\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59133 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9923474192619324 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924560785293579 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9926578998565674 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9927510023117065 tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9930459260940552 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9928286075592041 tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9930924773216248 tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9929838180541992 tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9930148720741272 tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9931235738788942\n",
      "Precision :  0.9725865605768633\n",
      "Recall :  0.9998721881390593\n",
      "f1_score :  0.9860406491255711\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64424\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59084 nodes and 64424 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1.edata['h'] after reshape :  64424\n",
      "Training acc: 0.9906401634216309 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906867146492004 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909350872039795 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912765622138977 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905625581741333 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911057949066162 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919285178184509 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991866409778595 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992083728313446 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9922854836706817\n",
      "Precision :  0.9691990503561164\n",
      "Recall :  0.9997422182122833\n",
      "f1_score :  0.9842337340989119\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138051\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.991510383843652\n",
      "Precision :  0.9674792051756007\n",
      "Recall :  0.9986286668256619\n",
      "f1_score :  0.9828071822556038\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset3.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460164\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset3.csv': [[0, 0.757532097252284], [1, 0.24246790274771604]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59046 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9889013171195984 tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897705912590027 tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904225468635559 tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990888237953186 tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908727407455444 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909347891807556 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990888237953186 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911831617355347 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990919291973114 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9910900003104529\n",
      "Precision :  0.9666645896940619\n",
      "Recall :  0.9974924451874236\n",
      "f1_score :  0.9818365926207202\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58862 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.991198718547821 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991741955280304 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918351173400879 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914159774780273 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915712475776672 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911831617355347 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917264580726624 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911831617355347 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918661713600159 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9918971779826767\n",
      "Precision :  0.9687944138911203\n",
      "Recall :  0.9989168525007964\n",
      "f1_score :  0.9836250705815923\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58823 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9911831617355347 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991741955280304 tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992052435874939 tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992052435874939 tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922387003898621 tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992300808429718 tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922387003898621 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.99226975440979 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922542572021484 tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9922541988761603\n",
      "Precision :  0.969356233782281\n",
      "Recall :  0.9998088319632957\n",
      "f1_score :  0.9843470623294331\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58936 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9889013171195984 tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900500178337097 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898637533187866 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9890255331993103 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897861480712891 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910434484481812 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912297129631042 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910590052604675 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910745024681091 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9912452267858806\n",
      "Precision :  0.967174238642037\n",
      "Recall :  0.9975540679711637\n",
      "f1_score :  0.98212927756654\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64426\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58871 nodes and 64426 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64426\n",
      "G1.edata['h'] after reshape :  64426\n",
      "Training acc: 0.9906249046325684 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991618275642395 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912302494049072 tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913234114646912 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910595417022705 tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913234114646912 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912923574447632 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915096759796143 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914475679397583 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9919287244280259\n",
      "Precision :  0.9693034238488784\n",
      "Recall :  0.998336\n",
      "f1_score :  0.9836055236774072\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9880912712785223\n",
      "Precision :  0.956198939372223\n",
      "Recall :  0.9965345203596929\n",
      "f1_score :  0.9759501448257701\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "from dgl import from_networkx\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from dgl.data.utils import save_graphs\n",
    "\n",
    "#constante\n",
    "size_embedding = 152\n",
    "nb_batch = 5\n",
    "\n",
    "# Accuracy --------------------------------------------------------------------\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------ Model Architecture -----------------------------------------------------------------\n",
    "\n",
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        self.W_msg = nn.Linear(ndim_in + edims, ndim_out)\n",
    "        self.W_apply = nn.Linear(ndim_in + ndim_out, ndim_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        x = th.cat([edges.src['h'], edges.data['h']], 2)\n",
    "        y = self.W_msg(x)\n",
    "        return {'m': y}\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "        with g_dgl.local_scope():\n",
    "            g = g_dgl\n",
    "            g.ndata['h'] = nfeats\n",
    "            g.edata['h'] = efeats\n",
    "            # Line 4 of algorithm 1 : update all because we are using a full neighborhood sampling and not a k-hop neigh sampling\n",
    "            g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "            # Line 5 of algorithm 1\n",
    "            g.ndata['h'] = F.relu(self.W_apply(th.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "            return g.ndata['h']\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGELayer(ndim_in, edim, size_embedding, activation))\n",
    "        self.layers.append(SAGELayer(size_embedding, edim, size_embedding, activation)) ##\n",
    "        self.layers.append(SAGELayer(size_embedding, edim, ndim_out, activation))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                nfeats = self.dropout(nfeats)\n",
    "            nfeats = layer(g, nfeats, efeats)\n",
    "            # Save edge_embeddings\n",
    "            # nf = 'edge_embeddings'+str(i)+'.txt'\n",
    "            # sourceFile = open(nf, 'w')\n",
    "            # print(nfeats, file = sourceFile)\n",
    "        return nfeats.sum(1)\n",
    "        # Return a list of node features [[node1_feature1, node1_feature2, ...], [node2_feature1, node2_feature2, ...], ...]\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        v = th.cat([h_u, h_v], 1)\n",
    "        # if(pr == True):\n",
    "            # sourceFile = open(filename, 'w')\n",
    "            # if pr:\n",
    "                # print(v, file = sourceFile)\n",
    "            # sourceFile.close()\n",
    "        score = self.W(v)\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            # Update the features of the specified edges by the provided function\n",
    "            # DGLGraph.apply_edges(func, edges='__ALL__', etype=None, inplace=False)\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.gnn = SAGE(ndim_in, ndim_out, edim, activation, dropout)\n",
    "        self.pred = MLPPredictor(ndim_out, nbclasses)\n",
    "    def forward(self, g, nfeats, efeats, eweight = None):\n",
    "        if eweight != None:\n",
    "            # apply eweight on the graph\n",
    "            efe = []\n",
    "            for i, x in enumerate(eweight):\n",
    "                efe.append(list(th.Tensor.cpu(g.edata['h'][i][0]).detach().numpy() * th.Tensor.cpu(x).detach().numpy()))\n",
    "\n",
    "            efe = th.FloatTensor(efe).cuda()\n",
    "            efe = th.reshape(efe, (efe.shape[0], 1, efe.shape[1]))\n",
    "            g.edata['h'] = efe = efe\n",
    "\n",
    "        h = self.gnn(g, nfeats, efeats)\n",
    "        # h = list of node features [[node1_feature1, node1_feature2, ...], [node2_feature1, node2_feature2, ...], ...]\n",
    "        return self.pred(g, h)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# # --------------------------------------------------- MAIN -----------------------------------------------------------\n",
    "\n",
    "#Data\n",
    "nbclasses =  2\n",
    "\n",
    "\n",
    "# Model *******************************************************************************************\n",
    "# G1.ndata['h'].shape[2] = sizeh = 76 dans ANIDS\n",
    "# model1 = Model(G1.ndata['h'].shape[2], size_embedding, G1.ndata['h'].shape[2], F.relu, 0.2).cuda()\n",
    "model1 = Model(76, size_embedding, 76, F.relu, 0.2).cuda()\n",
    "opt = th.optim.Adam(model1.parameters())\n",
    "\n",
    "\n",
    "\n",
    "path, dirs, files = next(os.walk(\"/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/input/Dataset/GlobalDataset/Splitted/\"))\n",
    "file_count = len(files)\n",
    "\n",
    "\n",
    "for nb_files in range(file_count):\n",
    "    data1 = pd.read_csv(f'{path}{files[nb_files]}', encoding=\"ISO-8859â€“1\", dtype = str)\n",
    "\n",
    "    print(f'{files[nb_files]} ++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print(\"nb total instances in the file : \", len(data1.values))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    # Delete two columns (U and V in the excel)\n",
    "    cols = list(set(list(data1.columns )) - set(list(['Flow Bytes/s',' Flow Packets/s'])) )\n",
    "    data1 = data1[cols]\n",
    "\n",
    "    # Mise en forme des noeuds\n",
    "    data1[' Source IP'] = data1[' Source IP'].apply(str)\n",
    "    data1[' Source Port'] = data1[' Source Port'].apply(str)\n",
    "    data1[' Destination IP'] = data1[' Destination IP'].apply(str)\n",
    "    data1[' Destination Port'] = data1[' Destination Port'].apply(str)\n",
    "    data1[' Source IP'] = data1[' Source IP'] + ':' + data1[' Source Port']\n",
    "    data1[' Destination IP'] = data1[' Destination IP'] + ':' + data1[' Destination Port']\n",
    "\n",
    "    data1.drop(columns=['Flow ID',' Source Port',' Destination Port',' Timestamp'], inplace=True)\n",
    "\n",
    "    # -------------------- ????????????????????????????????????????? --------------------\n",
    "    # simply do : nom = list(data1[' Label'].unique())\n",
    "    nom = []\n",
    "    nom = nom + [data1[' Label'].unique()[0]]\n",
    "    for i in range(1, len(data1[' Label'].unique())):\n",
    "        nom = nom + [data1[' Label'].unique()[i]]\n",
    "    \n",
    "    nom.insert(0, nom.pop(nom.index('BENIGN')))\n",
    "\n",
    "    # Naming the two classes BENIGN {0} / Any Intrusion {1}\n",
    "    data1[' Label'].replace(nom[0], 0,inplace = True)\n",
    "    for i in range(1,len(data1[' Label'].unique())):\n",
    "        data1[' Label'].replace(nom[i], 1,inplace = True)\n",
    "    \n",
    "    ##################### LABELS FREQ #######################################\n",
    "    print()\n",
    "    print(\"labels freq after changing labels to binary\")\n",
    "    counts = list(data1[' Label'].value_counts().to_dict().items())\n",
    "    for j, x in enumerate(counts):\n",
    "        x = list(x)\n",
    "        x[1] = x[1] / len(data1)\n",
    "        counts[j] = x\n",
    "    print({f'{files[nb_files]}' : counts})\n",
    "    ##############################################################################\n",
    "\n",
    "    data1.rename(columns={\" Label\": \"label\"},inplace = True)\n",
    "    label1 = data1.label\n",
    "    data1.drop(columns=['label'],inplace = True)\n",
    "\n",
    "    # ******** At this step data1 contains only the data without label column\n",
    "    # ******** The label column is stored in the label variale \n",
    "\n",
    "    # split train and test\n",
    "    data1 =  pd.concat([data1, label1], axis=1) # ??????? WHY ?\n",
    "\n",
    "    # -------------------- ????????????????????????????????????????? --------------------\n",
    "    # X will contain the label column due to the concatination made earlier !!\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(data1, label1, test_size=0.3, random_state=123, stratify= label1)\n",
    "\n",
    "    # Create mini batches on the Train set\n",
    "    # 1st step : Duplicate instances of least populated classes (nb occ < 100 => x100)\n",
    "    for indx, x in enumerate(X1_train[\"label\"].value_counts()) :\n",
    "        if x < 100 :\n",
    "            inst = X1_train.loc[X1_train['label'] == X1_train[\"label\"].value_counts().index[indx]]\n",
    "            for i in range(int(100 / x)) :\n",
    "                X1_train = pd.concat([X1_train, inst], ignore_index = True)\n",
    "    \n",
    "    X1_train = shuffle(X1_train)\n",
    "    \n",
    "    # At this step we duplicated the least populated classes in the Train Set\n",
    "    # 2nd step : Create the mini batches\n",
    "    a = b = mean_macro_f1 = 0\n",
    "    for batch in range(1, nb_batch + 1):\n",
    "        print(f\"+++++++++++++++++ Batch {batch} ++++++++++++++++\")\n",
    "        a = b\n",
    "        b = int(len(X1_train) / nb_batch) * batch\n",
    "        if batch == nb_batch :\n",
    "            b = len(X1_train)\n",
    "        # The batch :\n",
    "        X1_train_batched = X1_train.iloc[a:b]\n",
    "        # y1_train_batched = y1_train.iloc[a:b]\n",
    "        y1_train_batched = X1_train_batched['label']\n",
    "\n",
    "        # Each batch will contain 64500 instance and all classes are present (The least populated one has > 10 instances)\n",
    "\n",
    "        print(\"nb Train instances : \", len(X1_train_batched.values))\n",
    "\n",
    "        # for non numerical attributes (categorical data)\n",
    "        # Since we have a binary classification, the category values willl be replaced with the posterior probability (p(target = Ti | category = Cj))\n",
    "        # TargetEncoding is also called MeanEncoding, cuz it simply replace each value with (target_i_count_on_category_j) / (total_occurences_of_category_j)\n",
    "        encoder1 = ce.TargetEncoder(cols=[' Protocol',  'Fwd PSH Flags', ' Fwd URG Flags', ' Bwd PSH Flags', ' Bwd URG Flags'])\n",
    "        encoder1.fit(X1_train_batched, y1_train_batched)\n",
    "        X1_train_batched = encoder1.transform(X1_train_batched)\n",
    "\n",
    "        # scaler (normalization)\n",
    "        scaler1 = StandardScaler()\n",
    "\n",
    "        # Manipulate flow content (all columns except : label, Source IP & Destination IP)\n",
    "        cols_to_norm1 = list(set(list(X1_train_batched.iloc[:, :].columns )) - set(list(['label', ' Source IP', ' Destination IP'])) )\n",
    "        X1_train_batched[cols_to_norm1] = scaler1.fit_transform(X1_train_batched[cols_to_norm1])\n",
    "\n",
    "        ## Create the h attribute that will contain the content of our flows\n",
    "        X1_train_batched['h'] = X1_train_batched[ cols_to_norm1 ].values.tolist()\n",
    "        # size of the list containig the content of our flows\n",
    "        sizeh = len(cols_to_norm1)\n",
    "\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Before training the data :\n",
    "        # We need to delete all the attributes (cols_to_norm1) to have the {Source IP, Destination IP, label, h} representation\n",
    "        X1_train_batched.drop(columns = cols_to_norm1, inplace = True)\n",
    "\n",
    "        # Then we need to Swap {label, h} Columns to have the {Source IP, Destination IP, h, label} representation\n",
    "        columns_titles = [' Source IP', ' Destination IP', 'h', 'label']\n",
    "        X1_train_batched = X1_train_batched.reindex(columns=columns_titles)\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # ------------------------------------------- Creating the Graph Representation -------------------------------------------------------------\n",
    "        # Create our Multigraph\n",
    "        G1 = nx.from_pandas_edgelist(X1_train_batched, \" Source IP\", \" Destination IP\", ['h','label'], create_using=nx.MultiDiGraph())\n",
    "        print(\"initial nx multigraph G1 : \", G1)\n",
    "\n",
    "        # Convert it to a directed Graph\n",
    "        # NB : IT WILL CREATE A DEFAULT BIDIRECTIONAL RELATIONSHIPS BETWEEN NODES, and not the original relationships ???????????????????????\n",
    "        # G1 = G1.to_directed()\n",
    "        # print(\"G1 after todirected : \", G1)\n",
    "        # Convert the graph from a networkx Graph to a DGL Graph\n",
    "        G1 = from_networkx(G1,edge_attrs=['h','label'] )\n",
    "        print(\"G1.edata['h'] after converting it to a dgl graph : \", len(G1.edata['h']))\n",
    "\n",
    "        # nodes data // G1.edata['h'].shape[1] : sizeh = number of attributes in a flow\n",
    "        G1.ndata['h'] = th.ones(G1.num_nodes(), G1.edata['h'].shape[1])\n",
    "        # edges data // we create a tensor bool array that will represent the train mask\n",
    "        G1.edata['train_mask'] = th.ones(len(G1.edata['h']), dtype=th.bool)\n",
    "\n",
    "        # Reshape both tensor lists to a single value in each element for both axis\n",
    "        G1.ndata['h'] = th.reshape(G1.ndata['h'], (G1.ndata['h'].shape[0], 1, G1.ndata['h'].shape[1]))\n",
    "        G1.edata['h'] = th.reshape(G1.edata['h'], (G1.edata['h'].shape[0], 1, G1.edata['h'].shape[1]))\n",
    "        print(\"G1.edata['h'] after reshape : \", len(G1.edata['h']))\n",
    "        # ------------------------------------------- --------------------------------- -------------------------------------------------------------\n",
    "\n",
    "        # ------------------------------------------- Model -----------------------------------------------------------------------------------------\n",
    "        ## use of model\n",
    "        from sklearn.utils import class_weight\n",
    "        class_weights1 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(G1.edata['label'].cpu().numpy()),\n",
    "                                                        y = G1.edata['label'].cpu().numpy())\n",
    "        ''' \n",
    "            Using class weights, you make the classifier aware of how to treat the various classes in the loss function.\n",
    "            In this process, you give higher weights to certain classes & lower weights to other classes.\n",
    "            Example : [ 0.51600999 16.11525117] \n",
    "            Basically : \n",
    "                - For classes with small number of training images, you give it more weight\n",
    "                so that the network will be punished more if it makes mistakes predicting the label of these classes. \n",
    "                - For classes with large numbers of images, you give it small weight\n",
    "        '''\n",
    "        class_weights1 = th.FloatTensor(class_weights1).cuda()\n",
    "        criterion1 = nn.CrossEntropyLoss(weight = class_weights1)\n",
    "        G1 = G1.to('cuda:0')\n",
    "\n",
    "        node_features1 = G1.ndata['h']\n",
    "        edge_features1 = G1.edata['h']\n",
    "\n",
    "        edge_label1 = G1.edata['label']\n",
    "        train_mask1 = G1.edata['train_mask']\n",
    "\n",
    "\n",
    "        # to print\n",
    "        pr = True\n",
    "        # True if you want to print the embedding vectors\n",
    "        # the name of the file where the vectors are printed\n",
    "        filename = './models/M1_weights.txt'\n",
    "\n",
    "        for epoch in range(1,1000):\n",
    "            pred = model1(G1, node_features1, edge_features1).cuda()\n",
    "            loss = criterion1(pred[train_mask1], edge_label1[train_mask1])\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training acc:', compute_accuracy(pred[train_mask1], edge_label1[train_mask1]), loss)\n",
    "\n",
    "        pred1 = model1(G1, node_features1, edge_features1).cuda()\n",
    "        pred1 = pred1.argmax(1)\n",
    "        pred1 = th.Tensor.cpu(pred1).detach().numpy()\n",
    "        edge_label1 = th.Tensor.cpu(edge_label1).detach().numpy()\n",
    "\n",
    "        print('Train metrics :')\n",
    "        print(\"Accuracy : \", sklearn.metrics.accuracy_score(edge_label1, pred1))\n",
    "        print(\"Precision : \", sklearn.metrics.precision_score(edge_label1, pred1, labels = [0,1]))\n",
    "        print(\"Recall : \", sklearn.metrics.recall_score(edge_label1, pred1, labels = [0,1]))\n",
    "        print(\"f1_score : \", sklearn.metrics.f1_score(edge_label1, pred1, labels=[0,1]))\n",
    "\n",
    "    # ------------------------------------------------ Test ---------------------------------------------------------------------\n",
    "    print(\"++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\")\n",
    "    print(\"nb Test instances : \", len(X1_test.values))\n",
    "    X1_test = encoder1.transform(X1_test)\n",
    "    X1_test[cols_to_norm1] = scaler1.transform(X1_test[cols_to_norm1])\n",
    "\n",
    "    # Save X1_test for XAI\n",
    "    X1_test.to_csv(f'/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/input/Dataset/XAI/X_test{nb_files}.csv', sep=',', index = False)\n",
    "\n",
    "    X1_test['h'] = X1_test[ cols_to_norm1 ].values.tolist()\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Before training the data :\n",
    "    # We need to delete all the attributes (cols_to_norm1) to have the {Source IP, Destination IP, label, h} representation\n",
    "    X1_test.drop(columns = cols_to_norm1, inplace = True)\n",
    "\n",
    "    # Then we need to Swap {label, h} Columns to have the {Source IP, Destination IP, h, label} representation\n",
    "    columns_titles = [' Source IP', ' Destination IP', 'h', 'label']\n",
    "    X1_test=X1_test.reindex(columns=columns_titles)\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    G1_test = nx.from_pandas_edgelist(X1_test, \" Source IP\", \" Destination IP\", ['h','label'],create_using=nx.MultiDiGraph())\n",
    "    # G1_test = G1_test.to_directed()\n",
    "    G1_test = from_networkx(G1_test,edge_attrs=['h','label'] )\n",
    "    # actual1 = G1_test.edata.pop('label')\n",
    "    actual1 = G1_test.edata['label']\n",
    "    G1_test.ndata['feature'] = th.ones(G1_test.num_nodes(), G1.ndata['h'].shape[2])\n",
    "    G1_test.ndata['feature'] = th.reshape(G1_test.ndata['feature'], (G1_test.ndata['feature'].shape[0], 1, G1_test.ndata['feature'].shape[1]))\n",
    "    G1_test.edata['h'] = th.reshape(G1_test.edata['h'], (G1_test.edata['h'].shape[0], 1, G1_test.edata['h'].shape[1]))\n",
    "    G1_test = G1_test.to('cuda:0')\n",
    "    node_features_test1 = G1_test.ndata['feature']\n",
    "    edge_features_test1 = G1_test.edata['h']\n",
    "\n",
    "    # to print\n",
    "    pr = True\n",
    "    # True if you want to print the embedding vectors\n",
    "    # the name of the file where the vectors are printed\n",
    "    filename = './models/M1_weights.txt'\n",
    "\n",
    "    print(\"nb instances : \", len(X1_test.values))\n",
    "\n",
    "    test_pred1 = model1(G1_test, node_features_test1, edge_features_test1).cuda()\n",
    "    test_pred1 = test_pred1.argmax(1)\n",
    "    test_pred1 = th.Tensor.cpu(test_pred1).detach().numpy()\n",
    "\n",
    "    print('Metrics : ')\n",
    "    print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1, test_pred1))\n",
    "    print(\"Precision : \", sklearn.metrics.precision_score(actual1, test_pred1, labels = [0,1]))\n",
    "    print(\"Recall : \", sklearn.metrics.recall_score(actual1, test_pred1, labels = [0,1]))\n",
    "    print(\"f1_score : \", sklearn.metrics.f1_score(actual1, test_pred1, labels = [0,1]))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-blues",
   "metadata": {},
   "source": [
    "# XAI : explain_edges & explain_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fresh-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "from dgl import EID, NID, khop_out_subgraph\n",
    "\n",
    "\n",
    "# init mask\n",
    "def init_masks(graph, efeat):\n",
    "    # efeat.size() = torch.Size([nb_edges, 1, 76])\n",
    "    efeat_size = efeat.size()[2]\n",
    "    num_edges = graph.num_edges()\n",
    "    num_nodes = graph.num_nodes()\n",
    "    device = efeat.device\n",
    "    std = 0.1\n",
    "    # feat_mask = [[f1, f2, .... fn]] / n = nb_features\n",
    "    efeat_mask = nn.Parameter(th.randn(1, efeat_size, device=device) * std)\n",
    "    std = nn.init.calculate_gain(\"relu\") * sqrt(2.0 / (2 * num_nodes))\n",
    "    # edge_mask = [e1, e2, .... em] / m = nb_edges\n",
    "    edge_mask = nn.Parameter(th.randn(num_edges, device=device) * std)\n",
    "    return efeat_mask, edge_mask\n",
    "\n",
    "\n",
    "# Regularization loss\n",
    "def loss_regularize(loss, feat_mask, edge_mask):\n",
    "    # epsilon for numerical stability\n",
    "    eps = 1e-15\n",
    "    # From self GNNExplainer self\n",
    "    alpha1 = 0.005,\n",
    "    alpha2 = 1.0\n",
    "    beta1 = 1.0\n",
    "    beta2 = 0.1\n",
    "\n",
    "    edge_mask = edge_mask.sigmoid()\n",
    "    # Edge mask sparsity regularization\n",
    "    loss = loss + th.from_numpy(alpha1 * th.Tensor.cpu(th.sum(edge_mask)).detach().numpy()).cuda()\n",
    "    # Edge mask entropy regularization\n",
    "    ent = -edge_mask * th.log(edge_mask + eps) - (\n",
    "        1 - edge_mask\n",
    "    ) * th.log(1 - edge_mask + eps)\n",
    "    loss = loss + alpha2 * ent.mean()\n",
    "\n",
    "    feat_mask = feat_mask.sigmoid()\n",
    "    # Feature mask sparsity regularization\n",
    "    loss = loss + beta1 * th.mean(feat_mask)\n",
    "    # Feature mask entropy regularization\n",
    "    ent = -feat_mask * th.log(feat_mask + eps) - (\n",
    "        1 - feat_mask\n",
    "    ) * th.log(1 - feat_mask + eps)\n",
    "    loss = loss + beta2 * ent.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Edge features\n",
    "def explain_edge_features(model, graph, node_feat, edge_feat, **kwargs):\n",
    "    model = model.to(graph.device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "    print(\"graph : \", graph)\n",
    "    print(\"graph_edges : \", graph.edges()) # edges ids in graph.edges()\n",
    "    print(\"graph_nodes : \", graph.nodes()) # nodes ids in graph.nodes()\n",
    "    print(\"edge_feat : \", edge_feat)\n",
    "    print(\"node_feat : \", node_feat)\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "\n",
    "    # If we add kwargs\n",
    "    # for key, item in kwargs.items():\n",
    "    #     if th.is_tensor(item) and item.size(0) == num_nodes:\n",
    "    #         item = item[sg_nodes]\n",
    "    #     elif th.is_tensor(item) and item.size(0) == num_edges:\n",
    "    #         item = item[sg_edges]\n",
    "    #     kwargs[key] = item\n",
    "\n",
    "    # Get the initial prediction.\n",
    "    print(\"Get the initial prediction :\")\n",
    "    with th.no_grad():\n",
    "        # logits = model(g = sg, nfeats = node_feat, efeats = edge_feat, **kwargs)\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat)\n",
    "        pred_label = logits.argmax(dim=-1)\n",
    "        # pred_label1 = logits.argmax(1)\n",
    "\n",
    "    print(\"pred_label : \", pred_label)\n",
    "\n",
    "    #\n",
    "    efeat_mask, edge_mask = init_masks(graph, edge_feat)\n",
    "\n",
    "    params = [efeat_mask]\n",
    "    # params = [efeat_mask]\n",
    "    # lr=0.01\n",
    "    optimizer = th.optim.Adam(params, lr = 0.01)\n",
    "\n",
    "    # num_epochs = 100\n",
    "    print(\"***********************************\")\n",
    "    print(\"initial masks : \")\n",
    "    print(\"efeat_mask : \", efeat_mask)\n",
    "    print(\"***********************************\")\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights2 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(graph.edata['label'].cpu().numpy()),\n",
    "                                                        y = graph.edata['label'].cpu().numpy())\n",
    "    class_weights2 = th.FloatTensor(class_weights2).cuda()\n",
    "    criterion2 = nn.CrossEntropyLoss(weight = class_weights2)\n",
    "    train_mask2 = th.ones(len(graph.edata['h']), dtype=th.bool)\n",
    "    import datetime\n",
    "    \n",
    "    print(f'explanation starts at {datetime.datetime.now()}')\n",
    "    print(\"nb edges : \", graph.num_edges())\n",
    "    print(\"nb nodes : \", graph.num_nodes())\n",
    "    for epoch in range(1,300):\n",
    "        optimizer.zero_grad()\n",
    "        # Matrix multiplication\n",
    "        h = edge_feat * efeat_mask.sigmoid()\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = h).cuda()\n",
    "        # pred_label = tensor([0, 0, 0,  ..., 0, 1, 0], device='cuda:0')\n",
    "        # logits = tensor([[ 0.0059,  0.0517], [-0.0075,  0.0101], ..., device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "        loss = criterion2(logits[train_mask2], pred_label[train_mask2])\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"+++++++++++++++\")\n",
    "            print(f'epoch number {epoch}, CrossEntropy_Loss = {loss}, time = {datetime.datetime.now()}')\n",
    "            print(\"efeat_mask : \", efeat_mask.detach().sigmoid())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    efeat_mask = efeat_mask.detach().sigmoid().squeeze()\n",
    "\n",
    "    return efeat_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Edge mask\n",
    "def explain_edges(model, graph, node_feat, edge_feat, **kwargs):\n",
    "    model = model.to(graph.device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "    print(\"graph : \", graph)\n",
    "    print(\"graph_edges : \", graph.edges()) # edges ids in graph.edges()\n",
    "    print(\"graph_nodes : \", graph.nodes()) # nodes ids in graph.nodes()\n",
    "    print(\"edge_feat : \", edge_feat)\n",
    "    print(\"node_feat : \", node_feat)\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "\n",
    "    # If we add kwargs\n",
    "    # for key, item in kwargs.items():\n",
    "    #     if th.is_tensor(item) and item.size(0) == num_nodes:\n",
    "    #         item = item[sg_nodes]\n",
    "    #     elif th.is_tensor(item) and item.size(0) == num_edges:\n",
    "    #         item = item[sg_edges]\n",
    "    #     kwargs[key] = item\n",
    "\n",
    "    # Get the initial prediction.\n",
    "    print(\"Get the initial prediction :\")\n",
    "    with th.no_grad():\n",
    "        # logits = model(g = sg, nfeats = node_feat, efeats = edge_feat, **kwargs)\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat)\n",
    "        pred_label = logits.argmax(dim=-1)\n",
    "        # pred_label1 = logits.argmax(1)\n",
    "\n",
    "    print(\"pred_label : \", pred_label)\n",
    "\n",
    "    #\n",
    "    efeat_mask, edge_mask = init_masks(graph, edge_feat)\n",
    "\n",
    "    params = [edge_mask]\n",
    "    # params = [efeat_mask]\n",
    "    # lr=0.01\n",
    "    optimizer = th.optim.Adam(params, lr = 0.01)\n",
    "\n",
    "    # num_epochs = 100\n",
    "    print(\"***********************************\")\n",
    "    print(\"initial masks : \")\n",
    "    print(\"edge_mask : \", edge_mask)\n",
    "    print(\"***********************************\")\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights2 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(graph.edata['label'].cpu().numpy()),\n",
    "                                                        y = graph.edata['label'].cpu().numpy())\n",
    "    class_weights2 = th.FloatTensor(class_weights2).cuda()\n",
    "    criterion2 = nn.CrossEntropyLoss(weight = class_weights2)\n",
    "    train_mask2 = th.ones(len(graph.edata['h']), dtype=th.bool)\n",
    "    import datetime\n",
    "    \n",
    "    print(f'explanation starts at {datetime.datetime.now()}')\n",
    "    print(\"nb edges : \", graph.num_edges())\n",
    "    print(\"nb nodes : \", graph.num_nodes())\n",
    "    \n",
    "    for epoch in range(1,300):\n",
    "        optimizer.zero_grad()\n",
    "        # Matrix multiplication\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat, eweight=edge_mask.sigmoid()).cuda()\n",
    "        # pred_label = tensor([0, 0, 0,  ..., 0, 1, 0], device='cuda:0')\n",
    "        # logits = tensor([[ 0.0059,  0.0517], [-0.0075,  0.0101], ..., device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "        loss11 = criterion2(logits[train_mask2], pred_label[train_mask2])\n",
    "        loss = loss_regularize(loss11, efeat_mask, edge_mask)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"+++++++++++++++\")\n",
    "            print(f'epoch number {epoch}, CrossEntropy_Loss = {loss11}, final_loss = {loss}, time = {datetime.datetime.now()}')\n",
    "            print(\"edge_mask : \", edge_mask.detach().sigmoid())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    edge_mask = edge_mask.detach().sigmoid()\n",
    "\n",
    "    return edge_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-profession",
   "metadata": {},
   "source": [
    "# XAI : Results interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imposed-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++\n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "graph_edges :  (tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "graph_nodes :  tensor([     0,      1,      2,  ..., 104637, 104638, 104639], device='cuda:0')\n",
      "edge_feat :  tensor([[[ 0.0043,  0.7111,  1.6772,  ...,  0.0000, -0.0158,  0.1105]],\n",
      "\n",
      "        [[ 0.0043,  0.7111, -0.2094,  ...,  0.0000, -0.0158, -0.2778]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2080,  ...,  0.0000, -0.0158,  4.2246]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.0768,  ...,  0.0000, -0.0158,  0.3723]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2072,  ...,  0.0000, -0.0158,  0.2599]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2073,  ...,  0.0000, -0.0158,  0.6878]]],\n",
      "       device='cuda:0')\n",
      "node_feat :  tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "Get the initial prediction :\n",
      "pred_label :  tensor([0, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "***********************************\n",
      "initial masks : \n",
      "efeat_mask :  Parameter containing:\n",
      "tensor([[-0.0875,  0.0391, -0.0286, -0.0006, -0.1264,  0.0409, -0.0359,  0.0429,\n",
      "         -0.0367,  0.2013, -0.0849, -0.1532,  0.0062, -0.0256,  0.0632, -0.1974,\n",
      "         -0.0793,  0.1561,  0.0729,  0.0950,  0.0722, -0.0420,  0.0405,  0.0258,\n",
      "          0.0585,  0.0029, -0.0694, -0.1459,  0.0957, -0.1464,  0.0311, -0.0690,\n",
      "         -0.0003, -0.0515, -0.0763,  0.0078,  0.1383, -0.0343,  0.0481,  0.0719,\n",
      "          0.0925,  0.1034, -0.0149,  0.2045, -0.0299,  0.0476,  0.0312, -0.0284,\n",
      "         -0.1813, -0.0474, -0.0862,  0.0582,  0.0003,  0.1025, -0.0426, -0.0869,\n",
      "         -0.0627, -0.0879,  0.0908, -0.0316, -0.0265,  0.0946, -0.0320, -0.0838,\n",
      "         -0.0043, -0.0061, -0.1333,  0.0718, -0.0429, -0.2182, -0.0040, -0.0450,\n",
      "         -0.0657, -0.2312, -0.0362,  0.0485]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "***********************************\n",
      "explanation starts at 2023-05-17 11:07:18.096709\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "+++++++++++++++\n",
      "epoch number 100, CrossEntropy_Loss = 0.059856437146663666, time = 2023-05-17 11:07:26.453799\n",
      "efeat_mask :  tensor([[0.4904, 0.4070, 0.5242, 0.4609, 0.4684, 0.5384, 0.4271, 0.6207, 0.4771,\n",
      "         0.5302, 0.4788, 0.3536, 0.6779, 0.6044, 0.3882, 0.3563, 0.6005, 0.5390,\n",
      "         0.6518, 0.6059, 0.4501, 0.4257, 0.6340, 0.4618, 0.5146, 0.4313, 0.5834,\n",
      "         0.4827, 0.4443, 0.6201, 0.6034, 0.5677, 0.5799, 0.4150, 0.3427, 0.3420,\n",
      "         0.6174, 0.4775, 0.5959, 0.5180, 0.3865, 0.4244, 0.5778, 0.5637, 0.4187,\n",
      "         0.4304, 0.7152, 0.4929, 0.2901, 0.4131, 0.5926, 0.4434, 0.5727, 0.5256,\n",
      "         0.3951, 0.4123, 0.4050, 0.4106, 0.4504, 0.4186, 0.5273, 0.4381, 0.4134,\n",
      "         0.4008, 0.5832, 0.6967, 0.5572, 0.4228, 0.4228, 0.3831, 0.5792, 0.3941,\n",
      "         0.4021, 0.4424, 0.3516, 0.5942]], device='cuda:0')\n",
      "+++++++++++++++\n",
      "epoch number 200, CrossEntropy_Loss = 0.05064254254102707, time = 2023-05-17 11:07:34.816399\n",
      "efeat_mask :  tensor([[0.4339, 0.3588, 0.4489, 0.5423, 0.4684, 0.5224, 0.4710, 0.6009, 0.4339,\n",
      "         0.6300, 0.4788, 0.3006, 0.8027, 0.6281, 0.3863, 0.3338, 0.5870, 0.5390,\n",
      "         0.7624, 0.6147, 0.4750, 0.4589, 0.7752, 0.5256, 0.5146, 0.4545, 0.6244,\n",
      "         0.4651, 0.4472, 0.6109, 0.6390, 0.5846, 0.5860, 0.4306, 0.2409, 0.2961,\n",
      "         0.6272, 0.4361, 0.6058, 0.5180, 0.3621, 0.4099, 0.5844, 0.5080, 0.4308,\n",
      "         0.4323, 0.8346, 0.4929, 0.2206, 0.4354, 0.6190, 0.4612, 0.5677, 0.5256,\n",
      "         0.3603, 0.4367, 0.4118, 0.4408, 0.4670, 0.4350, 0.6574, 0.4426, 0.4202,\n",
      "         0.4090, 0.5931, 0.7902, 0.5777, 0.3191, 0.4492, 0.4164, 0.5844, 0.3755,\n",
      "         0.4014, 0.4424, 0.2592, 0.6015]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "graph_edges :  (tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "graph_nodes :  tensor([     0,      1,      2,  ..., 104637, 104638, 104639], device='cuda:0')\n",
      "edge_feat :  tensor([[[ 0.0043,  0.7111,  1.6772,  ...,  0.0000, -0.0158,  0.1105]],\n",
      "\n",
      "        [[ 0.0043,  0.7111, -0.2094,  ...,  0.0000, -0.0158, -0.2778]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2080,  ...,  0.0000, -0.0158,  4.2246]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.0768,  ...,  0.0000, -0.0158,  0.3723]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2072,  ...,  0.0000, -0.0158,  0.2599]],\n",
      "\n",
      "        [[ 0.0044,  0.7111, -0.2073,  ...,  0.0000, -0.0158,  0.6878]]],\n",
      "       device='cuda:0')\n",
      "node_feat :  tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "Get the initial prediction :\n",
      "pred_label :  tensor([0, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "***********************************\n",
      "initial masks : \n",
      "edge_mask :  Parameter containing:\n",
      "tensor([-0.0012, -0.0024,  0.0011,  ...,  0.0038,  0.0004, -0.0024],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "***********************************\n",
      "explanation starts at 2023-05-17 11:07:43.191957\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "+++++++++++++++\n",
      "epoch number 100, CrossEntropy_Loss = 0.016169250011444092, final_loss = tensor([346.8345], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), time = 2023-05-17 11:19:42.266042\n",
      "edge_mask :  tensor([0.2194, 0.2182, 0.7805,  ..., 0.7826, 0.7786, 0.2182], device='cuda:0')\n",
      "+++++++++++++++\n",
      "epoch number 200, CrossEntropy_Loss = 0.016169250011444092, final_loss = tensor([346.8889], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), time = 2023-05-17 11:31:40.175180\n",
      "edge_mask :  tensor([0.0744, 0.0743, 0.9256,  ..., 0.9257, 0.9255, 0.0743], device='cuda:0')\n",
      "\n",
      "final results : \n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "efeat_mask :  tensor([0.3624, 0.3053, 0.4011, 0.6265, 0.4684, 0.5058, 0.5250, 0.6237, 0.4145,\n",
      "        0.7254, 0.4788, 0.2508, 0.8700, 0.6208, 0.3584, 0.3207, 0.6174, 0.5390,\n",
      "        0.8420, 0.6258, 0.5058, 0.5022, 0.8557, 0.6025, 0.5146, 0.4830, 0.6830,\n",
      "        0.4252, 0.4523, 0.6280, 0.6795, 0.6054, 0.5956, 0.4511, 0.1704, 0.2632,\n",
      "        0.6384, 0.4177, 0.6171, 0.5180, 0.3134, 0.4366, 0.5919, 0.4324, 0.4463,\n",
      "        0.4352, 0.8905, 0.4929, 0.2334, 0.4642, 0.6108, 0.4837, 0.5698, 0.5256,\n",
      "        0.3216, 0.4700, 0.4192, 0.4807, 0.4887, 0.4572, 0.7892, 0.4667, 0.4249,\n",
      "        0.4151, 0.6048, 0.8296, 0.6033, 0.2194, 0.4839, 0.4621, 0.5928, 0.3670,\n",
      "        0.4024, 0.4424, 0.1924, 0.6106], device='cuda:0')\n",
      "edge_mask :  tensor([0.0338, 0.0338, 0.9662,  ..., 0.9662, 0.9663, 0.0338], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "efeat_mask = explain_edge_features(model1, G1_test, node_features_test1, edge_features_test1)\n",
    "edge_mask = explain_edges(model1, G1_test, node_features_test1, edge_features_test1)\n",
    "\n",
    "print()\n",
    "print(\"final results : \")\n",
    "print(\"graph : \", G1_test)\n",
    "print(\"nb edges : \", G1_test.num_edges())\n",
    "print(\"nb nodes : \", G1_test.num_nodes())\n",
    "print(\"efeat_mask : \", efeat_mask)\n",
    "print(\"edge_mask : \", edge_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "laughing-humanity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138050\n",
      "tensor([0., 0., 1.,  ..., 1., 1., 0.], device='cuda:0')\n",
      "(tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 7.9992e-32,  1.3074e-29, -3.8238e-30,  ...,  0.0000e+00,\n",
      "          -2.8976e-31,  7.7667e-29]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.7460e-31,  2.8271e-29, -3.0537e-30,  ...,  0.0000e+00,\n",
      "          -6.2660e-31,  1.4801e-29]],\n",
      "\n",
      "        [[ 4.0694e-32,  6.6517e-30, -1.9378e-30,  ...,  0.0000e+00,\n",
      "          -1.4742e-31,  2.4310e-30]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(len(edge_mask))\n",
    "print(th.round(edge_mask))\n",
    "\n",
    "print(G1_test.edges())\n",
    "print(G1_test.edata['h'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-population",
   "metadata": {},
   "source": [
    "### deep analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mask_round = th.round(edge_mask)\n",
    "print(edge_mask_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((edge_mask_round == 0).nonzero(as_tuple=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "subgraph = dgl.remove_edges(G1_test, (edge_mask_round == 0).nonzero(as_tuple=True)[0])\n",
    "isolated_nodes = ((subgraph.in_degrees() == 0) & (subgraph.out_degrees() == 0)).nonzero().squeeze(1)\n",
    "subgraph.remove_nodes(isolated_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_intrusions = sum(subgraph.edata['label'])\n",
    "nb_benign = subgraph.num_edges() - nb_intrusions\n",
    "print(\"% of intrusions in the g_intrusion subgraph =\", (nb_intrusions.item() / subgraph.num_edges()) * 100)\n",
    "print(\"% of benign in the g_intrusion subgraph =\", (nb_benign.item() / subgraph.num_edges()) * 100)\n",
    "\n",
    "print('*******')\n",
    "\n",
    "nb_intrusions_orig = sum(G1_test.edata['label'])\n",
    "nb_benign_orig = G1_test.num_edges() - nb_intrusions_orig\n",
    "\n",
    "print(\"% of intrusions taken from the original test graph =\", (nb_intrusions.item() / nb_intrusions_orig.item()) * 100)\n",
    "print(\"% of benign taken from the original test graph =\", (nb_benign.item() / nb_benign_orig.item()) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

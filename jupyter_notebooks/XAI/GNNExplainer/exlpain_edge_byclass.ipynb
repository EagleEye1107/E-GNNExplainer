{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ancient-malta",
   "metadata": {},
   "source": [
    "# E-GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hydraulic-mainland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIC-IDS-2017-Dataset4.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460167\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset4.csv': [[0, 0.7582073464633492], [1, 0.24179265353665083]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58870 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9861850142478943 tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9881563782691956 tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895533919334412 tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899104237556458 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897241592407227 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898172616958618 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904226660728455 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902829527854919 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900500774383545 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9908262577029943\n",
      "Precision :  0.9641194363399342\n",
      "Recall :  0.9991636644364384\n",
      "f1_score :  0.9813287840015164\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58872 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9895999431610107 tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899104237556458 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910125136375427 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908882975578308 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909969568252563 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904536604881287 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991291880607605 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909038543701172 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991540253162384 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9910280489887152\n",
      "Precision :  0.965040102859242\n",
      "Recall :  0.9995560910647473\n",
      "f1_score :  0.9819948912840322\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58896 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9907486438751221 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910745620727539 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991571307182312 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911832213401794 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.99160236120224 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911832213401794 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912142753601074 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910901188850403 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913539886474609 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9915558108129084\n",
      "Precision :  0.9692593740316083\n",
      "Recall :  0.9969401415184548\n",
      "f1_score :  0.9829049085538307\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59084 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9911677241325378 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918041229248047 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897862076759338 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990500271320343 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912763833999634 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911521673202515 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906865358352661 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913539886474609 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9912764074942179\n",
      "Precision :  0.9666520678220609\n",
      "Recall :  0.9981264939595581\n",
      "f1_score :  0.9821371813616427\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64424\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58941 nodes and 64424 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1.edata['h'] after reshape :  64424\n",
      "Training acc: 0.9908885359764099 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913697242736816 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991897463798523 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919129610061646 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991866409778595 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920061230659485 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919440150260925 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920061230659485 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920061230659485 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9920060846889358\n",
      "Precision :  0.9681821040055335\n",
      "Recall :  0.9994158120212904\n",
      "f1_score :  0.9835510556070141\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138051\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.9930460481995784\n",
      "Precision :  0.983996178191807\n",
      "Recall :  0.9872977831036549\n",
      "f1_score :  0.9856442158152888\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset1.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460165\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset1.csv': [[0, 0.7577868807927591], [1, 0.24221311920724087]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59227 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9898793697357178 tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903450608253479 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.989211916923523 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905933737754822 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908572435379028 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903916120529175 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907951951026917 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990469217300415 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990531325340271 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9906244664172733\n",
      "Precision :  0.9639829197351322\n",
      "Recall :  0.9985896531828964\n",
      "f1_score :  0.9809811700988728\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial nx multigraph G1 :  MultiDiGraph with 58643 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9905157685279846 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990531325340271 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909504055976868 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915247559547424 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910125136375427 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907330870628357 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909814596176147 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915247559547424 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991540253162384 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9910901386150909\n",
      "Precision :  0.9658844653581822\n",
      "Recall :  0.9985278115598797\n",
      "f1_score :  0.9819349153395859\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58793 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9916489124298096 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916644096374512 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991633415222168 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917110204696655 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921301007270813 tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921301007270813 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921301007270813 tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921456575393677 tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9921611846700712\n",
      "Precision :  0.9693877551020408\n",
      "Recall :  0.9993624481989162\n",
      "f1_score :  0.9841469157118192\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59094 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9864954948425293 tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9883581399917603 tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9885444045066833 tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9881874322891235 tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9886065125465393 tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9886220693588257 tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893050193786621 tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893826246261597 tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899104237556458 tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9897241668348261\n",
      "Precision :  0.9597903176071538\n",
      "Recall :  0.9993578629679574\n",
      "f1_score :  0.9791745312696615\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59037 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.990500271320343 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905623197555542 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906554818153381 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909814596176147 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899414777755737 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911987781524658 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904847145080566 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910280108451843 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990189790725708 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9903916303183645\n",
      "Precision :  0.9631837089464208\n",
      "Recall :  0.9983268983268984\n",
      "f1_score :  0.9804404840901191\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.8386381745744296\n",
      "Precision :  0.9205093429776974\n",
      "Recall :  0.36536276093067765\n",
      "f1_score :  0.523099978591308\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset0.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460165\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset0.csv': [[0, 0.7576130301087654], [1, 0.24238696989123468]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58637 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.991633415222168 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914626479148865 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913694858551025 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918041229248047 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920679926872253 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919127821922302 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991850733757019 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920524954795837 tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920369982719421 tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9920835726371017\n",
      "Precision :  0.9687924854776913\n",
      "Recall :  0.9996811631169493\n",
      "f1_score :  0.9839944765252322\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58961 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9921146035194397 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925336837768555 tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924405813217163 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924405813217163 tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925647377967834 tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924560785293579 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924715757369995 tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925336837768555 tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925026297569275 tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.99239402076898\n",
      "Precision :  0.9700926444071379\n",
      "Recall :  0.9994234834411633\n",
      "f1_score :  0.9845396605035652\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58780 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9885444045066833 tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.989149808883667 tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9886686205863953 tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9896620512008667 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897551536560059 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900656342506409 tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899104237556458 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9901742935180664 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908106923103333 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9904071527249585\n",
      "Precision :  0.9627340823970038\n",
      "Recall :  0.9986402486402487\n",
      "f1_score :  0.9803585049580473\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58767 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9917420744895935 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905623197555542 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911211729049683 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910745620727539 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912453293800354 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908572435379028 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911832213401794 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912298321723938 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9913384971205936\n",
      "Precision :  0.9664938911514254\n",
      "Recall :  0.999043245311902\n",
      "f1_score :  0.9824990590891983\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58763 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.990531325340271 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904226660728455 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906399846076965 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912453293800354 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904847145080566 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913694858551025 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899724721908569 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898483157157898 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909659028053284 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9910125265821212\n",
      "Precision :  0.9666460396039604\n",
      "Recall :  0.9974458846816934\n",
      "f1_score :  0.9818044687470538\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9919884099963782\n",
      "Precision :  0.9689819109461967\n",
      "Recall :  0.9989241527703067\n",
      "f1_score :  0.9837252420612732\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset2.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460167\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset2.csv': [[0, 0.7570208207020495], [1, 0.24297917929795051]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58777 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9924405813217163 tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9928907155990601 tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9929372668266296 tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9929062128067017 tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992921769618988 tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992952823638916 tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992952823638916 tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992952823638916 tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9929683208465576 tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.992968349812955\n",
      "Precision :  0.9716414173031176\n",
      "Recall :  1.0\n",
      "f1_score :  0.9856167645658042\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59103 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9898483157157898 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900035262107849 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911521673202515 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991260826587677 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920990467071533 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919593334197998 tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921922087669373 tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9921922294832591\n",
      "Precision :  0.969254562326013\n",
      "Recall :  0.9996172004593594\n",
      "f1_score :  0.9842017651308144\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58927 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.9892584681510925 tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898638129234314 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990189790725708 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906554818153381 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908417463302612 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910280108451843 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908882975578308 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914315938949585 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917575716972351 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9920835726371017\n",
      "Precision :  0.968949488689185\n",
      "Recall :  0.9994246627884676\n",
      "f1_score :  0.983951161180691\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58940 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "Training acc: 0.990189790725708 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906399846076965 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903916120529175 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990500271320343 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990500271320343 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908417463302612 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9911056160926819 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907020330429077 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908106923103333 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9908417801095882\n",
      "Precision :  0.9644191316063445\n",
      "Recall :  0.9994288252840008\n",
      "f1_score :  0.9816119179704543\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64424\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59158 nodes and 64424 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1.edata['h'] after reshape :  64424\n",
      "Training acc: 0.9913541674613953 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908574819564819 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916025400161743 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919595122337341 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919285178184509 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921613335609436 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992114782333374 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922854900360107 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923630952835083 tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.992378616664597\n",
      "Precision :  0.9701455558996593\n",
      "Recall :  0.9994257274119449\n",
      "f1_score :  0.9845679982399347\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138051\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.9916769889388705\n",
      "Precision :  0.9694791455320135\n",
      "Recall :  0.9971380872883377\n",
      "f1_score :  0.9831141156587553\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset3.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460164\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset3.csv': [[0, 0.757532097252284], [1, 0.24246790274771604]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58938 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9917575120925903 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899879097938538 tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991741955280304 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921765923500061 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991990327835083 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921455979347229 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921455979347229 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921610951423645 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922076463699341 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9921610629909037\n",
      "Precision :  0.968812375249501\n",
      "Recall :  0.9996781875522945\n",
      "f1_score :  0.9840032943710603\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58949 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9905157089233398 tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990670919418335 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900965690612793 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990950345993042 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990888237953186 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912297129631042 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916643500328064 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918351173400879 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992052435874939 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9917264288597063\n",
      "Precision :  0.9685519108868238\n",
      "Recall :  0.9986040609137056\n",
      "f1_score :  0.9833484332531475\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58666 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.9919437766075134 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9901586771011353 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914625883102417 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913539290428162 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921300411224365 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992300808429718 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991990327835083 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914936423301697 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919127225875854 tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9921765856384465\n",
      "Precision :  0.9696189818592681\n",
      "Recall :  0.9987069244197323\n",
      "f1_score :  0.983948022167017\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58796 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "Training acc: 0.992300808429718 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924249649047852 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919748306274414 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992021381855011 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920679330825806 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924405217170715 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920369386672974 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992052435874939 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992052435874939 tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9920834497531898\n",
      "Precision :  0.9692203723044787\n",
      "Recall :  0.9994298384542287\n",
      "f1_score :  0.9840933191940615\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64426\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58984 nodes and 64426 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64426\n",
      "G1.edata['h'] after reshape :  64426\n",
      "Training acc: 0.9911526441574097 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914786219596863 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911836981773376 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9907646179199219 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911836981773376 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911836981773376 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911526441574097 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912302494049072 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911836981773376 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9907956415111911\n",
      "Precision :  0.9633874365010532\n",
      "Recall :  0.999871407445509\n",
      "f1_score :  0.9812904243571541\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb Test instances :  138050\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9911119159724737\n",
      "Precision :  0.9674146228329564\n",
      "Recall :  0.9969228930780032\n",
      "f1_score :  0.9819471213971486\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "from dgl import from_networkx\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from dgl.data.utils import save_graphs\n",
    "\n",
    "#constante\n",
    "size_embedding = 152\n",
    "nb_batch = 5\n",
    "\n",
    "# Accuracy --------------------------------------------------------------------\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------ Model Architecture -----------------------------------------------------------------\n",
    "\n",
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        self.W_msg = nn.Linear(ndim_in + edims, ndim_out)\n",
    "        self.W_apply = nn.Linear(ndim_in + ndim_out, ndim_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        x = th.cat([edges.src['h'], edges.data['h']], 2)\n",
    "        y = self.W_msg(x)\n",
    "        return {'m': y}\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "        with g_dgl.local_scope():\n",
    "            g = g_dgl\n",
    "            g.ndata['h'] = nfeats\n",
    "            g.edata['h'] = efeats\n",
    "            # Line 4 of algorithm 1 : update all because we are using a full neighborhood sampling and not a k-hop neigh sampling\n",
    "            g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "            # Line 5 of algorithm 1\n",
    "            g.ndata['h'] = F.relu(self.W_apply(th.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "            return g.ndata['h']\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGELayer(ndim_in, edim, size_embedding, activation))\n",
    "        self.layers.append(SAGELayer(size_embedding, edim, size_embedding, activation)) ##\n",
    "        self.layers.append(SAGELayer(size_embedding, edim, ndim_out, activation))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                nfeats = self.dropout(nfeats)\n",
    "            nfeats = layer(g, nfeats, efeats)\n",
    "            # Save edge_embeddings\n",
    "            # nf = 'edge_embeddings'+str(i)+'.txt'\n",
    "            # sourceFile = open(nf, 'w')\n",
    "            # print(nfeats, file = sourceFile)\n",
    "        return nfeats.sum(1)\n",
    "        # Return a list of node features [[node1_feature1, node1_feature2, ...], [node2_feature1, node2_feature2, ...], ...]\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        v = th.cat([h_u, h_v], 1)\n",
    "        # if(pr == True):\n",
    "            # sourceFile = open(filename, 'w')\n",
    "            # if pr:\n",
    "                # print(v, file = sourceFile)\n",
    "            # sourceFile.close()\n",
    "        score = self.W(v)\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            # Update the features of the specified edges by the provided function\n",
    "            # DGLGraph.apply_edges(func, edges='__ALL__', etype=None, inplace=False)\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.gnn = SAGE(ndim_in, ndim_out, edim, activation, dropout)\n",
    "        self.pred = MLPPredictor(ndim_out, nbclasses)\n",
    "    def forward(self, g, nfeats, efeats, eweight = None):\n",
    "        if eweight != None:\n",
    "            # apply eweight on the graph\n",
    "            efe = []\n",
    "            for i, x in enumerate(eweight):\n",
    "                efe.append(list(th.Tensor.cpu(g.edata['h'][i][0]).detach().numpy() * th.Tensor.cpu(x).detach().numpy()))\n",
    "\n",
    "            efe = th.FloatTensor(efe).cuda()\n",
    "            efe = th.reshape(efe, (efe.shape[0], 1, efe.shape[1]))\n",
    "            g.edata['h'] = efe = efe\n",
    "\n",
    "        h = self.gnn(g, nfeats, efeats)\n",
    "        # h = list of node features [[node1_feature1, node1_feature2, ...], [node2_feature1, node2_feature2, ...], ...]\n",
    "        return self.pred(g, h)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# # --------------------------------------------------- MAIN -----------------------------------------------------------\n",
    "\n",
    "#Data\n",
    "nbclasses =  2\n",
    "\n",
    "\n",
    "# Model *******************************************************************************************\n",
    "# G1.ndata['h'].shape[2] = sizeh = 76 dans ANIDS\n",
    "# model1 = Model(G1.ndata['h'].shape[2], size_embedding, G1.ndata['h'].shape[2], F.relu, 0.2).cuda()\n",
    "model1 = Model(76, size_embedding, 76, F.relu, 0.2).cuda()\n",
    "opt = th.optim.Adam(model1.parameters())\n",
    "\n",
    "\n",
    "\n",
    "path, dirs, files = next(os.walk(\"/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/input/Dataset/GlobalDataset/Splitted/\"))\n",
    "file_count = len(files)\n",
    "\n",
    "\n",
    "for nb_files in range(file_count):\n",
    "    data1 = pd.read_csv(f'{path}{files[nb_files]}', encoding=\"ISO-8859–1\", dtype = str)\n",
    "\n",
    "    print(f'{files[nb_files]} ++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print(\"nb total instances in the file : \", len(data1.values))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    # Delete two columns (U and V in the excel)\n",
    "    cols = list(set(list(data1.columns )) - set(list(['Flow Bytes/s',' Flow Packets/s'])) )\n",
    "    data1 = data1[cols]\n",
    "\n",
    "    # Mise en forme des noeuds\n",
    "    data1[' Source IP'] = data1[' Source IP'].apply(str)\n",
    "    data1[' Source Port'] = data1[' Source Port'].apply(str)\n",
    "    data1[' Destination IP'] = data1[' Destination IP'].apply(str)\n",
    "    data1[' Destination Port'] = data1[' Destination Port'].apply(str)\n",
    "    data1[' Source IP'] = data1[' Source IP'] + ':' + data1[' Source Port']\n",
    "    data1[' Destination IP'] = data1[' Destination IP'] + ':' + data1[' Destination Port']\n",
    "\n",
    "    data1.drop(columns=['Flow ID',' Source Port',' Destination Port',' Timestamp'], inplace=True)\n",
    "\n",
    "    # -------------------- ????????????????????????????????????????? --------------------\n",
    "    # simply do : nom = list(data1[' Label'].unique())\n",
    "    nom = []\n",
    "    nom = nom + [data1[' Label'].unique()[0]]\n",
    "    for i in range(1, len(data1[' Label'].unique())):\n",
    "        nom = nom + [data1[' Label'].unique()[i]]\n",
    "    \n",
    "    nom.insert(0, nom.pop(nom.index('BENIGN')))\n",
    "\n",
    "    # Naming the two classes BENIGN {0} / Any Intrusion {1}\n",
    "    data1[' Label'].replace(nom[0], 0,inplace = True)\n",
    "    for i in range(1,len(data1[' Label'].unique())):\n",
    "        data1[' Label'].replace(nom[i], 1,inplace = True)\n",
    "    \n",
    "    ##################### LABELS FREQ #######################################\n",
    "    print()\n",
    "    print(\"labels freq after changing labels to binary\")\n",
    "    counts = list(data1[' Label'].value_counts().to_dict().items())\n",
    "    for j, x in enumerate(counts):\n",
    "        x = list(x)\n",
    "        x[1] = x[1] / len(data1)\n",
    "        counts[j] = x\n",
    "    print({f'{files[nb_files]}' : counts})\n",
    "    ##############################################################################\n",
    "\n",
    "    data1.rename(columns={\" Label\": \"label\"},inplace = True)\n",
    "    label1 = data1.label\n",
    "    data1.drop(columns=['label'],inplace = True)\n",
    "\n",
    "    # ******** At this step data1 contains only the data without label column\n",
    "    # ******** The label column is stored in the label variale \n",
    "\n",
    "    # split train and test\n",
    "    data1 =  pd.concat([data1, label1], axis=1) # ??????? WHY ?\n",
    "\n",
    "    # -------------------- ????????????????????????????????????????? --------------------\n",
    "    # X will contain the label column due to the concatination made earlier !!\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(data1, label1, test_size=0.3, random_state=123, stratify= label1)\n",
    "\n",
    "    # Create mini batches on the Train set\n",
    "    # 1st step : Duplicate instances of least populated classes (nb occ < 100 => x100)\n",
    "    for indx, x in enumerate(X1_train[\"label\"].value_counts()) :\n",
    "        if x < 100 :\n",
    "            inst = X1_train.loc[X1_train['label'] == X1_train[\"label\"].value_counts().index[indx]]\n",
    "            for i in range(int(100 / x)) :\n",
    "                X1_train = pd.concat([X1_train, inst], ignore_index = True)\n",
    "    \n",
    "    X1_train = shuffle(X1_train)\n",
    "    \n",
    "    # At this step we duplicated the least populated classes in the Train Set\n",
    "    # 2nd step : Create the mini batches\n",
    "    a = b = mean_macro_f1 = 0\n",
    "    for batch in range(1, nb_batch + 1):\n",
    "        print(f\"+++++++++++++++++ Batch {batch} ++++++++++++++++\")\n",
    "        a = b\n",
    "        b = int(len(X1_train) / nb_batch) * batch\n",
    "        if batch == nb_batch :\n",
    "            b = len(X1_train)\n",
    "        # The batch :\n",
    "        X1_train_batched = X1_train.iloc[a:b]\n",
    "        # y1_train_batched = y1_train.iloc[a:b]\n",
    "        y1_train_batched = X1_train_batched['label']\n",
    "\n",
    "        # Each batch will contain 64500 instance and all classes are present (The least populated one has > 10 instances)\n",
    "\n",
    "        print(\"nb Train instances : \", len(X1_train_batched.values))\n",
    "\n",
    "        # for non numerical attributes (categorical data)\n",
    "        # Since we have a binary classification, the category values willl be replaced with the posterior probability (p(target = Ti | category = Cj))\n",
    "        # TargetEncoding is also called MeanEncoding, cuz it simply replace each value with (target_i_count_on_category_j) / (total_occurences_of_category_j)\n",
    "        encoder1 = ce.TargetEncoder(cols=[' Protocol',  'Fwd PSH Flags', ' Fwd URG Flags', ' Bwd PSH Flags', ' Bwd URG Flags'])\n",
    "        encoder1.fit(X1_train_batched, y1_train_batched)\n",
    "        X1_train_batched = encoder1.transform(X1_train_batched)\n",
    "\n",
    "        # scaler (normalization)\n",
    "        scaler1 = StandardScaler()\n",
    "\n",
    "        # Manipulate flow content (all columns except : label, Source IP & Destination IP)\n",
    "        cols_to_norm1 = list(set(list(X1_train_batched.iloc[:, :].columns )) - set(list(['label', ' Source IP', ' Destination IP'])) )\n",
    "        X1_train_batched[cols_to_norm1] = scaler1.fit_transform(X1_train_batched[cols_to_norm1])\n",
    "\n",
    "        ## Create the h attribute that will contain the content of our flows\n",
    "        X1_train_batched['h'] = X1_train_batched[ cols_to_norm1 ].values.tolist()\n",
    "        # size of the list containig the content of our flows\n",
    "        sizeh = len(cols_to_norm1)\n",
    "\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Before training the data :\n",
    "        # We need to delete all the attributes (cols_to_norm1) to have the {Source IP, Destination IP, label, h} representation\n",
    "        X1_train_batched.drop(columns = cols_to_norm1, inplace = True)\n",
    "\n",
    "        # Then we need to Swap {label, h} Columns to have the {Source IP, Destination IP, h, label} representation\n",
    "        columns_titles = [' Source IP', ' Destination IP', 'h', 'label']\n",
    "        X1_train_batched = X1_train_batched.reindex(columns=columns_titles)\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # ------------------------------------------- Creating the Graph Representation -------------------------------------------------------------\n",
    "        # Create our Multigraph\n",
    "        G1 = nx.from_pandas_edgelist(X1_train_batched, \" Source IP\", \" Destination IP\", ['h','label'], create_using=nx.MultiDiGraph())\n",
    "        print(\"initial nx multigraph G1 : \", G1)\n",
    "\n",
    "        # Convert it to a directed Graph\n",
    "        # NB : IT WILL CREATE A DEFAULT BIDIRECTIONAL RELATIONSHIPS BETWEEN NODES, and not the original relationships ???????????????????????\n",
    "        # G1 = G1.to_directed()\n",
    "        # print(\"G1 after todirected : \", G1)\n",
    "        # Convert the graph from a networkx Graph to a DGL Graph\n",
    "        G1 = from_networkx(G1,edge_attrs=['h','label'] )\n",
    "        print(\"G1.edata['h'] after converting it to a dgl graph : \", len(G1.edata['h']))\n",
    "\n",
    "        # nodes data // G1.edata['h'].shape[1] : sizeh = number of attributes in a flow\n",
    "        G1.ndata['h'] = th.ones(G1.num_nodes(), G1.edata['h'].shape[1])\n",
    "        # edges data // we create a tensor bool array that will represent the train mask\n",
    "        G1.edata['train_mask'] = th.ones(len(G1.edata['h']), dtype=th.bool)\n",
    "\n",
    "        # Reshape both tensor lists to a single value in each element for both axis\n",
    "        G1.ndata['h'] = th.reshape(G1.ndata['h'], (G1.ndata['h'].shape[0], 1, G1.ndata['h'].shape[1]))\n",
    "        G1.edata['h'] = th.reshape(G1.edata['h'], (G1.edata['h'].shape[0], 1, G1.edata['h'].shape[1]))\n",
    "        print(\"G1.edata['h'] after reshape : \", len(G1.edata['h']))\n",
    "        # ------------------------------------------- --------------------------------- -------------------------------------------------------------\n",
    "\n",
    "        # ------------------------------------------- Model -----------------------------------------------------------------------------------------\n",
    "        ## use of model\n",
    "        from sklearn.utils import class_weight\n",
    "        class_weights1 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(G1.edata['label'].cpu().numpy()),\n",
    "                                                        y = G1.edata['label'].cpu().numpy())\n",
    "        ''' \n",
    "            Using class weights, you make the classifier aware of how to treat the various classes in the loss function.\n",
    "            In this process, you give higher weights to certain classes & lower weights to other classes.\n",
    "            Example : [ 0.51600999 16.11525117] \n",
    "            Basically : \n",
    "                - For classes with small number of training images, you give it more weight\n",
    "                so that the network will be punished more if it makes mistakes predicting the label of these classes. \n",
    "                - For classes with large numbers of images, you give it small weight\n",
    "        '''\n",
    "        class_weights1 = th.FloatTensor(class_weights1).cuda()\n",
    "        criterion1 = nn.CrossEntropyLoss(weight = class_weights1)\n",
    "        G1 = G1.to('cuda:0')\n",
    "\n",
    "        node_features1 = G1.ndata['h']\n",
    "        edge_features1 = G1.edata['h']\n",
    "\n",
    "        edge_label1 = G1.edata['label']\n",
    "        train_mask1 = G1.edata['train_mask']\n",
    "\n",
    "\n",
    "        # to print\n",
    "        pr = True\n",
    "        # True if you want to print the embedding vectors\n",
    "        # the name of the file where the vectors are printed\n",
    "        filename = './models/M1_weights.txt'\n",
    "\n",
    "        for epoch in range(1,1000):\n",
    "            pred = model1(G1, node_features1, edge_features1).cuda()\n",
    "            loss = criterion1(pred[train_mask1], edge_label1[train_mask1])\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training acc:', compute_accuracy(pred[train_mask1], edge_label1[train_mask1]), loss)\n",
    "\n",
    "        pred1 = model1(G1, node_features1, edge_features1).cuda()\n",
    "        pred1 = pred1.argmax(1)\n",
    "        pred1 = th.Tensor.cpu(pred1).detach().numpy()\n",
    "        edge_label1 = th.Tensor.cpu(edge_label1).detach().numpy()\n",
    "\n",
    "        print('Train metrics :')\n",
    "        print(\"Accuracy : \", sklearn.metrics.accuracy_score(edge_label1, pred1))\n",
    "        print(\"Precision : \", sklearn.metrics.precision_score(edge_label1, pred1, labels = [0,1]))\n",
    "        print(\"Recall : \", sklearn.metrics.recall_score(edge_label1, pred1, labels = [0,1]))\n",
    "        print(\"f1_score : \", sklearn.metrics.f1_score(edge_label1, pred1, labels=[0,1]))\n",
    "\n",
    "    # ------------------------------------------------ Test ---------------------------------------------------------------------\n",
    "    print(\"++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\")\n",
    "    print(\"nb Test instances : \", len(X1_test.values))\n",
    "    X1_test = encoder1.transform(X1_test)\n",
    "    X1_test[cols_to_norm1] = scaler1.transform(X1_test[cols_to_norm1])\n",
    "\n",
    "    # Save X1_test for XAI\n",
    "    X1_test.to_csv(f'/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/input/Dataset/XAI/X_test{nb_files}.csv', sep=',', index = False)\n",
    "\n",
    "    X1_test['h'] = X1_test[ cols_to_norm1 ].values.tolist()\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Before training the data :\n",
    "    # We need to delete all the attributes (cols_to_norm1) to have the {Source IP, Destination IP, label, h} representation\n",
    "    X1_test.drop(columns = cols_to_norm1, inplace = True)\n",
    "\n",
    "    # Then we need to Swap {label, h} Columns to have the {Source IP, Destination IP, h, label} representation\n",
    "    columns_titles = [' Source IP', ' Destination IP', 'h', 'label']\n",
    "    X1_test=X1_test.reindex(columns=columns_titles)\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    G1_test = nx.from_pandas_edgelist(X1_test, \" Source IP\", \" Destination IP\", ['h','label'],create_using=nx.MultiDiGraph())\n",
    "    # G1_test = G1_test.to_directed()\n",
    "    G1_test = from_networkx(G1_test,edge_attrs=['h','label'] )\n",
    "    # actual1 = G1_test.edata.pop('label')\n",
    "    actual1 = G1_test.edata['label']\n",
    "    G1_test.ndata['feature'] = th.ones(G1_test.num_nodes(), G1.ndata['h'].shape[2])\n",
    "    G1_test.ndata['feature'] = th.reshape(G1_test.ndata['feature'], (G1_test.ndata['feature'].shape[0], 1, G1_test.ndata['feature'].shape[1]))\n",
    "    G1_test.edata['h'] = th.reshape(G1_test.edata['h'], (G1_test.edata['h'].shape[0], 1, G1_test.edata['h'].shape[1]))\n",
    "    G1_test = G1_test.to('cuda:0')\n",
    "    node_features_test1 = G1_test.ndata['feature']\n",
    "    edge_features_test1 = G1_test.edata['h']\n",
    "\n",
    "    # to print\n",
    "    pr = True\n",
    "    # True if you want to print the embedding vectors\n",
    "    # the name of the file where the vectors are printed\n",
    "    filename = './models/M1_weights.txt'\n",
    "\n",
    "    print(\"nb instances : \", len(X1_test.values))\n",
    "\n",
    "    test_pred1 = model1(G1_test, node_features_test1, edge_features_test1).cuda()\n",
    "    test_pred1 = test_pred1.argmax(1)\n",
    "    test_pred1 = th.Tensor.cpu(test_pred1).detach().numpy()\n",
    "\n",
    "    print('Metrics : ')\n",
    "    print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1, test_pred1))\n",
    "    print(\"Precision : \", sklearn.metrics.precision_score(actual1, test_pred1, labels = [0,1]))\n",
    "    print(\"Recall : \", sklearn.metrics.recall_score(actual1, test_pred1, labels = [0,1]))\n",
    "    print(\"f1_score : \", sklearn.metrics.f1_score(actual1, test_pred1, labels = [0,1]))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-traffic",
   "metadata": {},
   "source": [
    "# XAI : explain_edges & explain_edge_features By Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "resident-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "from dgl import EID, NID, khop_out_subgraph\n",
    "\n",
    "\n",
    "# init mask\n",
    "def init_masks(graph, efeat):\n",
    "    # efeat.size() = torch.Size([nb_edges, 1, 76])\n",
    "    efeat_size = efeat.size()[2]\n",
    "    num_edges = graph.num_edges()\n",
    "    num_nodes = graph.num_nodes()\n",
    "    device = efeat.device\n",
    "    std = 0.1\n",
    "    # feat_mask = [[f1, f2, .... fn]] / n = nb_features\n",
    "    efeat_mask = nn.Parameter(th.randn(1, efeat_size, device=device) * std)\n",
    "    std = nn.init.calculate_gain(\"relu\") * sqrt(2.0 / (2 * num_nodes))\n",
    "    # edge_mask = [e1, e2, .... em] / m = nb_edges\n",
    "    edge_mask = nn.Parameter(th.randn(num_edges, device=device) * std)\n",
    "    return efeat_mask, edge_mask\n",
    "\n",
    "\n",
    "# Regularization loss\n",
    "def loss_regularize(loss, feat_mask, edge_mask):\n",
    "    # epsilon for numerical stability\n",
    "    eps = 1e-15\n",
    "    # From self GNNExplainer self\n",
    "    alpha1 = 0.005,\n",
    "    alpha2 = 1.0\n",
    "    beta1 = 1.0\n",
    "    beta2 = 0.1\n",
    "\n",
    "    edge_mask = edge_mask.sigmoid()\n",
    "    # Edge mask sparsity regularization\n",
    "    loss = loss + th.from_numpy(alpha1 * th.Tensor.cpu(th.sum(edge_mask)).detach().numpy()).cuda()\n",
    "    # Edge mask entropy regularization\n",
    "    ent = -edge_mask * th.log(edge_mask + eps) - (\n",
    "        1 - edge_mask\n",
    "    ) * th.log(1 - edge_mask + eps)\n",
    "    loss = loss + alpha2 * ent.mean()\n",
    "\n",
    "    feat_mask = feat_mask.sigmoid()\n",
    "    # Feature mask sparsity regularization\n",
    "    loss = loss + beta1 * th.mean(feat_mask)\n",
    "    # Feature mask entropy regularization\n",
    "    ent = -feat_mask * th.log(feat_mask + eps) - (\n",
    "        1 - feat_mask\n",
    "    ) * th.log(1 - feat_mask + eps)\n",
    "    loss = loss + beta2 * ent.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Edge features\n",
    "def explain_edge_features(model, graph, node_feat, edge_feat, class_to_explain = None, **kwargs):\n",
    "    model = model.to(graph.device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "    print(\"graph : \", graph)\n",
    "    print(\"graph_edges : \", graph.edges()) # edges ids in graph.edges()\n",
    "    print(\"graph_nodes : \", graph.nodes()) # nodes ids in graph.nodes()\n",
    "    print(\"edge_feat : \", edge_feat)\n",
    "    print(\"node_feat : \", node_feat)\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "\n",
    "    # If we add kwargs\n",
    "    # for key, item in kwargs.items():\n",
    "    #     if th.is_tensor(item) and item.size(0) == num_nodes:\n",
    "    #         item = item[sg_nodes]\n",
    "    #     elif th.is_tensor(item) and item.size(0) == num_edges:\n",
    "    #         item = item[sg_edges]\n",
    "    #     kwargs[key] = item\n",
    "\n",
    "    # Get the initial prediction.\n",
    "    print(\"Get the initial prediction :\")\n",
    "    with th.no_grad():\n",
    "        # logits = model(g = sg, nfeats = node_feat, efeats = edge_feat, **kwargs)\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat)\n",
    "        pred_label = logits.argmax(dim=-1)\n",
    "        # pred_label1 = logits.argmax(1)\n",
    "\n",
    "    print(\"pred_label : \", pred_label)\n",
    "\n",
    "    #\n",
    "    efeat_mask, edge_mask = init_masks(graph, edge_feat)\n",
    "\n",
    "    params = [efeat_mask]\n",
    "    # params = [efeat_mask]\n",
    "    # lr=0.01\n",
    "    optimizer = th.optim.Adam(params, lr = 0.01)\n",
    "\n",
    "    # num_epochs = 100\n",
    "    print(\"***********************************\")\n",
    "    print(\"initial masks : \")\n",
    "    print(\"efeat_mask : \", efeat_mask)\n",
    "    print(\"***********************************\")\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights2 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(graph.edata['label'].cpu().numpy()),\n",
    "                                                        y = graph.edata['label'].cpu().numpy())\n",
    "    class_weights2 = th.FloatTensor(class_weights2).cuda()\n",
    "    criterion2 = nn.CrossEntropyLoss(weight = class_weights2)\n",
    "    train_mask2 = th.ones(len(graph.edata['h']), dtype=th.bool)\n",
    "    import datetime\n",
    "    \n",
    "    print(f'explanation starts at {datetime.datetime.now()}')\n",
    "    print(\"nb edges : \", graph.num_edges())\n",
    "    print(\"nb nodes : \", graph.num_nodes())\n",
    "    \n",
    "    if (class_to_explain != None) :\n",
    "        print(f'explanation for class {class_to_explain} starts at {datetime.datetime.now()}')\n",
    "        label_column = graph.edata['label']\n",
    "        cls_indx = []\n",
    "        for i, x in enumerate(label_column):\n",
    "            if (x == float(class_to_explain)):\n",
    "                cls_indx.append(i)\n",
    "    \n",
    "    for epoch in range(1,300):\n",
    "        optimizer.zero_grad()\n",
    "        # Matrix multiplication\n",
    "        h = edge_feat * efeat_mask.sigmoid()\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = h).cuda()\n",
    "        # pred_label = tensor([0, 0, 0,  ..., 0, 1, 0], device='cuda:0')\n",
    "        # logits = tensor([[ 0.0059,  0.0517], [-0.0075,  0.0101], ..., device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "        if (class_to_explain != None) :\n",
    "            loss = criterion2(logits[cls_indx], pred_label[cls_indx])\n",
    "        else :\n",
    "            loss = criterion2(logits[train_mask2], pred_label[train_mask2])\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"+++++++++++++++\")\n",
    "            print(f'epoch number {epoch}, CrossEntropy_Loss = {loss}, time = {datetime.datetime.now()}')\n",
    "            print(\"efeat_mask : \", efeat_mask.detach().sigmoid())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    efeat_mask = efeat_mask.detach().sigmoid().squeeze()\n",
    "\n",
    "    return efeat_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Edge mask\n",
    "def explain_edges(model, graph, node_feat, edge_feat, class_to_explain = None, **kwargs):\n",
    "    model = model.to(graph.device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "    print(\"graph : \", graph)\n",
    "    print(\"graph_edges : \", graph.edges()) # edges ids in graph.edges()\n",
    "    print(\"graph_nodes : \", graph.nodes()) # nodes ids in graph.nodes()\n",
    "    print(\"edge_feat : \", edge_feat)\n",
    "    print(\"node_feat : \", node_feat)\n",
    "    print(\"+++++++++++++++++++++++\")\n",
    "\n",
    "    # If we add kwargs\n",
    "    # for key, item in kwargs.items():\n",
    "    #     if th.is_tensor(item) and item.size(0) == num_nodes:\n",
    "    #         item = item[sg_nodes]\n",
    "    #     elif th.is_tensor(item) and item.size(0) == num_edges:\n",
    "    #         item = item[sg_edges]\n",
    "    #     kwargs[key] = item\n",
    "\n",
    "    # Get the initial prediction.\n",
    "    print(\"Get the initial prediction :\")\n",
    "    with th.no_grad():\n",
    "        # logits = model(g = sg, nfeats = node_feat, efeats = edge_feat, **kwargs)\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat)\n",
    "        pred_label = logits.argmax(dim=-1)\n",
    "        # pred_label1 = logits.argmax(1)\n",
    "\n",
    "    print(\"pred_label : \", pred_label)\n",
    "\n",
    "    #\n",
    "    efeat_mask, edge_mask = init_masks(graph, edge_feat)\n",
    "\n",
    "    params = [edge_mask]\n",
    "    # params = [efeat_mask]\n",
    "    # lr=0.01\n",
    "    optimizer = th.optim.Adam(params, lr = 0.01)\n",
    "\n",
    "    # num_epochs = 100\n",
    "    print(\"***********************************\")\n",
    "    print(\"initial masks : \")\n",
    "    print(\"edge_mask : \", edge_mask)\n",
    "    print(\"***********************************\")\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights2 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(graph.edata['label'].cpu().numpy()),\n",
    "                                                        y = graph.edata['label'].cpu().numpy())\n",
    "    class_weights2 = th.FloatTensor(class_weights2).cuda()\n",
    "    criterion2 = nn.CrossEntropyLoss(weight = class_weights2)\n",
    "    train_mask2 = th.ones(len(graph.edata['h']), dtype=th.bool)\n",
    "    import datetime\n",
    "    \n",
    "    print(f'explanation starts at {datetime.datetime.now()}')\n",
    "    print(\"nb edges : \", graph.num_edges())\n",
    "    print(\"nb nodes : \", graph.num_nodes())\n",
    "    \n",
    "    if (class_to_explain != None) :\n",
    "        print(f'explanation for class {class_to_explain} starts at {datetime.datetime.now()}')\n",
    "        label_column = graph.edata['label']\n",
    "        cls_indx = []\n",
    "        for i, x in enumerate(label_column):\n",
    "            if (x == float(class_to_explain)):\n",
    "                cls_indx.append(i)\n",
    "    \n",
    "    for epoch in range(1,300):\n",
    "        optimizer.zero_grad()\n",
    "        # Matrix multiplication\n",
    "        logits = model(g = graph, nfeats = node_feat, efeats = edge_feat, eweight=edge_mask.sigmoid()).cuda()\n",
    "        # pred_label = tensor([0, 0, 0,  ..., 0, 1, 0], device='cuda:0')\n",
    "        # logits = tensor([[ 0.0059,  0.0517], [-0.0075,  0.0101], ..., device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "        if (class_to_explain != None) :\n",
    "            loss11 = criterion2(logits[cls_indx], pred_label[cls_indx])\n",
    "        else :\n",
    "            loss11 = criterion2(logits[train_mask2], pred_label[train_mask2])\n",
    "        loss = loss_regularize(loss11, efeat_mask, edge_mask)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"+++++++++++++++\")\n",
    "            print(f'epoch number {epoch}, CrossEntropy_Loss = {loss11}, final_loss = {loss}, time = {datetime.datetime.now()}')\n",
    "            print(\"edge_mask : \", edge_mask.detach().sigmoid())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    edge_mask = edge_mask.detach().sigmoid()\n",
    "\n",
    "    return edge_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-peninsula",
   "metadata": {},
   "source": [
    "# XAI : Results interpretation by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "novel-stomach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++\n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "graph_edges :  (tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "graph_nodes :  tensor([     0,      1,      2,  ..., 104637, 104638, 104639], device='cuda:0')\n",
      "edge_feat :  tensor([[[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -3.6182e-03, -1.8382e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.1850e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -1.3156e-02,  8.1388e-02]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -2.3382e-04, -1.8379e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "           1.1976e-01, -1.8380e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6073e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -1.4645e-03, -1.8274e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6070e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "           6.5607e-02, -1.8269e-01]]], device='cuda:0')\n",
      "node_feat :  tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "Get the initial prediction :\n",
      "pred_label :  tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "***********************************\n",
      "initial masks : \n",
      "efeat_mask :  Parameter containing:\n",
      "tensor([[-0.0433, -0.1315, -0.1070,  0.1519,  0.0571, -0.1693, -0.0972, -0.0325,\n",
      "          0.0973, -0.0703,  0.0205, -0.0238, -0.0036,  0.0911, -0.0483,  0.0358,\n",
      "          0.1096,  0.0158, -0.0834,  0.0378, -0.0429, -0.1069,  0.0810, -0.0341,\n",
      "         -0.0637, -0.0876, -0.0586,  0.0201,  0.0409,  0.2446, -0.0825, -0.0463,\n",
      "         -0.0834,  0.0311, -0.0188, -0.0870,  0.0871,  0.0413, -0.0177,  0.0939,\n",
      "         -0.0158,  0.0508, -0.0674, -0.0684,  0.0393,  0.0636, -0.1212, -0.0358,\n",
      "         -0.1059,  0.0724, -0.1078,  0.0504,  0.0330, -0.1180, -0.0129, -0.1615,\n",
      "          0.0012,  0.2001,  0.0689,  0.0465, -0.0505,  0.0730, -0.2749, -0.0844,\n",
      "          0.0044, -0.0806, -0.1617,  0.0485,  0.2167,  0.0330,  0.0451,  0.2190,\n",
      "         -0.0479,  0.0658,  0.0871, -0.0189]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "***********************************\n",
      "explanation starts at 2023-05-21 22:59:38.796298\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "explanation for class 1 starts at 2023-05-21 22:59:38.796871\n",
      "+++++++++++++++\n",
      "epoch number 100, CrossEntropy_Loss = 0.032135818153619766, time = 2023-05-21 22:59:48.922472\n",
      "efeat_mask :  tensor([[0.4781, 0.3969, 0.4733, 0.6152, 0.4570, 0.6078, 0.4036, 0.5693, 0.6827,\n",
      "         0.3142, 0.5709, 0.6382, 0.4150, 0.6723, 0.5713, 0.6647, 0.3624, 0.5808,\n",
      "         0.5560, 0.5095, 0.4614, 0.4192, 0.6643, 0.6538, 0.6500, 0.3078, 0.5520,\n",
      "         0.4738, 0.5766, 0.4719, 0.4960, 0.5602, 0.6485, 0.4347, 0.6343, 0.4783,\n",
      "         0.5893, 0.6194, 0.4956, 0.5856, 0.6050, 0.4232, 0.4832, 0.5501, 0.5776,\n",
      "         0.5830, 0.3931, 0.4202, 0.6439, 0.6709, 0.6431, 0.6675, 0.5923, 0.5677,\n",
      "         0.5133, 0.4597, 0.4607, 0.4595, 0.7403, 0.5808, 0.4874, 0.5855, 0.5639,\n",
      "         0.5438, 0.6557, 0.4744, 0.6253, 0.4739, 0.6186, 0.6913, 0.5041, 0.4116,\n",
      "         0.4022, 0.5164, 0.6737, 0.6029]], device='cuda:0')\n",
      "+++++++++++++++\n",
      "epoch number 200, CrossEntropy_Loss = 0.02186284400522709, time = 2023-05-21 22:59:57.087838\n",
      "efeat_mask :  tensor([[0.4160, 0.3841, 0.4733, 0.6349, 0.4100, 0.6702, 0.3888, 0.5935, 0.8035,\n",
      "         0.2268, 0.5825, 0.6927, 0.3938, 0.7515, 0.5960, 0.7461, 0.2661, 0.6100,\n",
      "         0.5748, 0.5095, 0.5007, 0.3720, 0.7776, 0.7263, 0.7353, 0.2204, 0.5604,\n",
      "         0.6640, 0.5900, 0.4467, 0.5989, 0.5748, 0.7528, 0.4137, 0.7082, 0.4783,\n",
      "         0.6042, 0.6632, 0.4956, 0.5946, 0.6655, 0.3959, 0.4832, 0.5640, 0.5917,\n",
      "         0.5951, 0.3543, 0.4012, 0.7394, 0.7499, 0.7386, 0.7420, 0.6132, 0.5959,\n",
      "         0.4960, 0.4597, 0.4835, 0.4340, 0.8456, 0.6009, 0.4874, 0.5980, 0.6167,\n",
      "         0.5596, 0.7388, 0.5836, 0.6914, 0.3928, 0.6271, 0.7814, 0.4489, 0.3588,\n",
      "         0.3782, 0.5164, 0.7532, 0.6606]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "graph_edges :  (tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "graph_nodes :  tensor([     0,      1,      2,  ..., 104637, 104638, 104639], device='cuda:0')\n",
      "edge_feat :  tensor([[[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -3.6182e-03, -1.8382e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.1850e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -1.3156e-02,  8.1388e-02]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -2.3382e-04, -1.8379e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "           1.1976e-01, -1.8380e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6073e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -1.4645e-03, -1.8274e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6070e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "           6.5607e-02, -1.8269e-01]]], device='cuda:0')\n",
      "node_feat :  tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "Get the initial prediction :\n",
      "pred_label :  tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "***********************************\n",
      "initial masks : \n",
      "edge_mask :  Parameter containing:\n",
      "tensor([ 0.0053, -0.0019, -0.0021,  ...,  0.0059,  0.0004,  0.0040],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "***********************************\n",
      "explanation starts at 2023-05-21 23:00:05.282910\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "explanation for class 1 starts at 2023-05-21 23:00:05.283241\n",
      "+++++++++++++++\n",
      "epoch number 100, CrossEntropy_Loss = 0.029566660523414612, final_loss = tensor([347.4567], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), time = 2023-05-21 23:11:46.713681\n",
      "edge_mask :  tensor([0.7832, 0.2186, 0.2184,  ..., 0.7834, 0.7787, 0.7827], device='cuda:0')\n",
      "+++++++++++++++\n",
      "epoch number 200, CrossEntropy_Loss = 0.029566660523414612, final_loss = tensor([347.8044], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), time = 2023-05-21 23:23:28.778279\n",
      "edge_mask :  tensor([0.9258, 0.0743, 0.0743,  ..., 0.9258, 0.9255, 0.9257], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "graph_edges :  (tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "graph_nodes :  tensor([     0,      1,      2,  ..., 104637, 104638, 104639], device='cuda:0')\n",
      "edge_feat :  tensor([[[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -3.6182e-03, -1.8382e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.1850e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -1.3156e-02,  8.1388e-02]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -2.3382e-04, -1.8379e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "           1.1976e-01, -1.8380e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6073e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -1.4645e-03, -1.8274e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6070e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "           6.5607e-02, -1.8269e-01]]], device='cuda:0')\n",
      "node_feat :  tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "Get the initial prediction :\n",
      "pred_label :  tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "***********************************\n",
      "initial masks : \n",
      "efeat_mask :  Parameter containing:\n",
      "tensor([[ 0.1741, -0.1997,  0.1623,  0.1116, -0.1137,  0.1906, -0.0674, -0.0443,\n",
      "          0.1375,  0.1052, -0.0842,  0.0118,  0.0710, -0.0480,  0.0822,  0.0800,\n",
      "         -0.1036,  0.2134, -0.2139,  0.3085, -0.0784,  0.0196,  0.0127, -0.1147,\n",
      "          0.0059,  0.0557, -0.0648, -0.0486,  0.1303, -0.0261,  0.0780,  0.0060,\n",
      "          0.0475,  0.0258,  0.1448, -0.0178,  0.0135, -0.2533, -0.0339, -0.0944,\n",
      "          0.1648,  0.0171,  0.0150, -0.2334, -0.0312, -0.1098, -0.1554, -0.0285,\n",
      "         -0.1746, -0.1056,  0.1594,  0.2466, -0.0245, -0.0035, -0.2524,  0.1179,\n",
      "         -0.0706, -0.0339, -0.1572,  0.1376,  0.1026,  0.1366,  0.1144,  0.0314,\n",
      "         -0.0175,  0.0384,  0.1252, -0.0357,  0.0144, -0.0781,  0.1912, -0.0131,\n",
      "          0.0668, -0.0422, -0.0086, -0.1950]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "***********************************\n",
      "explanation starts at 2023-05-21 23:35:05.230828\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "explanation for class 0 starts at 2023-05-21 23:35:05.230937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++\n",
      "epoch number 100, CrossEntropy_Loss = 0.008765459060668945, time = 2023-05-21 23:35:15.600822\n",
      "efeat_mask :  tensor([[0.3107, 0.2507, 0.5405, 0.6761, 0.6597, 0.6741, 0.4784, 0.6369, 0.7400,\n",
      "         0.3114, 0.5602, 0.3395, 0.5483, 0.6694, 0.6617, 0.4026, 0.2819, 0.7230,\n",
      "         0.5947, 0.5765, 0.3089, 0.6381, 0.7076, 0.3015, 0.6907, 0.3007, 0.4084,\n",
      "         0.7024, 0.6791, 0.7361, 0.7177, 0.3033, 0.7136, 0.3256, 0.3471, 0.4956,\n",
      "         0.6689, 0.6403, 0.4915, 0.5489, 0.7256, 0.5092, 0.5038, 0.6057, 0.6483,\n",
      "         0.4696, 0.6950, 0.3128, 0.7129, 0.3759, 0.7717, 0.7412, 0.5919, 0.2944,\n",
      "         0.6099, 0.5294, 0.2935, 0.7036, 0.6716, 0.7106, 0.5256, 0.6586, 0.3529,\n",
      "         0.6979, 0.7434, 0.6950, 0.3544, 0.6572, 0.4354, 0.6855, 0.3127, 0.7101,\n",
      "         0.4810, 0.4895, 0.7540, 0.5876]], device='cuda:0')\n",
      "+++++++++++++++\n",
      "epoch number 200, CrossEntropy_Loss = 0.006618345156311989, time = 2023-05-21 23:35:24.131227\n",
      "efeat_mask :  tensor([[0.2082, 0.1700, 0.5405, 0.7080, 0.7678, 0.6436, 0.6726, 0.7591, 0.8311,\n",
      "         0.2066, 0.5444, 0.2103, 0.7390, 0.6766, 0.7089, 0.2234, 0.1986, 0.8032,\n",
      "         0.6413, 0.5765, 0.2465, 0.7088, 0.7979, 0.2435, 0.7482, 0.1996, 0.4562,\n",
      "         0.8266, 0.7293, 0.8568, 0.8100, 0.2025, 0.8210, 0.2384, 0.2072, 0.4956,\n",
      "         0.7369, 0.7346, 0.4915, 0.5189, 0.8150, 0.6479, 0.5038, 0.6728, 0.7093,\n",
      "         0.3611, 0.8351, 0.2321, 0.7963, 0.2023, 0.8388, 0.8122, 0.4780, 0.1700,\n",
      "         0.7216, 0.5294, 0.2167, 0.8412, 0.7870, 0.7824, 0.5256, 0.6827, 0.2177,\n",
      "         0.7865, 0.7373, 0.7794, 0.2482, 0.6908, 0.5115, 0.7803, 0.2106, 0.8427,\n",
      "         0.5420, 0.4895, 0.7568, 0.6792]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "graph_edges :  (tensor([ 6444,  6444, 70375,  ..., 73434, 32319, 25791], device='cuda:0'), tensor([ 70375,  68097,  11453,  ...,  99923, 101037,  20714], device='cuda:0'))\n",
      "graph_nodes :  tensor([     0,      1,      2,  ..., 104637, 104638, 104639], device='cuda:0')\n",
      "edge_feat :  tensor([[[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -3.6182e-03, -1.8382e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.1850e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -1.3156e-02,  8.1388e-02]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -2.3382e-04, -1.8379e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7622e-02, -2.6085e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "           1.1976e-01, -1.8380e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6073e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "          -1.4645e-03, -1.8274e-01]],\n",
      "\n",
      "        [[-1.7622e-02, -2.6070e-01,  0.0000e+00,  ...,  1.1102e-16,\n",
      "           6.5607e-02, -1.8269e-01]]], device='cuda:0')\n",
      "node_feat :  tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n",
      "+++++++++++++++++++++++\n",
      "Get the initial prediction :\n",
      "pred_label :  tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "***********************************\n",
      "initial masks : \n",
      "edge_mask :  Parameter containing:\n",
      "tensor([ 4.6827e-03,  4.8043e-03, -2.8579e-03,  ...,  1.6381e-05,\n",
      "         4.8486e-03, -1.8255e-03], device='cuda:0', requires_grad=True)\n",
      "***********************************\n",
      "explanation starts at 2023-05-21 23:35:32.683334\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "explanation for class 0 starts at 2023-05-21 23:35:32.683624\n",
      "+++++++++++++++\n",
      "epoch number 100, CrossEntropy_Loss = 0.006972975097596645, final_loss = tensor([345.7368], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), time = 2023-05-21 23:47:17.763469\n",
      "edge_mask :  tensor([0.7830, 0.7830, 0.2179,  ..., 0.7713, 0.7831, 0.2186], device='cuda:0')\n",
      "+++++++++++++++\n",
      "epoch number 200, CrossEntropy_Loss = 0.006972975097596645, final_loss = tensor([345.2328], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), time = 2023-05-21 23:58:59.541527\n",
      "edge_mask :  tensor([0.9258, 0.9258, 0.0743,  ..., 0.9249, 0.9258, 0.0743], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "efeat_mask_intusion = explain_edge_features(model1, G1_test, node_features_test1, edge_features_test1, 1)\n",
    "edge_mask_intrusion = explain_edges(model1, G1_test, node_features_test1, edge_features_test1, 1)\n",
    "\n",
    "efeat_mask_benign = explain_edge_features(model1, G1_test, node_features_test1, edge_features_test1, 0)\n",
    "edge_mask_benign = explain_edges(model1, G1_test, node_features_test1, edge_features_test1, 0)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "considered-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final results : \n",
      "graph :  Graph(num_nodes=104640, num_edges=138050,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "nb edges :  138050\n",
      "nb nodes :  104640\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Intrusions masks : \n",
      "efeat_mask_intusion :  tensor([0.3711, 0.3812, 0.4733, 0.6481, 0.3865, 0.7131, 0.3806, 0.6060, 0.8580,\n",
      "        0.1745, 0.5886, 0.7296, 0.3812, 0.8019, 0.6136, 0.7977, 0.2052, 0.6335,\n",
      "        0.5872, 0.5095, 0.5142, 0.3421, 0.8402, 0.7649, 0.7828, 0.1692, 0.5599,\n",
      "        0.7785, 0.5987, 0.4387, 0.6832, 0.5832, 0.8162, 0.3996, 0.7585, 0.4783,\n",
      "        0.6137, 0.6948, 0.4956, 0.5989, 0.7121, 0.3846, 0.4832, 0.5726, 0.6005,\n",
      "        0.6015, 0.3286, 0.3886, 0.7983, 0.7999, 0.7976, 0.7807, 0.6264, 0.6150,\n",
      "        0.4842, 0.4597, 0.5002, 0.4265, 0.8963, 0.6161, 0.4874, 0.6053, 0.6550,\n",
      "        0.5745, 0.7917, 0.6820, 0.7365, 0.3330, 0.6272, 0.8310, 0.4011, 0.3220,\n",
      "        0.3701, 0.5164, 0.8031, 0.7061], device='cuda:0')\n",
      "edge_mask_intrusion :  tensor([0.9662, 0.0338, 0.0338,  ..., 0.9662, 0.9663, 0.9662], device='cuda:0')\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Benign masks : \n",
      "efeat_mask_benign :  tensor([0.1426, 0.1399, 0.5405, 0.7253, 0.8277, 0.6194, 0.8570, 0.8377, 0.8763,\n",
      "        0.1539, 0.5421, 0.1462, 0.8273, 0.5197, 0.7590, 0.1065, 0.1452, 0.8380,\n",
      "        0.6837, 0.5765, 0.2199, 0.7480, 0.8476, 0.2111, 0.7839, 0.1489, 0.5885,\n",
      "        0.8849, 0.7757, 0.9016, 0.8587, 0.1602, 0.8728, 0.1727, 0.1234, 0.4956,\n",
      "        0.7790, 0.7941, 0.4915, 0.5002, 0.8453, 0.6339, 0.5038, 0.7145, 0.7496,\n",
      "        0.2932, 0.8903, 0.1721, 0.8064, 0.0949, 0.8471, 0.8482, 0.3795, 0.1184,\n",
      "        0.7950, 0.5294, 0.1902, 0.8917, 0.8474, 0.8213, 0.5256, 0.6946, 0.1521,\n",
      "        0.8331, 0.6689, 0.8281, 0.1919, 0.6867, 0.6598, 0.8388, 0.1449, 0.8921,\n",
      "        0.4960, 0.4895, 0.6984, 0.6951], device='cuda:0')\n",
      "edge_mask_benign :  tensor([0.9662, 0.9662, 0.0338,  ..., 0.9663, 0.9662, 0.0338], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"final results : \")\n",
    "print(\"graph : \", G1_test)\n",
    "print(\"nb edges : \", G1_test.num_edges())\n",
    "print(\"nb nodes : \", G1_test.num_nodes())\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(\"Intrusions masks : \")\n",
    "print(\"efeat_mask_intusion : \", efeat_mask_intusion)\n",
    "print(\"edge_mask_intrusion : \", edge_mask_intrusion)\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(\"Benign masks : \")\n",
    "print(\"efeat_mask_benign : \", efeat_mask_benign)\n",
    "print(\"edge_mask_benign : \", edge_mask_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sized-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0.,  ..., 1., 1., 1.], device='cuda:0')\n",
      "tensor([1., 1., 0.,  ..., 1., 1., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "edge_mask_intrusion_round = th.round(edge_mask_intrusion)\n",
    "edge_mask_benign_round = th.round(edge_mask_benign)\n",
    "\n",
    "print(edge_mask_intrusion_round)\n",
    "print(edge_mask_benign_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floral-trout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([     1,      2,     10,  ..., 138042, 138043, 138045], device='cuda:0')\n",
      "tensor([     2,      5,      7,  ..., 138041, 138044, 138049], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print((edge_mask_intrusion_round == 0).nonzero(as_tuple=True)[0])\n",
    "print((edge_mask_benign_round == 0).nonzero(as_tuple=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complete-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "g_intrusion = dgl.remove_edges(G1_test, (edge_mask_intrusion_round == 0).nonzero(as_tuple=True)[0])\n",
    "isolated_nodes = ((g_intrusion.in_degrees() == 0) & (g_intrusion.out_degrees() == 0)).nonzero().squeeze(1)\n",
    "g_intrusion.remove_nodes(isolated_nodes)\n",
    "\n",
    "g_benign = dgl.remove_edges(G1_test, (edge_mask_benign_round == 0).nonzero(as_tuple=True)[0])\n",
    "isolated_nodes = ((g_benign.in_degrees() == 0) & (g_benign.out_degrees() == 0)).nonzero().squeeze(1)\n",
    "g_benign.remove_nodes(isolated_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "artificial-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=62598, num_edges=69450,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n",
      "Graph(num_nodes=61960, num_edges=68851,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(1, 76), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "print(g_intrusion)\n",
    "print(g_benign)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-jersey",
   "metadata": {},
   "source": [
    "### g_intrusion analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "unlikely-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of intrusions in the g_intrusion subgraph = 24.05183585313175\n",
      "% of benign in the g_intrusion subgraph = 75.94816414686825\n",
      "*******\n",
      "% of intrusions taken from the original test graph = 49.90290682042243\n",
      "% of benign taken from the original test graph = 50.4374766918156\n"
     ]
    }
   ],
   "source": [
    "nb_intrusions = sum(g_intrusion.edata['label'])\n",
    "nb_benign = g_intrusion.num_edges() - nb_intrusions\n",
    "print(\"% of intrusions in the g_intrusion subgraph =\", (nb_intrusions.item() / g_intrusion.num_edges()) * 100)\n",
    "print(\"% of benign in the g_intrusion subgraph =\", (nb_benign.item() / g_intrusion.num_edges()) * 100)\n",
    "\n",
    "print('*******')\n",
    "\n",
    "nb_intrusions_orig = sum(G1_test.edata['label'])\n",
    "nb_benign_orig = G1_test.num_edges() - nb_intrusions_orig\n",
    "\n",
    "print(\"% of intrusions taken from the original test graph =\", (nb_intrusions.item() / nb_intrusions_orig.item()) * 100)\n",
    "print(\"% of benign taken from the original test graph =\", (nb_benign.item() / nb_benign_orig.item()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-editor",
   "metadata": {},
   "source": [
    "### g_benign analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "guilty-scotland",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g_benign' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d356f55acc06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb_intrusions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_benign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnb_benign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_benign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnb_intrusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"% of intrusions in the g_benign subgraph =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnb_intrusions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mg_benign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"% of benign in the g_benign subgraph =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnb_benign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mg_benign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g_benign' is not defined"
     ]
    }
   ],
   "source": [
    "nb_intrusions = sum(g_benign.edata['label'])\n",
    "nb_benign = g_benign.num_edges() - nb_intrusions\n",
    "print(\"% of intrusions in the g_benign subgraph =\", (nb_intrusions.item() / g_benign.num_edges()) * 100)\n",
    "print(\"% of benign in the g_benign subgraph =\", (nb_benign.item() / g_benign.num_edges()) * 100)\n",
    "\n",
    "print('*******')\n",
    "\n",
    "nb_intrusions_orig = sum(G1_test.edata['label'])\n",
    "nb_benign_orig = G1_test.num_edges() - nb_intrusions_orig\n",
    "\n",
    "print(\"% of intrusions taken from the original test graph =\", (nb_intrusions.item() / nb_intrusions_orig.item()) * 100)\n",
    "print(\"% of benign taken from the original test graph =\", (nb_benign.item() / nb_benign_orig.item()) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-starter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

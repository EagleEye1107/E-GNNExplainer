{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mental-astronomy",
   "metadata": {},
   "source": [
    "# E-GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "violent-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl import from_networkx\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from dgl.data.utils import save_graphs\n",
    "\n",
    "#constante\n",
    "size_embedding = 152\n",
    "nb_batch = 5\n",
    "\n",
    "#Data\n",
    "nbclasses =  2\n",
    "\n",
    "# Accuracy --------------------------------------------------------------------\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------ Model Architecture -----------------------------------------------------------------\n",
    "\n",
    "class SAGELayer(nn.Module):\n",
    "    def __init__(self, ndim_in, edims, ndim_out, activation):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        self.W_msg = nn.Linear(ndim_in + edims, ndim_out)\n",
    "        self.W_apply = nn.Linear(ndim_in + ndim_out, ndim_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        x = th.cat([edges.src['h'], edges.data['h']], 2)\n",
    "        y = self.W_msg(x)\n",
    "        return {'m': y}\n",
    "\n",
    "    def forward(self, g_dgl, nfeats, efeats):\n",
    "        with g_dgl.local_scope():\n",
    "            g = g_dgl\n",
    "            g.ndata['h'] = nfeats\n",
    "            g.edata['h'] = efeats\n",
    "            # Line 4 of algorithm 1 : update all because we are using a full neighborhood sampling and not a k-hop neigh sampling\n",
    "            g.update_all(self.message_func, fn.mean('m', 'h_neigh'))\n",
    "            # Line 5 of algorithm 1\n",
    "            g.ndata['h'] = F.relu(self.W_apply(th.cat([g.ndata['h'], g.ndata['h_neigh']], 2)))\n",
    "            return g.ndata['h']\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGELayer(ndim_in, edim, size_embedding, activation))\n",
    "        self.layers.append(SAGELayer(size_embedding, edim, size_embedding, activation)) ##\n",
    "        self.layers.append(SAGELayer(size_embedding, edim, ndim_out, activation))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, g, nfeats, efeats):\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                nfeats = self.dropout(nfeats)\n",
    "            nfeats = layer(g, nfeats, efeats)\n",
    "            # Save edge_embeddings\n",
    "            # nf = 'edge_embeddings'+str(i)+'.txt'\n",
    "            # sourceFile = open(nf, 'w')\n",
    "            # print(nfeats, file = sourceFile)\n",
    "        return nfeats.sum(1)\n",
    "        # Return a list of node features [[node1_feature1, node1_feature2, ...], [node2_feature1, node2_feature2, ...], ...]\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        v = th.cat([h_u, h_v], 1)\n",
    "        # if(pr == True):\n",
    "            # sourceFile = open(filename, 'w')\n",
    "            # if pr:\n",
    "                # print(v, file = sourceFile)\n",
    "            # sourceFile.close()\n",
    "        score = self.W(v)\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            # Update the features of the specified edges by the provided function\n",
    "            # DGLGraph.apply_edges(func, edges='__ALL__', etype=None, inplace=False)\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ndim_in, ndim_out, edim, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.gnn = SAGE(ndim_in, ndim_out, edim, activation, dropout)\n",
    "        self.pred = MLPPredictor(ndim_out, nbclasses)\n",
    "    def forward(self, g, nfeats, efeats, eweight = None):\n",
    "        if eweight != None:\n",
    "            # apply eweight on the graph\n",
    "            efe = []\n",
    "            for i, x in enumerate(eweight):\n",
    "                efe.append(list(th.Tensor.cpu(g.edata['h'][i][0]).detach().numpy() * th.Tensor.cpu(x).detach().numpy()))\n",
    "\n",
    "            efe = th.FloatTensor(efe).cuda()\n",
    "            efe = th.reshape(efe, (efe.shape[0], 1, efe.shape[1]))\n",
    "            g.edata['h'] = efe = efe\n",
    "\n",
    "        h = self.gnn(g, nfeats, efeats)\n",
    "        # h = list of node features [[node1_feature1, node1_feature2, ...], [node2_feature1, node2_feature2, ...], ...]\n",
    "        return self.pred(g, h)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-pursuit",
   "metadata": {},
   "source": [
    "## Graph + Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fallen-uruguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIC-IDS-2017-Dataset4.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460167\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset4.csv': [[0, 0.7582073464633492], [1, 0.24179265353665083]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58978 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9852691888809204 tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9869921803474426 tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9873492121696472 tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9880322217941284 tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990251898765564 tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900966882705688 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893360733985901 tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903450608253479 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910901188850403 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.991260885087624\n",
      "Precision :  0.9677663533716016\n",
      "Recall :  0.9966220605430687\n",
      "f1_score :  0.9819822702979486\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.93534916639328 tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9467270970344543 tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9532464742660522 tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9517873525619507 tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9638793468475342 tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9647330641746521 tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.972944438457489 tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.973736047744751 tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9780046939849854 tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9745432531859739\n",
      "Precision :  0.9128346740304959\n",
      "Recall :  0.98778744965571\n",
      "f1_score :  0.9488331461375265\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58970 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9916954636573792 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917265176773071 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917886257171631 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917420744895935 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918041229248047 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917730689048767 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917420744895935 tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916644096374512 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909814596176147 tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9917420796920354\n",
      "Precision :  0.9676881255405906\n",
      "Recall :  0.9994257274119449\n",
      "f1_score :  0.9833008977336932\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9606661796569824 tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9684739708900452 tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9773682951927185 tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9793862104415894 tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9827390313148499 tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9816679954528809 tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9816369414329529 tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9835461974143982 tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.982133686542511 tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9802554988125359\n",
      "Precision :  0.9332651341918402\n",
      "Recall :  0.9895992853496682\n",
      "f1_score :  0.9606069990709198\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58979 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9892584681510925 tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910901188850403 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914626479148865 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913850426673889 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899724721908569 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9892584681510925 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911056160926819 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897396564483643 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909038543701172 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9913850643403753\n",
      "Precision :  0.967759935519871\n",
      "Recall :  0.9977627205318332\n",
      "f1_score :  0.9825323387782079\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9641276597976685 tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9688930511474609 tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9662387371063232 tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9684274196624756 tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9701504111289978 tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9689706563949585 tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9678685665130615 tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9685360789299011 tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9713921546936035 tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9765766884497773\n",
      "Precision :  0.9212612505215474\n",
      "Recall :  0.9879826131424189\n",
      "f1_score :  0.9534560932728788\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58964 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9912763833999634 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913694858551025 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9914160966873169 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915247559547424 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914937019348145 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915247559547424 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991540253162384 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914781451225281 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915247559547424 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9915402884063145\n",
      "Precision :  0.9660826032540676\n",
      "Recall :  0.9998057120652808\n",
      "f1_score :  0.9826549123197861\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9584309458732605 tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.971314549446106 tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9755366444587708 tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9760488867759705 tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9751641154289246 tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9758471250534058 tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9769647121429443 tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9776942729949951 tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9763748645782471 tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9788119149993015\n",
      "Precision :  0.9325221238938053\n",
      "Recall :  0.9827083738099864\n",
      "f1_score :  0.9569577145019392\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64424\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58800 nodes and 64424 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1.edata['h'] after reshape :  64424\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128848 nodes and 64424 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1_ab.edata['h'] after reshape :  64424\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9894604682922363 tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9898951053619385 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916025400161743 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920526742935181 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911213517189026 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991804301738739 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921613335609436 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916956424713135 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918509125709534 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9905625232832485\n",
      "Precision :  0.9633055623621661\n",
      "Recall :  0.9994279903393924\n",
      "f1_score :  0.9810343751949592\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9608376026153564 tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9647026062011719 tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9665963053703308 tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9689246416091919 tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9731621742248535 tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9683347940444946 tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9695299863815308 tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.971159815788269 tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9734416007995605 tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9724015894697629\n",
      "Precision :  0.9044279587342066\n",
      "Recall :  0.9918011948646244\n",
      "f1_score :  0.9461016127076513\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138051\n",
      "\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.9905687028706782\n",
      "Precision :  0.9648985507246377\n",
      "Recall :  0.9972738166566807\n",
      "f1_score :  0.980819092516205\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.970902057935111\n",
      "Precision :  0.900379066786659\n",
      "Recall :  0.9890952666267225\n",
      "f1_score :  0.9426544276149551\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset1.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460165\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset1.csv': [[0, 0.7577868807927591], [1, 0.24221311920724087]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58743 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9920679926872253 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918972849845886 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918972849845886 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922232627868652 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919438362121582 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920524954795837 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916178584098816 tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923784732818604 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991571307182312 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.992021483010726\n",
      "Precision :  0.9693550376125293\n",
      "Recall :  0.9989198119201932\n",
      "f1_score :  0.9839153836525222\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9618614315986633 tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9622960686683655 tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9600142240524292 tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9639258980751038 tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9629790186882019 tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9635844230651855 tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9681945443153381 tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.961023211479187 tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9664404988288879 tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9698554863946106\n",
      "Precision :  0.9006737918215614\n",
      "Recall :  0.9852586097344008\n",
      "f1_score :  0.9410693694240457\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58991 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9913384914398193 tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913074374198914 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910590648651123 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9901121854782104 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911832213401794 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914626479148865 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915868043899536 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914160966873169 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909814596176147 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9915868556260963\n",
      "Precision :  0.9675101530771634\n",
      "Recall :  0.998581285870897\n",
      "f1_score :  0.9828002030972328\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9717646837234497 tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9726805090904236 tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9713456034660339 tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9685360789299011 tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9722303748130798 tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9730064868927002 tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.975365936756134 tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9707092046737671 tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9733635187149048 tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9741707154277199\n",
      "Precision :  0.9118714668253496\n",
      "Recall :  0.9881988779260978\n",
      "f1_score :  0.9485021044813073\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59127 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.991291880607605 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917110204696655 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916799664497375 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912763833999634 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918351769447327 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916799664497375 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918196797370911 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917886257171631 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918351769447327 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9911832730546544\n",
      "Precision :  0.9661175007771216\n",
      "Recall :  0.9985221358349933\n",
      "f1_score :  0.9820525783619819\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9732393622398376 tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9740620255470276 tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9686757326126099 tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9747294783592224 tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9777563810348511 tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9748846888542175 tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9762351512908936 tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9771509766578674 tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9783772826194763 tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9785169892740171\n",
      "Precision :  0.9288609279535418\n",
      "Recall :  0.9866349675512434\n",
      "f1_score :  0.9568766747678694\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59025 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9921611547470093 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922543168067932 tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923163652420044 tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923474192619324 tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992362916469574 tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992362916469574 tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992362916469574 tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923784732818604 tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923784732818604 tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9920835726371017\n",
      "Precision :  0.9687402313222883\n",
      "Recall :  0.9993550467591099\n",
      "f1_score :  0.9838095238095238\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9813109636306763 tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9821181297302246 tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9822733998298645 tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9831271171569824 tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9827235341072083 tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9825372695922852 tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9827856421470642 tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9829563498497009 tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9833444356918335 tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9835462490104466\n",
      "Precision :  0.9454209065679926\n",
      "Recall :  0.9887133182844243\n",
      "f1_score :  0.9665825977301387\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58868 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9926268458366394 tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9926578998565674 tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992642343044281 tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992642343044281 tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992642343044281 tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9926578998565674 tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992673397064209 tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992642343044281 tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992642343044281 tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9926734240876706\n",
      "Precision :  0.9712924580832766\n",
      "Recall :  0.999490672948367\n",
      "f1_score :  0.9851898336994037\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9746984243392944 tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9756763577461243 tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9758936762809753 tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.975878119468689 tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9759712815284729 tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9771044254302979 tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.977570116519928 tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9774614572525024 tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9769181609153748 tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.97833072039489\n",
      "Precision :  0.9274236903410787\n",
      "Recall :  0.9884764754568027\n",
      "f1_score :  0.9569773175542408\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9919087287214777\n",
      "Precision :  0.9690592981743245\n",
      "Recall :  0.9984747891620313\n",
      "f1_score :  0.9835471564714027\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9781383556682361\n",
      "Precision :  0.9267676767676768\n",
      "Recall :  0.9877983132962498\n",
      "f1_score :  0.9563102579692523\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset0.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460165\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset0.csv': [[0, 0.7576130301087654], [1, 0.24238696989123468]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59007 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9926578998565674 tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9926888942718506 tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9927199482917786 tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9927199482917786 tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9927199482917786 tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9927664995193481 tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9927355051040649 tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9927510023117065 tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9927355051040649 tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9927510361206402\n",
      "Precision :  0.9716149068322981\n",
      "Recall :  0.9993611448284674\n",
      "f1_score :  0.98529272824615\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9819474220275879 tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.982102632522583 tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9828011393547058 tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9834375381469727 tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.983235776424408 tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9836393594741821 tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9833909869194031 tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9832668304443359 tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9837324619293213 tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9838101299225432\n",
      "Precision :  0.9443430656934306\n",
      "Recall :  0.9918226538043825\n",
      "f1_score :  0.9675007010874644\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58741 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9907486438751221 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920990467071533 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921301007270813 tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921456575393677 tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921456575393677 tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9921146035194397 tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922232627868652 tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9922698140144348 tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.992223274296447\n",
      "Precision :  0.969320987654321\n",
      "Recall :  0.9997453364741835\n",
      "f1_score :  0.9842981164007898\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9705539345741272 tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9760023355484009 tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.976219654083252 tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9759867787361145 tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9764524698257446 tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9770112633705139 tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9771820306777954 tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.977476954460144 tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9799760580062866 tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9787032581531441\n",
      "Precision :  0.9278338208082134\n",
      "Recall :  0.989622461322977\n",
      "f1_score :  0.9577325939617991\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58760 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9889479875564575 tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9887307286262512 tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895844459533691 tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895223379135132 tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.989211916923523 tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895844459533691 tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893050193786621 tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9895378947257996 tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902673959732056 tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9897552116480139\n",
      "Precision :  0.9613030846263213\n",
      "Recall :  0.9978184151427655\n",
      "f1_score :  0.9792204521125875\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9604178071022034 tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9619700908660889 tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9624667763710022 tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9659438133239746 tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9615509510040283 tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9625443816184998 tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9618614315986633 tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9602315425872803 tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9643760323524475 tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9626220449218447\n",
      "Precision :  0.8735189069675152\n",
      "Recall :  0.9886429258902791\n",
      "f1_score :  0.9275222730556225\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58969 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.990531325340271 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9897707104682922 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909193515777588 tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9893050193786621 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909349083900452 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991260826587677 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9896775484085083 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909504055976868 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9911211729049683 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9896465548018565\n",
      "Precision :  0.959248036608744\n",
      "Recall :  0.9994845360824742\n",
      "f1_score :  0.9789530150515919\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9676978588104248 tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9675736427307129 tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9718733429908752 tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9731927514076233 tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9728047251701355 tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9749312996864319 tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9717646837234497 tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9727891683578491 tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9743414521217346 tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9757384784937057\n",
      "Precision :  0.9140805791253783\n",
      "Recall :  0.992590206185567\n",
      "f1_score :  0.9517190251135206\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58765 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.991291880607605 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991260826587677 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991322934627533 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912763833999634 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903605580329895 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918196797370911 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908882975578308 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912142753601074 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9901432394981384 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9913540195271875\n",
      "Precision :  0.966183574879227\n",
      "Recall :  0.9992953686503107\n",
      "f1_score :  0.9824605598765626\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9763748645782471 tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9767318964004517 tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9763438105583191 tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9770112633705139 tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9771199226379395 tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9783617258071899 tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9762351512908936 tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9770888686180115 tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9779426455497742 tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9825683373950297\n",
      "Precision :  0.9397232001942455\n",
      "Recall :  0.9916725385945807\n",
      "f1_score :  0.9649992208196977\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9920536037667512\n",
      "Precision :  0.9709554436716045\n",
      "Recall :  0.9970414201183432\n",
      "f1_score :  0.9838255459062559\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9815501629844259\n",
      "Precision :  0.9371835845801397\n",
      "Recall :  0.990257605642221\n",
      "f1_score :  0.9629898719830279\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset2.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460167\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset2.csv': [[0, 0.7570208207020495], [1, 0.24297917929795051]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59063 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9925026297569275 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925336837768555 tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925957918167114 tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992642343044281 tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925647377967834 tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9926268458366394 tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9926578998565674 tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9925957918167114 tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9926888942718506 tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9926268568678888\n",
      "Precision :  0.9707165109034268\n",
      "Recall :  0.9996791786974655\n",
      "f1_score :  0.984984984984985\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9822423458099365 tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9828321933746338 tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9830805659294128 tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9830650091171265 tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.982925295829773 tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9831271171569824 tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9832512736320496 tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9832823276519775 tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9834375381469727 tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9835928162302283\n",
      "Precision :  0.9417954020192191\n",
      "Recall :  0.9935835739493102\n",
      "f1_score :  0.9669965966216005\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58979 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9901742935180664 tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913074374198914 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915868043899536 tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918041229248047 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919127821922302 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919593334197998 tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918662309646606 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919593334197998 tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920214414596558 tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9919438709777564\n",
      "Precision :  0.9684743043987908\n",
      "Recall :  0.9994906405195467\n",
      "f1_score :  0.983738054206486\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9751330614089966 tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9787653088569641 tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9804572463035583 tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.981000542640686 tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9817922115325928 tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9816990494728088 tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9820871353149414 tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9824286103248596 tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9829719066619873 tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9831426664390047\n",
      "Precision :  0.9458404488899732\n",
      "Recall :  0.98739335285878\n",
      "f1_score :  0.9661703320665379\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 59268 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9883891940116882 tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9899259209632874 tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909659028053284 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906709790229797 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9900500774383545 tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910590648651123 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906244277954102 tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905623197555542 tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910435676574707 tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.990686556043649\n",
      "Precision :  0.9644685707235827\n",
      "Recall :  0.9984673350788684\n",
      "f1_score :  0.9811735174144963\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9628859162330627 tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9657264947891235 tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9665491580963135 tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9671856164932251 tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9670459032058716 tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9679772257804871 tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9684274196624756 tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9689241051673889 tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.969110369682312 tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9692966797572296\n",
      "Precision :  0.8949252352635529\n",
      "Recall :  0.9899099559358835\n",
      "f1_score :  0.9400242571255305\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64423\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58811 nodes and 64423 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1.edata['h'] after reshape :  64423\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128846 nodes and 64423 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64423\n",
      "G1_ab.edata['h'] after reshape :  64423\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9913850426673889 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912142753601074 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991571307182312 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917575716972351 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916954636573792 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917575716972351 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9917886257171631 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919903874397278 tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9918351769447327 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9916179004392841\n",
      "Precision :  0.9669385737238907\n",
      "Recall :  0.9996805520061334\n",
      "f1_score :  0.9830370044606396\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9813886284828186 tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.98238205909729 tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9831271171569824 tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9834065437316895 tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9837014675140381 tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9836704134941101 tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9839187264442444 tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9839187264442444 tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9838256239891052 tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9837790851093553\n",
      "Precision :  0.9449284191288456\n",
      "Recall :  0.990991566572962\n",
      "f1_score :  0.967411981164437\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64424\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58815 nodes and 64424 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1.edata['h'] after reshape :  64424\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128848 nodes and 64424 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64424\n",
      "G1_ab.edata['h'] after reshape :  64424\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9900503158569336 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9906401634216309 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902831315994263 tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9903297424316406 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905780553817749 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909661412239075 tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904693961143494 tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990826427936554 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914628267288208 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9919129516950205\n",
      "Precision :  0.9684484038604306\n",
      "Recall :  0.9992977976380466\n",
      "f1_score :  0.9836312796506331\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9669067859649658 tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9685366153717041 tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9684590101242065 tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9696696996688843 tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9718583226203918 tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9725257754325867 tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9733484387397766 tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9731621742248535 tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.974155604839325 tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9756767664224513\n",
      "Precision :  0.9159192825112108\n",
      "Recall :  0.9909352058729652\n",
      "f1_score :  0.9519516757121393\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138051\n",
      "\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.9907715264648572\n",
      "Precision :  0.9679524361948956\n",
      "Recall :  0.9949618411638445\n",
      "f1_score :  0.9812713160061154\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138051\n",
      "Metrics : \n",
      "Accuracy :  0.9751396223134928\n",
      "Precision :  0.9150173658966867\n",
      "Recall :  0.9895957548294777\n",
      "f1_score :  0.9508464380854171\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "CIC-IDS-2017-Dataset3.csv ++++++++++++++++++++++++++++++++++++++++++++++\n",
      "nb total instances in the file :  460164\n",
      "++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\n",
      "\n",
      "labels freq after changing labels to binary\n",
      "{'CIC-IDS-2017-Dataset3.csv': [[0, 0.757532097252284], [1, 0.24246790274771604]]}\n",
      "+++++++++++++++++ Batch 1 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58914 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128844 nodes and 64422 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1_ab.edata['h'] after reshape :  64422\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9906554222106934 tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908571839332581 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905312061309814 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9909968972206116 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990577757358551 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9906864166259766 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913228750228882 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9908416867256165 tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.990639865398407 tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9914625438514794\n",
      "Precision :  0.966786689843556\n",
      "Recall :  0.9990377213240954\n",
      "f1_score :  0.9826476527006561\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9620782136917114 tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9620471596717834 tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9622955322265625 tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9630716443061829 tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9631026983261108 tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9645463228225708 tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9685822129249573 tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.964980959892273 tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9665021300315857 tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9673093042749371\n",
      "Precision :  0.8880382224268939\n",
      "Recall :  0.989671542211958\n",
      "f1_score :  0.9361043689320389\n",
      "+++++++++++++++++ Batch 2 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58888 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128844 nodes and 64422 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1_ab.edata['h'] after reshape :  64422\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9922852516174316 tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9923939108848572 tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924249649047852 tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924560189247131 tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9919127225875854 tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924560189247131 tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924715757369995 tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924870729446411 tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9924870729446411 tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9924870385893018\n",
      "Precision :  0.9704344578613221\n",
      "Recall :  0.999423076923077\n",
      "f1_score :  0.9847154676940567\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9829406142234802 tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9836236238479614 tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9838719964027405 tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.983732283115387 tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9836546778678894 tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9842755794525146 tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9843687415122986 tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9846481084823608 tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9844463467597961 tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9842445127440936\n",
      "Precision :  0.9467622373338235\n",
      "Recall :  0.9906410256410256\n",
      "f1_score :  0.9682047426620304\n",
      "+++++++++++++++++ Batch 3 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58797 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128844 nodes and 64422 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1_ab.edata['h'] after reshape :  64422\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9902673363685608 tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9912763237953186 tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc: 0.9903138875961304 tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9902052283287048 tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914315342903137 tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9905933141708374 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904536008834839 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9913073778152466 tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9904846549034119 tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9909192511874826\n",
      "Precision :  0.964287913305831\n",
      "Recall :  0.9996808374824461\n",
      "f1_score :  0.9816654652584074\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9601068496704102 tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9613641500473022 tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9618298411369324 tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9629319310188293 tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9624507427215576 tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9618608951568604 tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9633200168609619 tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9635838866233826 tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9621092677116394 tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9635217782745025\n",
      "Precision :  0.8773093052249802\n",
      "Recall :  0.9881909868505043\n",
      "f1_score :  0.929454851104707\n",
      "+++++++++++++++++ Batch 4 ++++++++++++++++\n",
      "nb Train instances :  64422\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58830 nodes and 64422 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1.edata['h'] after reshape :  64422\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128844 nodes and 64422 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64422\n",
      "G1_ab.edata['h'] after reshape :  64422\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9917575120925903 tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920058846473694 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992021381855011 tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920369386672974 tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920369386672974 tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992052435874939 tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992052435874939 tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920679330825806 tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.992052435874939 tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9920213591630188\n",
      "Precision :  0.9686592075168449\n",
      "Recall :  0.9995534859985967\n",
      "f1_score :  0.9838638789476989\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.977352499961853 tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9776939749717712 tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9781131148338318 tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9783614873886108 tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.978423535823822 tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9784390926361084 tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.978485643863678 tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9788116216659546 tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9787184596061707 tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9789357672844681\n",
      "Precision :  0.9278713995458349\n",
      "Recall :  0.9904318428270715\n",
      "f1_score :  0.9581314985653019\n",
      "+++++++++++++++++ Batch 5 ++++++++++++++++\n",
      "nb Train instances :  64426\n",
      "initial nx multigraph G1 :  MultiDiGraph with 58802 nodes and 64426 edges\n",
      "G1.edata['h'] after converting it to a dgl graph :  64426\n",
      "G1.edata['h'] after reshape :  64426\n",
      "\n",
      "initial nx multigraph G1_ab :  MultiDiGraph with 128852 nodes and 64426 edges\n",
      "G1_ab.edata['h'] after converting it to a dgl graph :  64426\n",
      "G1_ab.edata['h'] after reshape :  64426\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9912923574447632 tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915096759796143 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9910750389099121 tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916648864746094 tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.991649329662323 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9914941191673279 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9920839667320251 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9916648864746094 tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9915406703948975 tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9918976810604414\n",
      "Precision :  0.9678542560467575\n",
      "Recall :  0.9996788902446856\n",
      "f1_score :  0.9835091931509445\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Training acc: 0.9678546190261841 tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9691429138183594 tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9704933166503906 tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9696395993232727 tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9713314771652222 tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.972697377204895 tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9724024534225464 tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9764536023139954 tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training acc: 0.9748859405517578 tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train metrics :\n",
      "Accuracy :  0.9760500419085463\n",
      "Precision :  0.9162611275964392\n",
      "Recall :  0.9915227024597008\n",
      "f1_score :  0.9524073902717374\n",
      "++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\n",
      "nb Test instances :  138050\n",
      "\n",
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9904744657732706\n",
      "Precision :  0.9667073029141995\n",
      "Recall :  0.9949810294864517\n",
      "f1_score :  0.9806404122193596\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9753495110467222\n",
      "Precision :  0.9160210293303819\n",
      "Recall :  0.9890060645893706\n",
      "f1_score :  0.9511154525735136\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# # --------------------------------------------------- MAIN -----------------------------------------------------------\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "# Model *******************************************************************************************\n",
    "# G1.ndata['h'].shape[2] = sizeh = 76 dans ANIDS\n",
    "# model1 = Model(G1.ndata['h'].shape[2], size_embedding, G1.ndata['h'].shape[2], F.relu, 0.2).cuda()\n",
    "model1 = Model(76, size_embedding, 76, F.relu, 0.2).cuda()\n",
    "opt = th.optim.Adam(model1.parameters())\n",
    "\n",
    "model1_ab = Model(76, size_embedding, 76, F.relu, 0.2).cuda()\n",
    "opt_ab = th.optim.Adam(model1_ab.parameters())\n",
    "\n",
    "\n",
    "\n",
    "path, dirs, files = next(os.walk(\"/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/input/Dataset/GlobalDataset/Splitted/\"))\n",
    "file_count = len(files)\n",
    "\n",
    "\n",
    "for nb_files in range(file_count):\n",
    "    data1 = pd.read_csv(f'{path}{files[nb_files]}', encoding=\"ISO-88591\", dtype = str)\n",
    "\n",
    "    print(f'{files[nb_files]} ++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print(\"nb total instances in the file : \", len(data1.values))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++ Train ++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    # Delete two columns (U and V in the excel)\n",
    "    cols = list(set(list(data1.columns )) - set(list(['Flow Bytes/s',' Flow Packets/s'])) )\n",
    "    data1 = data1[cols]\n",
    "\n",
    "    # Mise en forme des noeuds\n",
    "    data1[' Source IP'] = data1[' Source IP'].apply(str)\n",
    "    data1[' Source Port'] = data1[' Source Port'].apply(str)\n",
    "    data1[' Destination IP'] = data1[' Destination IP'].apply(str)\n",
    "    data1[' Destination Port'] = data1[' Destination Port'].apply(str)\n",
    "    data1[' Source IP'] = data1[' Source IP'] + ':' + data1[' Source Port']\n",
    "    data1[' Destination IP'] = data1[' Destination IP'] + ':' + data1[' Destination Port']\n",
    "\n",
    "    data1.drop(columns=['Flow ID',' Source Port',' Destination Port',' Timestamp'], inplace=True)\n",
    "\n",
    "    # -------------------- ????????????????????????????????????????? --------------------\n",
    "    # simply do : nom = list(data1[' Label'].unique())\n",
    "    nom = []\n",
    "    nom = nom + [data1[' Label'].unique()[0]]\n",
    "    for i in range(1, len(data1[' Label'].unique())):\n",
    "        nom = nom + [data1[' Label'].unique()[i]]\n",
    "    \n",
    "    nom.insert(0, nom.pop(nom.index('BENIGN')))\n",
    "\n",
    "    # Naming the two classes BENIGN {0} / Any Intrusion {1}\n",
    "    data1[' Label'].replace(nom[0], 0,inplace = True)\n",
    "    for i in range(1,len(data1[' Label'].unique())):\n",
    "        data1[' Label'].replace(nom[i], 1,inplace = True)\n",
    "    \n",
    "    ##################### LABELS FREQ #######################################\n",
    "    print()\n",
    "    print(\"labels freq after changing labels to binary\")\n",
    "    counts = list(data1[' Label'].value_counts().to_dict().items())\n",
    "    for j, x in enumerate(counts):\n",
    "        x = list(x)\n",
    "        x[1] = x[1] / len(data1)\n",
    "        counts[j] = x\n",
    "    print({f'{files[nb_files]}' : counts})\n",
    "    ##############################################################################\n",
    "\n",
    "    data1.rename(columns={\" Label\": \"label\"},inplace = True)\n",
    "    label1 = data1.label\n",
    "    data1.drop(columns=['label'],inplace = True)\n",
    "\n",
    "    # ******** At this step data1 contains only the data without label column\n",
    "    # ******** The label column is stored in the label variale \n",
    "\n",
    "    # split train and test\n",
    "    data1 =  pd.concat([data1, label1], axis=1) # ??????? WHY ?\n",
    "        \n",
    "\n",
    "    # Split\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(data1, label1, test_size=0.3, random_state=123, stratify= label1)\n",
    "\n",
    "    \n",
    "    # Create mini batches on the Train set\n",
    "    X1_train = shuffle(X1_train)\n",
    "    a = b = mean_macro_f1 = 0\n",
    "    for batch in range(1, nb_batch + 1):\n",
    "        print(f\"+++++++++++++++++ Batch {batch} ++++++++++++++++\")\n",
    "        a = b\n",
    "        b = int(len(X1_train) / nb_batch) * batch\n",
    "        if batch == nb_batch :\n",
    "            b = len(X1_train)\n",
    "        # The batch :\n",
    "        X1_train_batched = X1_train.iloc[a:b]\n",
    "        # y1_train_batched = y1_train.iloc[a:b]\n",
    "        y1_train_batched = X1_train_batched['label']        \n",
    "        \n",
    "        print(\"nb Train instances : \", len(X1_train_batched.values))\n",
    "\n",
    "        # for non numerical attributes (categorical data)\n",
    "        # Since we have a binary classification, the category values willl be replaced with the posterior probability (p(target = Ti | category = Cj))\n",
    "        # TargetEncoding is also called MeanEncoding, cuz it simply replace each value with (target_i_count_on_category_j) / (total_occurences_of_category_j)\n",
    "        encoder1 = ce.TargetEncoder(cols=[' Protocol',  'Fwd PSH Flags', ' Fwd URG Flags', ' Bwd PSH Flags', ' Bwd URG Flags'])\n",
    "        encoder1.fit(X1_train_batched, y1_train_batched)\n",
    "        X1_train_batched = encoder1.transform(X1_train_batched)\n",
    "\n",
    "        # scaler (normalization)\n",
    "        scaler1 = StandardScaler()\n",
    "\n",
    "        # Manipulate flow content (all columns except : label, Source IP & Destination IP)\n",
    "        cols_to_norm1 = list(set(list(X1_train_batched.iloc[:, :].columns )) - set(list(['label', ' Source IP', ' Destination IP'])) )\n",
    "        X1_train_batched[cols_to_norm1] = scaler1.fit_transform(X1_train_batched[cols_to_norm1])\n",
    "\n",
    "        ## Create the h attribute that will contain the content of our flows\n",
    "        X1_train_batched['h'] = X1_train_batched[ cols_to_norm1 ].values.tolist()\n",
    "        # size of the list containig the content of our flows\n",
    "        sizeh = len(cols_to_norm1)\n",
    "\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Before training the data :\n",
    "        # We need to delete all the attributes (cols_to_norm1) to have the {Source IP, Destination IP, label, h} representation\n",
    "        X1_train_batched.drop(columns = cols_to_norm1, inplace = True)\n",
    "        \n",
    "        # Edge index\n",
    "        X1_train_batched['Edge_indx'] = list(range(len(X1_train_batched.values)))\n",
    "\n",
    "        # Then we need to Swap {label, h} Columns to have the {Source IP, Destination IP, h, label} representation\n",
    "        columns_titles = [' Source IP', ' Destination IP', 'h', 'label', 'Edge_indx']\n",
    "        X1_train_batched = X1_train_batched.reindex(columns=columns_titles)\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # ------------------------------------------- Creating the Graph Representation -------------------------------------------------------------\n",
    "        # Create our Multigraph\n",
    "        G1 = nx.from_pandas_edgelist(X1_train_batched, \" Source IP\", \" Destination IP\", ['h','label', 'Edge_indx'], create_using=nx.MultiDiGraph())\n",
    "        print(\"initial nx multigraph G1 : \", G1)\n",
    "\n",
    "        G1 = from_networkx(G1,edge_attrs=['h','label', 'Edge_indx'] )\n",
    "        print(\"G1.edata['h'] after converting it to a dgl graph : \", len(G1.edata['h']))\n",
    "\n",
    "        # nodes data // G1.edata['h'].shape[1] : sizeh = number of attributes in a flow\n",
    "        G1.ndata['h'] = th.ones(G1.num_nodes(), G1.edata['h'].shape[1])\n",
    "        # edges data // we create a tensor bool array that will represent the train mask\n",
    "        G1.edata['train_mask'] = th.ones(len(G1.edata['h']), dtype=th.bool)\n",
    "\n",
    "        # Reshape both tensor lists to a single value in each element for both axis\n",
    "        G1.ndata['h'] = th.reshape(G1.ndata['h'], (G1.ndata['h'].shape[0], 1, G1.ndata['h'].shape[1]))\n",
    "        G1.edata['h'] = th.reshape(G1.edata['h'], (G1.edata['h'].shape[0], 1, G1.edata['h'].shape[1]))\n",
    "        print(\"G1.edata['h'] after reshape : \", len(G1.edata['h']))\n",
    "        # ------------------------------------------- --------------------------------- -------------------------------------------------------------\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # Ablation *************************************************************************\n",
    "        X1_train_ab = copy.deepcopy(X1_train_batched)\n",
    "\n",
    "        # print(\"data IP Addr before changing them : \")\n",
    "        # print(X1_train_ab[[' Source IP', ' Destination IP']])\n",
    "\n",
    "        X1_train_ab = X1_train_ab.drop(' Source IP', axis=1)\n",
    "        X1_train_ab = X1_train_ab.drop(' Destination IP', axis=1)\n",
    "        \n",
    "        X1_train_ab[' Source IP'] = list(range(len(X1_train_ab.values)))\n",
    "        X1_train_ab[' Destination IP'] = list(range(len(X1_train_ab.values), 2 * len(X1_train_ab.values)))\n",
    "\n",
    "        print()\n",
    "        # print(\"data IP Addr after changing them : \")\n",
    "        # print(X1_train_ab[[' Source IP', ' Destination IP']])\n",
    "        \n",
    "        # ------------------------------------------- Creating the Graph Representation -------------------------------------------------------------\n",
    "        # Create our Ablation Multigraph\n",
    "        G1_ab = nx.from_pandas_edgelist(X1_train_ab, \" Source IP\", \" Destination IP\", ['h','label', 'Edge_indx'], create_using=nx.MultiDiGraph())\n",
    "        print(\"initial nx multigraph G1_ab : \", G1_ab)\n",
    "\n",
    "        G1_ab = from_networkx(G1_ab, edge_attrs=['h','label', 'Edge_indx'] )\n",
    "        print(\"G1_ab.edata['h'] after converting it to a dgl graph : \", len(G1_ab.edata['h']))\n",
    "\n",
    "        # nodes data // G1_ab.edata['h'].shape[1] : sizeh = number of attributes in a flow\n",
    "        G1_ab.ndata['h'] = th.ones(G1_ab.num_nodes(), G1_ab.edata['h'].shape[1])\n",
    "        # edges data // we create a tensor bool array that will represent the train mask\n",
    "        G1_ab.edata['train_mask'] = th.ones(len(G1_ab.edata['h']), dtype=th.bool)\n",
    "\n",
    "        # Reshape both tensor lists to a single value in each element for both axis\n",
    "        G1_ab.ndata['h'] = th.reshape(G1_ab.ndata['h'], (G1_ab.ndata['h'].shape[0], 1, G1_ab.ndata['h'].shape[1]))\n",
    "        G1_ab.edata['h'] = th.reshape(G1_ab.edata['h'], (G1_ab.edata['h'].shape[0], 1, G1_ab.edata['h'].shape[1]))\n",
    "        print(\"G1_ab.edata['h'] after reshape : \", len(G1_ab.edata['h']))\n",
    "        # ------------------------------------------- --------------------------------- -------------------------------------------------------------\n",
    "        \n",
    "        # ***********************************************************************************\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------- Model -----------------------------------------------------------------------------------------\n",
    "        ## use of model\n",
    "        from sklearn.utils import class_weight\n",
    "        class_weights1 = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(G1.edata['label'].cpu().numpy()),\n",
    "                                                        y = G1.edata['label'].cpu().numpy())\n",
    "        class_weights1 = th.FloatTensor(class_weights1).cuda()\n",
    "        criterion1 = nn.CrossEntropyLoss(weight = class_weights1)\n",
    "        G1 = G1.to('cuda:0')\n",
    "\n",
    "        node_features1 = G1.ndata['h']\n",
    "        edge_features1 = G1.edata['h']\n",
    "\n",
    "        edge_label1 = G1.edata['label']\n",
    "        train_mask1 = G1.edata['train_mask']\n",
    "\n",
    "\n",
    "        # to print\n",
    "        pr = True\n",
    "        # True if you want to print the embedding vectors\n",
    "        # the name of the file where the vectors are printed\n",
    "        filename = './models/M1_weights.txt'\n",
    "        \n",
    "        print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "\n",
    "        for epoch in range(1, 1000):\n",
    "            pred = model1(G1, node_features1, edge_features1).cuda()\n",
    "            loss = criterion1(pred[train_mask1], edge_label1[train_mask1])\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training acc:', compute_accuracy(pred[train_mask1], edge_label1[train_mask1]), loss)\n",
    "\n",
    "        pred1 = model1(G1, node_features1, edge_features1).cuda()\n",
    "        pred1 = pred1.argmax(1)\n",
    "        pred1 = th.Tensor.cpu(pred1).detach().numpy()\n",
    "        edge_label1 = th.Tensor.cpu(edge_label1).detach().numpy()\n",
    "\n",
    "        print('Train metrics :')\n",
    "        print(\"Accuracy : \", sklearn.metrics.accuracy_score(edge_label1, pred1))\n",
    "        print(\"Precision : \", sklearn.metrics.precision_score(edge_label1, pred1, labels = [0,1]))\n",
    "        print(\"Recall : \", sklearn.metrics.recall_score(edge_label1, pred1, labels = [0,1]))\n",
    "        print(\"f1_score : \", sklearn.metrics.f1_score(edge_label1, pred1, labels=[0,1]))\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # ------------------------------------------- AB Model -----------------------------------------------------------------------------------------\n",
    "        ## use of model\n",
    "        from sklearn.utils import class_weight\n",
    "        class_weights1_ab = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                        classes = np.unique(G1_ab.edata['label'].cpu().numpy()),\n",
    "                                                        y = G1_ab.edata['label'].cpu().numpy())\n",
    "        class_weights1_ab = th.FloatTensor(class_weights1_ab).cuda()\n",
    "        criterion1_ab = nn.CrossEntropyLoss(weight = class_weights1_ab)\n",
    "        G1_ab = G1_ab.to('cuda:0')\n",
    "\n",
    "        node_features1_ab = G1_ab.ndata['h']\n",
    "        edge_features1_ab = G1_ab.edata['h']\n",
    "\n",
    "        edge_label1_ab = G1_ab.edata['label']\n",
    "        train_mask1_ab = G1_ab.edata['train_mask']\n",
    "        \n",
    "        print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "        \n",
    "        for epoch in range(1, 1000):\n",
    "            pred_ab = model1_ab(G1_ab, node_features1_ab, edge_features1_ab).cuda()\n",
    "            loss_ab = criterion1_ab(pred_ab[train_mask1], edge_label1_ab[train_mask1])\n",
    "            opt_ab.zero_grad()\n",
    "            loss_ab.backward()\n",
    "            opt_ab.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training acc:', compute_accuracy(pred_ab[train_mask1], edge_label1_ab[train_mask1]), loss_ab)\n",
    "\n",
    "        pred1_ab = model1_ab(G1_ab, node_features1_ab, edge_features1_ab).cuda()\n",
    "        pred1_ab = pred1_ab.argmax(1)\n",
    "        pred1_ab = th.Tensor.cpu(pred1_ab).detach().numpy()\n",
    "        edge_label1_ab = th.Tensor.cpu(edge_label1_ab).detach().numpy()\n",
    "\n",
    "        print('Train metrics :')\n",
    "        print(\"Accuracy : \", sklearn.metrics.accuracy_score(edge_label1_ab, pred1_ab))\n",
    "        print(\"Precision : \", sklearn.metrics.precision_score(edge_label1_ab, pred1_ab, labels = [0,1]))\n",
    "        print(\"Recall : \", sklearn.metrics.recall_score(edge_label1_ab, pred1_ab, labels = [0,1]))\n",
    "        print(\"f1_score : \", sklearn.metrics.f1_score(edge_label1_ab, pred1_ab, labels=[0,1]))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------ Test ---------------------------------------------------------------------\n",
    "    print(\"++++++++++++++++++++++++++++ Test ++++++++++++++++++++++++++++++++\")\n",
    "    print(\"nb Test instances : \", len(X1_test.values))\n",
    "    X1_test = encoder1.transform(X1_test)\n",
    "    X1_test[cols_to_norm1] = scaler1.transform(X1_test[cols_to_norm1])\n",
    "\n",
    "    # Save X1_test for XAI\n",
    "    X1_test.to_csv(f'/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/input/Dataset/XAI/X_test{nb_files}.csv', sep=',', index = False)\n",
    "\n",
    "    X1_test['h'] = X1_test[ cols_to_norm1 ].values.tolist()\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Before training the data :\n",
    "    # We need to delete all the attributes (cols_to_norm1) to have the {Source IP, Destination IP, label, h} representation\n",
    "    X1_test.drop(columns = cols_to_norm1, inplace = True)\n",
    "    \n",
    "    # Edge index\n",
    "    X1_test['Edge_indx'] = list(range(len(X1_test.values)))\n",
    "\n",
    "    # Then we need to Swap {label, h} Columns to have the {Source IP, Destination IP, h, label} representation\n",
    "    columns_titles = [' Source IP', ' Destination IP', 'h', 'label', 'Edge_indx']\n",
    "    X1_test = X1_test.reindex(columns=columns_titles)\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    # Graph Construction\n",
    "    G1_test = nx.from_pandas_edgelist(X1_test, \" Source IP\", \" Destination IP\", ['h','label', 'Edge_indx'],create_using=nx.MultiDiGraph())\n",
    "    # G1_test = G1_test.to_directed()\n",
    "    G1_test = from_networkx(G1_test,edge_attrs=['h','label', 'Edge_indx'] )\n",
    "    # actual1 = G1_test.edata.pop('label')\n",
    "    actual1 = G1_test.edata['label']\n",
    "    G1_test.ndata['feature'] = th.ones(G1_test.num_nodes(), G1.ndata['h'].shape[2])\n",
    "    G1_test.ndata['feature'] = th.reshape(G1_test.ndata['feature'], (G1_test.ndata['feature'].shape[0], 1, G1_test.ndata['feature'].shape[1]))\n",
    "    G1_test.edata['h'] = th.reshape(G1_test.edata['h'], (G1_test.edata['h'].shape[0], 1, G1_test.edata['h'].shape[1]))\n",
    "    G1_test = G1_test.to('cuda:0')\n",
    "    node_features_test1 = G1_test.ndata['feature']\n",
    "    edge_features_test1 = G1_test.edata['h']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Ablation *************************************************************************\n",
    "    X1_test_ab = copy.deepcopy(X1_test)\n",
    "\n",
    "    # print(\"data IP Addr before changing them : \")\n",
    "    # print(X1_test_ab[[' Source IP', ' Destination IP']])\n",
    "\n",
    "    X1_test_ab = X1_test_ab.drop(' Source IP', axis=1)\n",
    "    X1_test_ab = X1_test_ab.drop(' Destination IP', axis=1)\n",
    "\n",
    "    X1_test_ab[' Source IP'] = list(range(len(X1_test_ab.values)))\n",
    "    X1_test_ab[' Destination IP'] = list(range(len(X1_test_ab.values), 2 * len(X1_test_ab.values)))\n",
    "\n",
    "    print()\n",
    "    # print(\"data IP Addr after changing them : \")\n",
    "    # print(X1_test_ab[[' Source IP', ' Destination IP']])\n",
    "    \n",
    "    # Graph Construction\n",
    "    G1_test_ab = nx.from_pandas_edgelist(X1_test_ab, \" Source IP\", \" Destination IP\", ['h','label', 'Edge_indx'],create_using=nx.MultiDiGraph())\n",
    "    G1_test_ab = from_networkx(G1_test_ab, edge_attrs=['h','label', 'Edge_indx'] )\n",
    "    actual1_ab = G1_test_ab.edata['label']\n",
    "    G1_test_ab.ndata['feature'] = th.ones(G1_test_ab.num_nodes(), G1_ab.ndata['h'].shape[2])\n",
    "    G1_test_ab.ndata['feature'] = th.reshape(G1_test_ab.ndata['feature'], (G1_test_ab.ndata['feature'].shape[0], 1, G1_test_ab.ndata['feature'].shape[1]))\n",
    "    G1_test_ab.edata['h'] = th.reshape(G1_test_ab.edata['h'], (G1_test_ab.edata['h'].shape[0], 1, G1_test_ab.edata['h'].shape[1]))\n",
    "    G1_test_ab = G1_test_ab.to('cuda:0')\n",
    "    node_features_test1_ab = G1_test_ab.ndata['feature']\n",
    "    edge_features_test1_ab = G1_test_ab.edata['h']\n",
    "    \n",
    "    # ***********************************************************************************\n",
    "\n",
    "    print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "    print(\"nb instances : \", len(X1_test.values))\n",
    "\n",
    "    test_pred1 = model1(G1_test, node_features_test1, edge_features_test1).cuda()\n",
    "    test_pred1 = test_pred1.argmax(1)\n",
    "    test_pred1 = th.Tensor.cpu(test_pred1).detach().numpy()\n",
    "\n",
    "    print('Metrics : ')\n",
    "    print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1, test_pred1))\n",
    "    print(\"Precision : \", sklearn.metrics.precision_score(actual1, test_pred1, labels = [0,1]))\n",
    "    print(\"Recall : \", sklearn.metrics.recall_score(actual1, test_pred1, labels = [0,1]))\n",
    "    print(\"f1_score : \", sklearn.metrics.f1_score(actual1, test_pred1, labels = [0,1]))\n",
    "    \n",
    "    \n",
    "    print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "    print(\"nb instances : \", len(X1_test_ab.values))\n",
    "\n",
    "    test_pred1_ab = model1_ab(G1_test_ab, node_features_test1_ab, edge_features_test1_ab).cuda()\n",
    "    test_pred1_ab = test_pred1_ab.argmax(1)\n",
    "    test_pred1_ab = th.Tensor.cpu(test_pred1_ab).detach().numpy()\n",
    "\n",
    "    print('Metrics : ')\n",
    "    print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1_ab, test_pred1_ab))\n",
    "    print(\"Precision : \", sklearn.metrics.precision_score(actual1_ab, test_pred1_ab, labels = [0,1]))\n",
    "    print(\"Recall : \", sklearn.metrics.recall_score(actual1_ab, test_pred1_ab, labels = [0,1]))\n",
    "    print(\"f1_score : \", sklearn.metrics.f1_score(actual1_ab, test_pred1_ab, labels = [0,1]))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-stomach",
   "metadata": {},
   "source": [
    "### ACC + F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "closed-spotlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9911119159724737\n",
      "Precision :  0.9679029543212027\n",
      "Recall :  0.9963851462372658\n",
      "f1_score :  0.9819375542830224\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "nb instances :  138050\n",
      "Metrics : \n",
      "Accuracy :  0.9753350235421948\n",
      "Precision :  0.9160163816481266\n",
      "Recall :  0.9889463149403998\n",
      "f1_score :  0.9510853169757654\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "print(\"nb instances : \", len(X1_test.values))\n",
    "\n",
    "test_pred1 = model1(G1_test, node_features_test1, edge_features_test1).cuda()\n",
    "test_pred1 = test_pred1.argmax(1)\n",
    "test_pred1 = th.Tensor.cpu(test_pred1).detach().numpy()\n",
    "\n",
    "print('Metrics : ')\n",
    "print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1, test_pred1))\n",
    "print(\"Precision : \", sklearn.metrics.precision_score(actual1, test_pred1, labels = [0,1]))\n",
    "print(\"Recall : \", sklearn.metrics.recall_score(actual1, test_pred1, labels = [0,1]))\n",
    "print(\"f1_score : \", sklearn.metrics.f1_score(actual1, test_pred1, labels = [0,1]))\n",
    "\n",
    "\n",
    "print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "print(\"nb instances : \", len(X1_test_ab.values))\n",
    "\n",
    "test_pred1_ab = model1_ab(G1_test_ab, node_features_test1_ab, edge_features_test1_ab).cuda()\n",
    "test_pred1_ab = test_pred1_ab.argmax(1)\n",
    "test_pred1_ab = th.Tensor.cpu(test_pred1_ab).detach().numpy()\n",
    "\n",
    "print('Metrics : ')\n",
    "print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1_ab, test_pred1_ab))\n",
    "print(\"Precision : \", sklearn.metrics.precision_score(actual1_ab, test_pred1_ab, labels = [0,1]))\n",
    "print(\"Recall : \", sklearn.metrics.recall_score(actual1_ab, test_pred1_ab, labels = [0,1]))\n",
    "print(\"f1_score : \", sklearn.metrics.f1_score(actual1_ab, test_pred1_ab, labels = [0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-rates",
   "metadata": {},
   "source": [
    "### Save X_Test + Models + Graphs + Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "capital-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test.to_csv(f'/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Global_X_Test.csv', sep=',', index = False)\n",
    "\n",
    "# Models\n",
    "th.save(model1.state_dict(), '/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/GNN.pt')\n",
    "th.save(model1_ab.state_dict(), '/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/GNN_ab.pt')\n",
    "\n",
    "# Graphs\n",
    "from dgl.data.utils import save_graphs\n",
    "save_graphs(\"/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/G1_test.bin\", [G1_test])\n",
    "save_graphs(\"/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/G1_test_ab.bin\", [G1_test_ab])\n",
    "\n",
    "np.savetxt('/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/test_pred1.txt', test_pred1, fmt='%d')\n",
    "np.savetxt('/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/test_pred1_ab.txt', test_pred1_ab, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-sequence",
   "metadata": {},
   "source": [
    "### Load graphs and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affiliated-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0,  ..., 0, 0, 0])\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0])\n",
      "(array([     1,      9,     14, ..., 138035, 138036, 138040]),)\n",
      "[1 1 0 ... 0 0 0]\n",
      "[1 0 0 ... 0 1 0]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[ True False  True ...  True False  True]\n",
      "(array([     1,      9,     14, ..., 138038, 138040, 138048]),)\n"
     ]
    }
   ],
   "source": [
    "# loading Graphs and Predictions\n",
    "from dgl.data.utils import load_graphs\n",
    "import numpy as np\n",
    "\n",
    "Test_Graph = load_graphs(\"/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/G1_test.bin\")\n",
    "Test_Graph = Test_Graph[0][0]\n",
    "Test_Graph_ab = load_graphs(\"/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/G1_test_ab.bin\")\n",
    "Test_Graph_ab = Test_Graph_ab[0][0]\n",
    "\n",
    "Test_pred_Graph = np.loadtxt('/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/test_pred1.txt', dtype=int)\n",
    "Test_pred_Graph_ab = np.loadtxt('/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/test_pred1_ab.txt', dtype=int)\n",
    "\n",
    "\n",
    "actual1 = Test_Graph.edata['label']\n",
    "actual1_ab = Test_Graph_ab.edata['label']\n",
    "\n",
    "\n",
    "# Test the loading\n",
    "print(actual1)\n",
    "print(actual1_ab)\n",
    "aa = actual1 == actual1_ab\n",
    "aa = aa.numpy()\n",
    "print(np.where(aa == False))\n",
    "\n",
    "print(Test_pred_Graph)\n",
    "print(Test_pred_Graph_ab)\n",
    "aa = Test_pred_Graph == Test_pred_Graph_ab\n",
    "\n",
    "print(type(Test_pred_Graph))\n",
    "print(type(Test_pred_Graph_ab))\n",
    "print(type(aa))\n",
    "\n",
    "print(aa)\n",
    "print(np.where(aa == False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-reducing",
   "metadata": {},
   "source": [
    "### Models testing on the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "curious-complexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\n",
      "Metrics : \n",
      "Accuracy :  0.9911626222383194\n",
      "Precision :  0.9677737490935461\n",
      "Recall :  0.9967436441310907\n",
      "f1_score :  0.9820450933066464\n",
      "\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\n",
      "Metrics : \n",
      "Accuracy :  0.9749873234335386\n",
      "Precision :  0.9135441921974873\n",
      "Recall :  0.9905894302870971\n",
      "f1_score :  0.9505081053189812\n"
     ]
    }
   ],
   "source": [
    "model1_test = Model(76, size_embedding, 76, F.relu, 0.2).cuda()\n",
    "model1_test.load_state_dict(th.load('/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/GNN.pt'))\n",
    "model1_test.eval()\n",
    "\n",
    "model1_test_ab = Model(76, size_embedding, 76, F.relu, 0.2).cuda()\n",
    "model1_test_ab.load_state_dict(th.load('/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/GNN_ab.pt'))\n",
    "model1_test_ab.eval()\n",
    "\n",
    "\n",
    "print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ NORMAL \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "\n",
    "Test_Graph1 = Test_Graph.to('cuda:0')\n",
    "node_features_test1 = Test_Graph1.ndata['feature']\n",
    "edge_features_test1 = Test_Graph1.edata['h']\n",
    "\n",
    "test_pred1 = model1_test(Test_Graph1, node_features_test1, edge_features_test1).cuda()\n",
    "test_pred1 = test_pred1.argmax(1)\n",
    "test_pred1 = th.Tensor.cpu(test_pred1).detach().numpy()\n",
    "\n",
    "print('Metrics : ')\n",
    "print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1, test_pred1))\n",
    "print(\"Precision : \", sklearn.metrics.precision_score(actual1, test_pred1, labels = [0,1]))\n",
    "print(\"Recall : \", sklearn.metrics.recall_score(actual1, test_pred1, labels = [0,1]))\n",
    "print(\"f1_score : \", sklearn.metrics.f1_score(actual1, test_pred1, labels = [0,1]))\n",
    "\n",
    "\n",
    "print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ ABLATION \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "\n",
    "Test_Graph_ab1 = Test_Graph_ab.to('cuda:0')\n",
    "node_features_test1_ab = Test_Graph_ab1.ndata['feature']\n",
    "edge_features_test1_ab = Test_Graph_ab1.edata['h']\n",
    "\n",
    "test_pred1_ab = model1_test_ab(Test_Graph_ab1, node_features_test1_ab, edge_features_test1_ab).cuda()\n",
    "test_pred1_ab = test_pred1_ab.argmax(1)\n",
    "test_pred1_ab = th.Tensor.cpu(test_pred1_ab).detach().numpy()\n",
    "\n",
    "print('Metrics : ')\n",
    "print(\"Accuracy : \", sklearn.metrics.accuracy_score(actual1_ab, test_pred1_ab))\n",
    "print(\"Precision : \", sklearn.metrics.precision_score(actual1_ab, test_pred1_ab, labels = [0,1]))\n",
    "print(\"Recall : \", sklearn.metrics.recall_score(actual1_ab, test_pred1_ab, labels = [0,1]))\n",
    "print(\"f1_score : \", sklearn.metrics.f1_score(actual1_ab, test_pred1_ab, labels = [0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-stranger",
   "metadata": {},
   "source": [
    "### Extract the 3% of difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "enormous-treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([     0,  71207,     22,  ..., 138044, 138045, 138046])\n",
      "tensor([     0,      1,      2,  ..., 138047, 138048, 138049])\n"
     ]
    }
   ],
   "source": [
    "print(Test_Graph.edata['Edge_indx'])\n",
    "print(Test_Graph_ab.edata['Edge_indx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "respiratory-productivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3495   3825   4535 ... 137915 137984 137995]\n",
      "[   107    167    274 ... 138031 138038 138048]\n"
     ]
    }
   ],
   "source": [
    "actual1_np = actual1.numpy()\n",
    "aa = actual1_np == Test_pred_Graph\n",
    "wrong_preds = np.where(aa == False)[0]\n",
    "print(wrong_preds)\n",
    "\n",
    "actual1_ab_np = actual1_ab.numpy()\n",
    "bb = actual1_ab_np == Test_pred_Graph_ab\n",
    "wrong_preds_ab = np.where(bb == False)[0]\n",
    "print(wrong_preds_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "downtown-cycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    76,  87306,  79223,  ..., 137736, 137871, 137893])\n",
      "tensor([   107,    167,    274,  ..., 138031, 138038, 138048])\n",
      "3124\n",
      "{65536, 49153, 8194, 24579, 122884, 16391, 73741, 81936, 49170, 49171, 49173, 65560, 106525, 24609, 98343, 90152, 131111, 122923, 90158, 81968, 98355, 114743, 73786, 65595, 106556, 98369, 16453, 90181, 98376, 106568, 16459, 41040, 49233, 65620, 90201, 8283, 122974, 41056, 57440, 32868, 24678, 41063, 107, 57451, 49262, 24689, 73842, 106623, 98432, 24705, 57471, 123014, 41100, 8334, 123028, 49301, 131220, 98456, 106649, 8346, 82076, 24736, 114850, 98469, 106662, 167, 16552, 98477, 106670, 123055, 90288, 41138, 131250, 114870, 16569, 106681, 65725, 49347, 16580, 49348, 65734, 106694, 114891, 24787, 106715, 49376, 24804, 90341, 106724, 41191, 106725, 114916, 90356, 41207, 123131, 24829, 57597, 98560, 16641, 33025, 33027, 49415, 49416, 16649, 98567, 8459, 73999, 274, 123156, 57625, 82202, 33056, 65830, 33063, 65834, 82218, 24881, 16700, 115006, 106816, 57665, 131397, 33094, 41295, 115030, 115033, 33117, 41310, 106847, 90470, 131432, 57707, 82285, 131443, 49524, 123252, 49529, 98691, 106887, 41352, 396, 8588, 90511, 33175, 408, 123289, 82335, 16804, 106920, 123306, 74157, 25006, 41393, 33202, 131505, 16825, 106946, 41412, 16840, 458, 98763, 82382, 57809, 33235, 41428, 25045, 106966, 98778, 16859, 33249, 57829, 486, 74213, 16873, 490, 49642, 41457, 8691, 131575, 49657, 41470, 25091, 131587, 25094, 16905, 115212, 33293, 8718, 49679, 98832, 74258, 41497, 25114, 41498, 115226, 123417, 66080, 66083, 33318, 16936, 554, 57900, 8751, 82479, 131632, 82483, 33333, 25146, 57914, 82492, 8767, 82495, 107074, 57932, 107087, 90706, 16984, 57948, 605, 131683, 25190, 8810, 17002, 74348, 107115, 107118, 66159, 115306, 8818, 90741, 49785, 66170, 17022, 90751, 107134, 41601, 98945, 49795, 33412, 8837, 66182, 115327, 651, 82584, 66203, 41629, 74403, 25253, 131751, 74410, 8874, 33451, 58027, 8877, 131758, 25264, 115376, 690, 692, 8885, 33462, 33461, 49853, 58046, 33471, 131781, 99014, 82631, 714, 17099, 716, 49867, 123595, 58064, 49873, 722, 41682, 8917, 115415, 41688, 123612, 17118, 115425, 49890, 8932, 25316, 33510, 107239, 115435, 115436, 25325, 49905, 74483, 90870, 17145, 58108, 115456, 99077, 66310, 41735, 82694, 123656, 782, 17167, 74510, 115471, 82707, 123667, 58134, 25367, 25368, 33563, 25378, 803, 8995, 808, 115498, 66348, 131889, 25395, 123700, 25397, 49974, 49975, 74549, 82743, 90937, 49979, 33596, 17213, 33598, 107322, 131900, 115522, 33606, 99146, 58192, 852, 33621, 33625, 17243, 41820, 115548, 50029, 107375, 17271, 66423, 9081, 9084, 74621, 131969, 123779, 123783, 50056, 91020, 99214, 41874, 33683, 74642, 131989, 115607, 115614, 91039, 115621, 936, 74665, 33706, 74668, 33709, 74670, 99247, 9136, 123823, 132016, 82868, 949, 107444, 115639, 99258, 41915, 74683, 960, 961, 50112, 107457, 66503, 82889, 41933, 41937, 107473, 9174, 25558, 50135, 107498, 25579, 123884, 33773, 9200, 17393, 25584, 58353, 66544, 91126, 50172, 107519, 74754, 41987, 74756, 17413, 41992, 123918, 66578, 42004, 66596, 123941, 42022, 42024, 74792, 74797, 91183, 82993, 9266, 25651, 132147, 66613, 9273, 42051, 66628, 99398, 33866, 132174, 115791, 33872, 66640, 132177, 74840, 1118, 33887, 33891, 115815, 9320, 50283, 91245, 99437, 107632, 9330, 42098, 107635, 58490, 42118, 83078, 83086, 115855, 58512, 50324, 115863, 1176, 9368, 33946, 115870, 42143, 132266, 124080, 25778, 42162, 17597, 9408, 66754, 66755, 107715, 9414, 107718, 99533, 83155, 50390, 99542, 124120, 42203, 50401, 66785, 34019, 9446, 66790, 25832, 40955, 74991, 99571, 132339, 17653, 25850, 42237, 83197, 9475, 107782, 75015, 132360, 9485, 115984, 50449, 91410, 9491, 50454, 58648, 99610, 34075, 42268, 99611, 75044, 107816, 1323, 34094, 116015, 1331, 83254, 75064, 75065, 25914, 124216, 34108, 124222, 34112, 9537, 42308, 50502, 107847, 124235, 75084, 107852, 116046, 50511, 75089, 124242, 50515, 132440, 58714, 66908, 66909, 99678, 42338, 17765, 50534, 132460, 50544, 58736, 75121, 99700, 75127, 42370, 132486, 83336, 107913, 34189, 91533, 99729, 116114, 124307, 75156, 91547, 34207, 99744, 107939, 83368, 99753, 66986, 42411, 107946, 132528, 42417, 99761, 42421, 107958, 17850, 9659, 67007, 42432, 26050, 9668, 50629, 132553, 50635, 1484, 26060, 1486, 116171, 67024, 116179, 83412, 17877, 107989, 107991, 9689, 75229, 75233, 67044, 91621, 26087, 49147, 1513, 9709, 132590, 50677, 67065, 91643, 67074, 58883, 99843, 67077, 75272, 83468, 83469, 91661, 108044, 116240, 132625, 42520, 42521, 75290, 75291, 34333, 91688, 91692, 9775, 116275, 26164, 26165, 67127, 108088, 50746, 17987, 116292, 58950, 34380, 91735, 108124, 42589, 132703, 91745, 132705, 58983, 50793, 132717, 1646, 34414, 83566, 83569, 108146, 124527, 58996, 124531, 18039, 116347, 116348, 18048, 83586, 75396, 34437, 67206, 18057, 91786, 132746, 18061, 67214, 116368, 1689, 99995, 99998, 132766, 18080, 91811, 50854, 42668, 116399, 26288, 50867, 18105, 34491, 91835, 108223, 67264, 100032, 9924, 67270, 108239, 18128, 50897, 42709, 83671, 116449, 91879, 42731, 108279, 26360, 75512, 18171, 75516, 124671, 75520, 132866, 67331, 100101, 34566, 26375, 50951, 1804, 83724, 18192, 34576, 108311, 59162, 1820, 42781, 59172, 34597, 10023, 59177, 75562, 10033, 124721, 75571, 1846, 18230, 91960, 34618, 100156, 132926, 132927, 91968, 132929, 100162, 132937, 91981, 10063, 75605, 34648, 42840, 124763, 91999, 92004, 124773, 18279, 132969, 116588, 132979, 1908, 1909, 26486, 83829, 18296, 75641, 124796, 51070, 59262, 75653, 10119, 67463, 10121, 59277, 133009, 116628, 75671, 42905, 18332, 51101, 34722, 67496, 51113, 100264, 92078, 75699, 124851, 116662, 42937, 133050, 75707, 34750, 51134, 34758, 116680, 116681, 34763, 116683, 124875, 34766, 75726, 92110, 92112, 124879, 133073, 59348, 59351, 92121, 133082, 18397, 100319, 83947, 92141, 10222, 18420, 116724, 75767, 124923, 83968, 2053, 18440, 75784, 51210, 83977, 59408, 2065, 26644, 108566, 92183, 75809, 59433, 116778, 2091, 124974, 2096, 84017, 10290, 59443, 100404, 124980, 133172, 26693, 75847, 59465, 2139, 59487, 26722, 116836, 92265, 116842, 26734, 133233, 59507, 116854, 75899, 51324, 133247, 10369, 51332, 133257, 26762, 75914, 18572, 26765, 43150, 43151, 100495, 116875, 108690, 18581, 116885, 108695, 75927, 92311, 75936, 125089, 18595, 67748, 125095, 116904, 108717, 59566, 2225, 26801, 34995, 59572, 59573, 75953, 34999, 92341, 43193, 51385, 75965, 10434, 18626, 26823, 92359, 108753, 43218, 133334, 100571, 116956, 125148, 18654, 43231, 67809, 84203, 43246, 125166, 59635, 2293, 10485, 84217, 108794, 116987, 84223, 84225, 125185, 51459, 100615, 59657, 10510, 18705, 67861, 51481, 59674, 43292, 67874, 100645, 133421, 35132, 18749, 100668, 108862, 43328, 117056, 117058, 133439, 35141, 84293, 10567, 10570, 10576, 125266, 18771, 76115, 76116, 59735, 76121, 76123, 92507, 108894, 18783, 2400, 59744, 92513, 100708, 92525, 2414, 108919, 76152, 2430, 18815, 27008, 27009, 10626, 133505, 2438, 84358, 10632, 125319, 2453, 18845, 92573, 125342, 117152, 125343, 125355, 18860, 59823, 117168, 133552, 59826, 51635, 68020, 68021, 92599, 117176, 133559, 117181, 51647, 68034, 68035, 125380, 27077, 100810, 27084, 125389, 2515, 59860, 59863, 10712, 10713, 35290, 84441, 2525, 68061, 109028, 51687, 59881, 35316, 2549, 109044, 2551, 125429, 133625, 92666, 109052, 27133, 59902, 84482, 35333, 100869, 51719, 100871, 76297, 43530, 27148, 59916, 76303, 10769, 133649, 109079, 117271, 18970, 68123, 35359, 59936, 125479, 117292, 92719, 51760, 125487, 133683, 133687, 19001, 92732, 109116, 133694, 59972, 117318, 117319, 51785, 59978, 109134, 133715, 100948, 125524, 51801, 100955, 51806, 109153, 27234, 19044, 84584, 117353, 19054, 68206, 2674, 92788, 27253, 2678, 125558, 43642, 10875, 84605, 84607, 68228, 133764, 92808, 133770, 19084, 125580, 84622, 35472, 10900, 43671, 51865, 125595, 101022, 133794, 43683, 51878, 68265, 27309, 101040, 51896, 60088, 117434, 125628, 133820, 133821, 92863, 125633, 76483, 10950, 51910, 60108, 125644, 35542, 60124, 19165, 19166, 125664, 133856, 117474, 27366, 125676, 51949, 117491, 43764, 43765, 60152, 84729, 117500, 19198, 84743, 117512, 101129, 76554, 84752, 27410, 60178, 76563, 133908, 51990, 43807, 11040, 133920, 68386, 109347, 133921, 43815, 117543, 27438, 84783, 92975, 92977, 43826, 84788, 84789, 133942, 92980, 117560, 101172, 11066, 109379, 43846, 2891, 93009, 43859, 35669, 43861, 76629, 11097, 84825, 84826, 101210, 60261, 52074, 76655, 2929, 52082, 134002, 76663, 93048, 93049, 101245, 27520, 11138, 93065, 109456, 43925, 11160, 60317, 2980, 60325, 125863, 93095, 117671, 60329, 35759, 19376, 60336, 11186, 19379, 84918, 101308, 43965, 84925, 27583, 76735, 101314, 134084, 11207, 35783, 125896, 27606, 3031, 11226, 52189, 125918, 44000, 125922, 60387, 35812, 44005, 101349, 52201, 60395, 93163, 3054, 93166, 68592, 109552, 44021, 52215, 35832, 93176, 125946, 3066, 109561, 35842, 44035, 11272, 134152, 117771, 125966, 52242, 19478, 3097, 109601, 52261, 11302, 85035, 109612, 134190, 27704, 76856, 35907, 76867, 52298, 11341, 76881, 93269, 126037, 134229, 134231, 85088, 134242, 3172, 19560, 93288, 109673, 35955, 35956, 76917, 109683, 101505, 44167, 11401, 85132, 11407, 44177, 117910, 68760, 52378, 52386, 134306, 134309, 60582, 44199, 52391, 68774, 126122, 93355, 68782, 93361, 3257, 93370, 134329, 11454, 101567, 134338, 93379, 27846, 101575, 11466, 19659, 60620, 3277, 52429, 109773, 27856, 68817, 36050, 11478, 27862, 126168, 36057, 126173, 19678, 77027, 93411, 117994, 44272, 44273, 77047, 44287, 85254, 134410, 68875, 93454, 3343, 68881, 68882, 118038, 126232, 68890, 19744, 118052, 36142, 3379, 19765, 126264, 134456, 93501, 101695, 60737, 27974, 68934, 44360, 68936, 44362, 93515, 101712, 126288, 109913, 109914, 11611, 44382, 52575, 126306, 28003, 3428, 134499, 109927, 60780, 19821, 28014, 11631, 52589, 85357, 93557, 77174, 109945, 28026, 36228, 118149, 101766, 36238, 93582, 77200, 69012, 93589, 28054, 36247, 134549, 28057, 85401, 93595, 3485, 93597, 93599, 101798, 52647, 101799, 28076, 11693, 77239, 44472, 77241, 60861, 126397, 11715, 3524, 44489, 69067, 134604, 3536, 3541, 36309, 77271, 28120, 28121, 11741, 69086, 134625, 77284, 134630, 44525, 60910, 3572, 118260, 52732, 69117, 93699, 3588, 101891, 36359, 118283, 93713, 69139, 60949, 28182, 110105, 36379, 3617, 93734, 52776, 101929, 44586, 126506, 20012, 44589, 110126, 126510, 134700, 36402, 77364, 93748, 134708, 69176, 77375, 11842, 69188, 11845, 110149, 3655, 3657, 110159, 77394, 101972, 110165, 11862, 69207, 77401, 69216, 69217, 77413, 44647, 3691, 118380, 44654, 77425, 69238, 28279, 77430, 85624, 134774, 110210, 126594, 11908, 52872, 85642, 126602, 3730, 52884, 61077, 102036, 61079, 52888, 110230, 134813, 20129, 102052, 36518, 20136, 77480, 52906, 61098, 93866, 134827, 52915, 126647, 134843, 102076, 36548, 28357, 93894, 20167, 11976, 36552, 69321, 134854, 110284, 110287, 11984, 118486, 69336, 36570, 77531, 61149, 36574, 28391, 44785, 134897, 93939, 118518, 118521, 12035, 93961, 61195, 3853, 85774, 36625, 102164, 77597, 53022, 110369, 28454, 110375, 102184, 36650, 110379, 126765, 110388, 85821, 110397, 28479, 12096, 3909, 36677, 85831, 28492, 134991, 61266, 126803, 126805, 20315, 53086, 110440, 110441, 135016, 28525, 20334, 28528, 135024, 20339, 126838, 135037, 12158, 44927, 85893, 53131, 94092, 94093, 85906, 118676, 53145, 102297, 94107, 3998, 12193, 53153, 61348, 126885, 61354, 135087, 36784, 77750, 85942, 61368, 61371, 12220, 12221, 94139, 118718, 110528, 126919, 85960, 77769, 94152, 28619, 110537, 20430, 61390, 85971, 135133, 4063, 53219, 85990, 102378, 61425, 86002, 77812, 36853, 126966, 135161, 53251, 94213, 94221, 53262, 45071, 135184, 4114, 4115, 94229, 4118, 135189, 36888, 4122, 61467, 53280, 45089, 28706, 53284, 77866, 77868, 86060, 110636, 118837, 86074, 110660, 135237, 102470, 36936, 86089, 45130, 127049, 28752, 61521, 61523, 135252, 102485, 4182, 53342, 135262, 135271, 36968, 53352, 127081, 69741, 77933, 118893, 86128, 135277, 135278, 36981, 12406, 45180, 45184, 53381, 135301, 69769, 94346, 102538, 127113, 37005, 12437, 20629, 102553, 77983, 135328, 135329, 118946, 53411, 86180, 118950, 28841, 110764, 86189, 94381, 127151, 94384, 20658, 127154, 4277, 53431, 127159, 127164, 20674, 69826, 110791, 127176, 127178, 4299, 28876, 86220, 53454, 110796, 135378, 86229, 135391, 135394, 69860, 20710, 119017, 37100, 53485, 69876, 28921, 127225, 135420, 20735, 119044, 135441, 110868, 37143, 45336, 61719, 94487, 28959, 69931, 45359, 20785, 53553, 61746, 94529, 53572, 61765, 86342, 12615, 110928, 78163, 37206, 69976, 4441, 110936, 29023, 135526, 78183, 135527, 4462, 29039, 53615, 110961, 37236, 119163, 53629, 20866, 29059, 53637, 61830, 45447, 86409, 4490, 4496, 135570, 61843, 53659, 29089, 37282, 12708, 29092, 94631, 102825, 86442, 45489, 86452, 29111, 61880, 102843, 37308, 127425, 45506, 29123, 135620, 70089, 20939, 135628, 127437, 37326, 70094, 135630, 45521, 94680, 12761, 94681, 53725, 86494, 86498, 135653, 61932, 135662, 111088, 53746, 4596, 119288, 70141, 94720, 37379, 61955, 119301, 70154, 37388, 102925, 70158, 119313, 94738, 102930, 111126, 86558, 86559, 111143, 21034, 21035, 70188, 61997, 94766, 119338, 70195, 45623, 111167, 102978, 111174, 70220, 45645, 135760, 135768, 127583, 12897, 12899, 127590, 111207, 127594, 111212, 135790, 21106, 127606, 127609, 21115, 103036, 70275, 53893, 103050, 127629, 21139, 86681, 111261, 119457, 21155, 94884, 21158, 53938, 111285, 37566, 12991, 94911, 103106, 21187, 53955, 94917, 45766, 53964, 94926, 111310, 103126, 94948, 62181, 29414, 53991, 4840, 103148, 135921, 94964, 127733, 94970, 135931, 45825, 78595, 135939, 21258, 86796, 13069, 127756, 13078, 45855, 78623, 127775, 103203, 13093, 29480, 37677, 54063, 111408, 78642, 78644, 78646, 45879, 78647, 95032, 62269, 135997, 45888, 37701, 86854, 78663, 78664, 103237, 95050, 45903, 136017, 54098, 70483, 119635, 86869, 111448, 119641, 111453, 136029, 86880, 136033, 95076, 103269, 13159, 78696, 4970, 13171, 54133, 111479, 29560, 4987, 13179, 45948, 127868, 4997, 45958, 54149, 37768, 13194, 54154, 111500, 103310, 45967, 21392, 78738, 54164, 119702, 103322, 45981, 95135, 119713, 62378, 78775, 136139, 62413, 5073, 46036, 5083, 13280, 111588, 21477, 87014, 136164, 78825, 95214, 54258, 29686, 87030, 37880, 78840, 21499, 21500, 54268, 54270, 78843, 46080, 128003, 37896, 78859, 111627, 29710, 46096, 13333, 103445, 111639, 5145, 87065, 103457, 46122, 136234, 136236, 111661, 78900, 54325, 111675, 103485, 29761, 95299, 70724, 111688, 78927, 21584, 70738, 54358, 128087, 37977, 103517, 111710, 5217, 29793, 37987, 103527, 111727, 5232, 38010, 128124, 54397, 87165, 13446, 46215, 21640, 21643, 13452, 13457, 62609, 119956, 13463, 111769, 5284, 29867, 21678, 70830, 70831, 38065, 54449, 111795, 136367, 21695, 79042, 95428, 29898, 95434, 87249, 21716, 54485, 87266, 87268, 62693, 128230, 5353, 95467, 95469, 29934, 38127, 5360, 46318, 87286, 128246, 13561, 95481, 5373, 38141, 29952, 87298, 111875, 103685, 5388, 120078, 87312, 46355, 46368, 62754, 128293, 111912, 5417, 38185, 21804, 95537, 21810, 5431, 30008, 70967, 79162, 62781, 128318, 13635, 111942, 87367, 5450, 120138, 111949, 62799, 87375, 95567, 136530, 70998, 13659, 54628, 79207, 120169, 21866, 111982, 13681, 95602, 136561, 46454, 62847, 120192, 128384, 120196, 136580, 71047, 120201, 5516, 21901, 46477, 38291, 128409, 46490, 13722, 30107, 62875, 5534, 38302, 13728, 5537, 95643, 128416, 79268, 136605, 79270, 103846, 62889, 13744, 120241, 79284, 21946, 79296, 95680, 120259, 112069, 13767, 87497, 21964, 103886, 13775, 136655, 79317, 38360, 5596, 120287, 136672, 30181, 79337, 112112, 128498, 120307, 103935, 87564, 13838, 38416, 95761, 22034, 120336, 136722, 120341, 136725, 63003, 71197, 22046, 103967, 112159, 112160, 54819, 30248, 46634, 22060, 38445, 103983, 79413, 13879, 87608, 95802, 63035, 71239, 128583, 22097, 46679, 79448, 120412, 79454, 63074, 112227, 5732, 13927, 38503, 120424, 63082, 136808, 5742, 87668, 136822, 71288, 79480, 112251, 38524, 104061, 95870, 128640, 63107, 112260, 104072, 54921, 128649, 5771, 87692, 13967, 71313, 128667, 63132, 79520, 136869, 30376, 104108, 30381, 95918, 120494, 22193, 54964, 22199, 79549, 5822, 128703, 136897, 30402, 54982, 63176, 38601, 22221, 46799, 14038, 87766, 128728, 71387, 55004, 120551, 87786, 87788, 120559, 55025, 38647, 112376, 120567, 95995, 136959, 112390, 5899, 63244, 14096, 46865, 30496, 46886, 71462, 96039, 128813, 5935, 5939, 120630, 104250, 46910, 22336, 55107, 71494, 46923, 120655, 128850, 30549, 120662, 120664, 128860, 5982, 22367, 22368, 22369, 22370, 79710, 5991, 55143, 63336, 55149, 46961, 71541, 22390, 87927, 46970, 55169, 112513, 38790, 87943, 6025, 137097, 128907, 14221, 46990, 112525, 14224, 87952, 46995, 137108, 71573, 128918, 46999, 30618, 87962, 112541, 96166, 30636, 87980, 137133, 22460, 30652, 79804, 87997, 6081, 38849, 55233, 71622, 120774, 47056, 47058, 22484, 55255, 79833, 137179, 47073, 14308, 71658, 96235, 22510, 6128, 63472, 47091, 38903, 88055, 112631, 6139, 129020, 137212, 71682, 63492, 137220, 71702, 104471, 120854, 137240, 55324, 30749, 88092, 14367, 88100, 129063, 38952, 22571, 88108, 14387, 6198, 47164, 104510, 71743, 6210, 120901, 71750, 30792, 129098, 79949, 129101, 47185, 104535, 6234, 30811, 39004, 14429, 129118, 88160, 55399, 129128, 47213, 30830, 129137, 30834, 79986, 96370, 104565, 120946, 129143, 63608, 63612, 47240, 55432, 47242, 71819, 88203, 6300, 71840, 137376, 47268, 6310, 63656, 80045, 129202, 96435, 14516, 137398, 71865, 104634, 112825, 112829, 112832, 112833, 47299, 6340, 104645, 22727, 6350, 112847, 121040, 63697, 6355, 96472, 71898, 14557, 96477, 129246, 80097, 137441, 112868, 30957, 55535, 104695, 88312, 63742, 6401, 80136, 39177, 14608, 39185, 47376, 14611, 80148, 96528, 80150, 80151, 96530, 96536, 129303, 121121, 96550, 129321, 47402, 137513, 39213, 137517, 71986, 39229, 80190, 22847, 63807, 112959, 55618, 47430, 80200, 22857, 129353, 55627, 121167, 55634, 129364, 63831, 47448, 47451, 104799, 96610, 112999, 113002, 47468, 14701, 129389, 14703, 96624, 113011, 22903, 14714, 47483, 80251, 104826, 113023, 88450, 88452, 14725, 96648, 113035, 55695, 39324, 72092, 137631, 31136, 88480, 113056, 63907, 47529, 80299, 88492, 96686, 39348, 121268, 55736, 96697, 121276, 31166, 47559, 72137, 39370, 39371, 6604, 129481, 63950, 113102, 121294, 31185, 72145, 104914, 63956, 14808, 129496, 137688, 80347, 121308, 39396, 137704, 14827, 113133, 104947, 55796, 88565, 63990, 129524, 63996, 23037, 6656, 64003, 129542, 23051, 129547, 96781, 129551, 14865, 55828, 23064, 47640, 14876, 55840, 137762, 14883, 23076, 55848, 121385, 137769, 113195, 88620, 39473, 96820, 14908, 14909, 39484, 64060, 105021, 47682, 105028, 72261, 14918, 80453, 6728, 80457, 72271, 96849, 14933, 55893, 80469, 105047, 55899, 88667, 55901, 14942, 80478, 6752, 31329, 55906, 31331, 72288, 72292, 105054, 129633, 137832, 47721, 72297, 113263, 39536, 64116, 23160, 23165, 121470, 47743, 14976, 113281, 137854, 96899, 55943, 31368, 129671, 72330, 6795, 80523, 121482, 121495, 55962, 88730, 137883, 80545, 55973, 64169, 72369, 105139, 55989, 96951, 96957, 96959, 56007, 31432, 15051, 39627, 47819, 137936, 137939, 72405, 137943, 6872, 31448, 31453, 15072, 72418, 6887, 88809, 129772, 129773, 47854, 31482, 56059, 105211, 97024, 97029, 105223, 56073, 47885, 137999, 121616, 121617, 88851, 47892, 56084, 121619, 47898, 138011, 97052, 39713, 31524, 72488, 97064, 31535, 88879, 47921, 113457, 138031, 105270, 138038, 138048, 64321, 121668, 121671, 64337, 113490, 56148, 97109, 15195, 31580, 39771, 7008, 80738, 7015, 64361, 56177, 72562, 31603, 47989, 23423, 105346, 48007, 97159, 129930, 56203, 64397, 64405, 31638, 72599, 56216, 80793, 15260, 56222, 56224, 80802, 15276, 7089, 80818, 72628, 89015, 113593, 121786, 15299, 23491, 105411, 7112, 80849, 48084, 105430, 105432, 72669, 72670, 7139, 64484, 72680, 113641, 97262, 39919, 7152, 72688, 64498, 23543, 97273, 97276, 113663, 15364, 15365, 15366, 7183, 97296, 105487, 121874, 80917, 39958, 72727, 105507, 7208, 7209, 113705, 15403, 23596, 121897, 64558, 113710, 15408, 15411, 89139, 64569, 105534, 23616, 15426, 130116, 121937, 23634, 31826, 64595, 72793, 48219, 121947, 97374, 113759, 56416, 64614, 56429, 64621, 7284, 113784, 48249, 48250, 15483, 81023, 121986, 31876, 121988, 89223, 15496, 64657, 7314, 122001, 7316, 130195, 122007, 81052, 105628, 113824, 40104, 81065, 89259, 81068, 97454, 130222, 64688, 97457, 23733, 97462, 15546, 113850, 7358, 15553, 105665, 130241, 105670, 40135, 113865, 64718, 48335, 72910, 89297, 31955, 130261, 31960, 81113, 97502, 122078, 7392, 64736, 23779, 48356, 31973, 97509, 23784, 113898, 64750, 48371, 23800, 89350, 56586, 7435, 23820, 48397, 122143, 48419, 122147, 105766, 7464, 97577, 105771, 73009, 48434, 32052, 105782, 32056, 48445, 89410, 56643, 130371, 105797, 105801, 122189, 32081, 7506, 114001, 48470, 7515, 56669, 89438, 15713, 48481, 81250, 130411, 81261, 97647, 130415, 122225, 89461, 114040, 56708, 32133, 64907, 89483, 89490, 114066, 122258, 23957, 23962, 40348, 130461, 114080, 130465, 81314, 32163, 23972, 32164, 56743, 32175, 23984, 105908, 73141, 23990, 89525, 40378, 81338, 122299, 130491, 89543, 89544, 56782, 81360, 15835, 24027, 32220, 48603, 32227, 48615, 32236, 7661, 15856, 97777, 122353, 48632, 114168, 7675, 24060, 40450, 105987, 81412, 105998, 48655, 32275, 97811, 130580, 65046, 40471, 106008, 73241, 130581, 48669, 40478, 81439, 89630, 130593, 106020, 122404, 24102, 32300, 122414, 65071, 130606, 73270, 48695, 65084, 7742, 65088, 48705, 15938, 130624, 56900, 106053, 73286, 89671, 114247, 114250, 122442, 114253, 32335, 32345, 24154, 89690, 40543, 73313, 15971, 56936, 89705, 24173, 130673, 24179, 15990, 7801, 32377, 32379, 65147, 15997, 73337, 130683, 114305, 89733, 24198, 89736, 7819, 16011, 32401, 81557, 122523, 81565, 81567, 89760, 40609, 16036, 16039, 122537, 32428, 97966, 24245, 16054, 16058, 65217, 106179, 24262, 16080, 57042, 40659, 89811, 89812, 24280, 122587, 57061, 122600, 32489, 122601, 40685, 114413, 130800, 48881, 89841, 106227, 130803, 106237, 7934, 89858, 89865, 73482, 106250, 65294, 122640, 57107, 32534, 98070, 81688, 57113, 89881, 106265, 40733, 81698, 122658, 122662, 89897, 98089, 73517, 98094, 81713, 106294, 130878, 16203, 122704, 8026, 114523, 81758, 122722, 57189, 114534, 73575, 89959, 49005, 16240, 98162, 65395, 8058, 106363, 40829, 65407, 24449, 122754, 73606, 114566, 122761, 114569, 40842, 81803, 49036, 8077, 24463, 8080, 16272, 32653, 98188, 16279, 73623, 106392, 57247, 98207, 32673, 73633, 122787, 98215, 130983, 24489, 130984, 32684, 49077, 16311, 24507, 90044, 106427, 8126, 106429, 24515, 49092, 73669, 73670, 8146, 32724, 90070, 57303, 114648, 114651, 16358, 73702, 90092, 49133, 57325, 114674, 122871, 24570, 57339, 49149}\n"
     ]
    }
   ],
   "source": [
    "wpred = Test_Graph.edata['Edge_indx'][wrong_preds]\n",
    "wpred_ab = Test_Graph_ab.edata['Edge_indx'][wrong_preds_ab]\n",
    "\n",
    "print(wpred)\n",
    "print(wpred_ab)\n",
    "\n",
    "edges_to_explain = set(wpred_ab.numpy()) - set(wpred.numpy())\n",
    "\n",
    "print(len(edges_to_explain))\n",
    "print(edges_to_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "passing-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Edge_indx  label\n",
      "3123      98607      0\n",
      "1106     102377      0\n",
      "1105       4448      0\n",
      "1104      39811      0\n",
      "1103      23951      0\n",
      "...         ...    ...\n",
      "348      127231      1\n",
      "2516      42725      1\n",
      "351       84483      1\n",
      "334        7892      1\n",
      "1657      85275      1\n",
      "\n",
      "[3124 rows x 2 columns]\n",
      "nb attacks : 322\n",
      "nb benign : 2802\n"
     ]
    }
   ],
   "source": [
    "indx_to_explain = []\n",
    "\n",
    "for x in edges_to_explain:\n",
    "    indx_to_explain.append((Test_Graph.edata['Edge_indx'] == x).nonzero(as_tuple=True)[0].item())\n",
    "\n",
    "# print(indx_to_explain)\n",
    "\n",
    "df_indx = pd.DataFrame(columns = ['Edge_indx', 'label'])\n",
    "\n",
    "for x in indx_to_explain:\n",
    "    df_indx.loc[-1] = [x, Test_Graph.edata['label'][x].item()]  # adding a row\n",
    "    df_indx.index = df_indx.index + 1  # shifting index\n",
    "\n",
    "    \n",
    "df_indx = df_indx.sort_values('label')\n",
    "print(df_indx)\n",
    "\n",
    "print('nb attacks :', len(df_indx.loc[df_indx['label'] == 1]))\n",
    "print('nb benign :', len(df_indx.loc[df_indx['label'] == 0]))\n",
    "\n",
    "df_indx.to_csv(f'/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/Edges_to_explain.csv', sep=',', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-knight",
   "metadata": {},
   "source": [
    "### GNN-Edge-Explainer (Local Explanation for each one of the 3% edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "described-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Explanation ***********************************************************************\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "from dgl import EID, NID, khop_out_subgraph\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "\n",
    "# init mask\n",
    "def init_masks(graph, efeat):\n",
    "    # efeat.size() = torch.Size([nb_edges, 1, 76])\n",
    "    efeat_size = efeat.size()[2]\n",
    "    num_edges = graph.num_edges()\n",
    "    num_nodes = graph.num_nodes()\n",
    "\n",
    "    device = efeat.device\n",
    "\n",
    "    std = 0.1\n",
    "    # feat_mask = [[f1, f2, .... fn]] / n = nb_features\n",
    "    efeat_mask = nn.Parameter(th.randn(1, efeat_size, device=device) * std)\n",
    "\n",
    "    std = nn.init.calculate_gain(\"relu\") * sqrt(2.0 / (2 * num_nodes))\n",
    "    # edge_mask = [e1, e2, .... em] / m = nb_edges\n",
    "    edge_mask = nn.Parameter(th.randn(num_edges, device=device) * std)\n",
    "\n",
    "    # print(\"efeat_mask : \", efeat_mask)\n",
    "    # print(\"edge_mask : \", edge_mask)\n",
    "\n",
    "    return efeat_mask, edge_mask\n",
    "\n",
    "\n",
    "# Regularization loss\n",
    "def loss_regularize(loss, feat_mask, edge_mask):\n",
    "    # epsilon for numerical stability\n",
    "    eps = 1e-15\n",
    "    # From self GNNExplainer self\n",
    "    alpha1 = 0.005,\n",
    "    alpha2 = 1.0\n",
    "    beta1 = 1.0\n",
    "    beta2 = 0.1\n",
    "\n",
    "    edge_mask = edge_mask.sigmoid()\n",
    "    # Edge mask sparsity regularization\n",
    "    loss = loss + th.from_numpy(alpha1 * th.Tensor.cpu(th.sum(edge_mask)).detach().numpy()).cuda()\n",
    "    # Edge mask entropy regularization\n",
    "    ent = -edge_mask * th.log(edge_mask + eps) - (\n",
    "        1 - edge_mask\n",
    "    ) * th.log(1 - edge_mask + eps)\n",
    "    loss = loss + alpha2 * ent.mean()\n",
    "\n",
    "    feat_mask = feat_mask.sigmoid()\n",
    "    # Feature mask sparsity regularization\n",
    "    loss = loss + beta1 * th.mean(feat_mask)\n",
    "    # Feature mask entropy regularization\n",
    "    ent = -feat_mask * th.log(feat_mask + eps) - (\n",
    "        1 - feat_mask\n",
    "    ) * th.log(1 - feat_mask + eps)\n",
    "    loss = loss + beta2 * ent.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# Edge mask\n",
    "def explain_edges(model, edge_id, graph, node_feat, edge_feat, **kwargs):\n",
    "    model = model.to(graph.device)\n",
    "    model.eval()\n",
    "\n",
    "    #print(graph.edges())\n",
    "\n",
    "    # Extract source node-centered k-hop subgraph from the edge_id and its associated node and edge features.\n",
    "    num_hops = 3\n",
    "    source_node = th.Tensor.cpu(graph.edges()[0][edge_id]).detach().numpy()\n",
    "    #print(\"source_node : \", source_node)\n",
    "    edge_h = graph.edata['h'][edge_id]\n",
    "    sg, inverse_indices = khop_out_subgraph(graph, source_node, num_hops)\n",
    "    #print(\"new_node_indice : \", inverse_indices)\n",
    "\n",
    "    #print(sg.edges())\n",
    "    #print(edge_h)\n",
    "    #print(sg.edata['h'])\n",
    "\n",
    "    for indx, nd_id in enumerate(sg.edges()[0]):\n",
    "        if inverse_indices == nd_id :\n",
    "            if (sg.edata['h'][indx][0] == edge_h[0]).all() :\n",
    "                # print(\"edge index is : \", indx)\n",
    "                edge_indice = indx\n",
    "                break\n",
    "    \n",
    "    #print(\"new_edge_indice : \", edge_indice)\n",
    "\n",
    "    # EID = NID = _ID\n",
    "    # tensor([0, 1, 2, 4]) : nodes and edges ids\n",
    "    sg_edges = sg.edata[EID].long()\n",
    "    sg_nodes = sg.ndata[NID].long()\n",
    "\n",
    "    #print(\"+++++++++++++++++++++++\")\n",
    "    #print(\"sg : \", sg)\n",
    "    #print(\"sg_edges : \", sg_edges) # edges ids in graph.edges()\n",
    "    #print(\"sg_nodes : \", sg_nodes) # nodes ids in graph.nodes()\n",
    "\n",
    "    #print()\n",
    "    edge_feat = edge_feat[sg_edges]\n",
    "    node_feat = node_feat[sg_nodes]\n",
    "\n",
    "    #print(\"edge_feat : \", edge_feat)\n",
    "    #print(\"node_feat : \", node_feat)\n",
    "    #print(\"+++++++++++++++++++++++\")\n",
    "    \n",
    "    \n",
    "    # Get the initial prediction.\n",
    "    #print(\"Get the initial prediction :\")\n",
    "    with th.no_grad():\n",
    "        # logits = model(g = sg, nfeats = node_feat, efeats = edge_feat, **kwargs)\n",
    "        logits = model(g = sg, nfeats = node_feat, efeats = edge_feat)\n",
    "        pred_label = logits.argmax(dim=-1)\n",
    "        # pred_label1 = logits.argmax(1)\n",
    "\n",
    "    #print(\"pred_label : \", pred_label)\n",
    "    # print(pred_label1)\n",
    "\n",
    "    #\n",
    "    efeat_mask, edge_mask = init_masks(sg, edge_feat)\n",
    "\n",
    "    params = [edge_mask]\n",
    "    optimizer = th.optim.Adam(params, lr = 0.01)\n",
    "\n",
    "    # num_epochs = 300\n",
    "    #print(\"***********************************\")\n",
    "    #print(\"initial masks : \")\n",
    "    #print(\"efeat_mask : \", efeat_mask)\n",
    "    #print(\"edge_mask : \", edge_mask)\n",
    "    #print(\"***********************************\")\n",
    "    \n",
    "    \n",
    "    from sklearn.utils import class_weight\n",
    "    # class_weights2 = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(sg.edata['label'].cpu().numpy()), y = sg.edata['label'].cpu().numpy())\n",
    "    # class_weights2 = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.array([0, 1]), y = sg.edata['label'].cpu().numpy())\n",
    "    # class_weights2 = th.FloatTensor(class_weights2).cuda()\n",
    "    # criterion2 = nn.CrossEntropyLoss(weight = class_weights2)\n",
    "    criterion2 = nn.CrossEntropyLoss()\n",
    "    train_mask2 = th.ones(len(sg.edata['h']), dtype=th.bool)\n",
    "    import datetime\n",
    "    \n",
    "    #print(f'explanation starts at {datetime.datetime.now()}')\n",
    "    #print(\"nb edges : \", sg.num_edges())\n",
    "    #print(\"nb nodes : \", sg.num_nodes())\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, 300):\n",
    "        optimizer.zero_grad()\n",
    "        # Edge mask\n",
    "        logits = model(g = sg, nfeats = node_feat, efeats = edge_feat, eweight=edge_mask.sigmoid()).cuda()\n",
    "        # logits = model(g = sg, nfeats = node_feat, efeats = h)\n",
    "        # pred_label = tensor([0, 0, 0,  ..., 0, 1, 0], device='cuda:0')\n",
    "        # logits = tensor([[ 0.0059,  0.0517], [-0.0075,  0.0101], ..., device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "        # log_probs = logits.log_softmax(dim=-1)\n",
    "        # loss = -log_probs[edge_indice, pred_label[edge_indice]]\n",
    "        loss11 = criterion2(logits[train_mask2], pred_label[train_mask2])\n",
    "        loss = loss_regularize(loss11, efeat_mask, edge_mask)\n",
    "        # loss = loss_regularize(loss, efeat_mask, edge_mask)\n",
    "        \n",
    "        #if epoch % 100 == 0:\n",
    "            #print(\"+++++++++++++++\")\n",
    "            #print(f'epoch number {epoch}, CrossEntropy_Loss = {loss11}, final_loss = {loss}, time = {datetime.datetime.now()}')\n",
    "            #print(\"edge_mask : \", edge_mask.detach().sigmoid())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #print(\"final results before sigmoid : \")\n",
    "    #print(\"edge_mask : \", edge_mask)\n",
    "    #print(\"***********************************\")\n",
    "\n",
    "    edge_mask = edge_mask.detach().sigmoid()\n",
    "\n",
    "    return edge_indice, sg, edge_mask, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "sophisticated-truth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final results : \n",
      "edge_mask :  tensor([0.0337, 0.0335, 0.0333, 0.0337, 0.9667, 0.9667, 0.0338, 0.9662, 0.0334,\n",
      "        0.0337, 0.0338, 0.0337, 0.9663, 0.0337, 0.0324, 0.0333, 0.0333, 0.0333,\n",
      "        0.0338, 0.9663, 0.0335, 0.9674, 0.0338, 0.9662])\n",
      "sub_graph :  Graph(num_nodes=21, num_edges=24,\n",
      "      ndata_schemes={'feature': Scheme(shape=(1, 76), dtype=torch.float32), '_ID': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'Edge_indx': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(), dtype=torch.int64), 'h': Scheme(shape=(1, 76), dtype=torch.float32), '_ID': Scheme(shape=(), dtype=torch.int64)})\n",
      "edge_indice :  5\n",
      "loss :  0.7548076179623604\n"
     ]
    }
   ],
   "source": [
    "Test_Graph1 = Test_Graph.to('cuda:0')\n",
    "node_features_test1 = Test_Graph1.ndata['feature']\n",
    "edge_features_test1 = Test_Graph1.edata['h']\n",
    "\n",
    "edge_indice, sub_graph, edge_mask, loss = explain_edges(model1_test, 98607, Test_Graph1, node_features_test1, edge_features_test1)\n",
    "\n",
    "print(\"final results : \")\n",
    "print(\"edge_mask : \", edge_mask)\n",
    "print(\"sub_graph : \", sub_graph)\n",
    "print(\"edge_indice : \", edge_indice)\n",
    "print(\"loss : \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "legal-oasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge_indx    int64\n",
      "label        int64\n",
      "dtype: object\n",
      "      Edge_indx  label\n",
      "0         98607      0\n",
      "1        102377      0\n",
      "2          4448      0\n",
      "3         39811      0\n",
      "4         23951      0\n",
      "...         ...    ...\n",
      "3119     127231      1\n",
      "3120      42725      1\n",
      "3121      84483      1\n",
      "3122       7892      1\n",
      "3123      85275      1\n",
      "\n",
      "[3124 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "indx_edges_to_explain = pd.read_csv('/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/Edges_to_explain.csv', encoding=\"ISO-88591\", dtype = str)\n",
    "indx_edges_to_explain = indx_edges_to_explain.apply(pd.to_numeric)\n",
    "print(indx_edges_to_explain.dtypes)\n",
    "print(indx_edges_to_explain)\n",
    "\n",
    "# results_df = pd.DataFrame(columns = ['edge_indx', 'label', 'edge_indice', 'sg', 'edge_mask', 'loss'])\n",
    "results_df = pd.DataFrame(columns = ['edge_indx', 'label', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "amateur-simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb edges to explain = 3124\n",
      "0th edge\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dddddddd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-0ee9068957eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# shifting index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdddddddd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dddddddd' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"nb edges to explain =\", len(indx_edges_to_explain['Edge_indx']))\n",
    "\n",
    "for i, x in enumerate(indx_edges_to_explain['Edge_indx']):\n",
    "    if (i % 100) == 0 :\n",
    "        print(f\"{i}th edge\")\n",
    "    edge_indice, sub_graph, edge_mask, loss = explain_edges(model1_test, x, Test_Graph1, node_features_test1, edge_features_test1)\n",
    "    results_df.loc[-1] = [x, indx_edges_to_explain['label'][i], loss]  # adding a row\n",
    "    results_df.index = results_df.index + 1  # shifting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dried-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   edge_indx  label      loss\n",
      "0    98607.0    0.0  0.760596\n"
     ]
    }
   ],
   "source": [
    "results_df.to_csv(f'/home/ahmed/GNN-Based-ANIDS/GNN-Based-ANIDS/jupyter_notebooks/XAI/GNNExplainer/Models/Final_results.csv', sep=',', index = False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-weight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
